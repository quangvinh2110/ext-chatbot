{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67be679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=/data/snowflake-arctic-embed-l-v2.0\n",
    "volume=/Users/vinhnguyen/Projects/ext-chatbot/model_hub # share a volume with the Docker container to avoid downloading weights every run\n",
    "\n",
    "docker run --gpus all -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-embeddings-inference:cpu-1.8 --model-id $model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete AI Query Processing Flow\n",
    "Consolidated version of the chatbot's query processing pipeline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Dict, Optional, Tuple, Union\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# LLM Configuration\n",
    "LLM_BASE_URL = os.getenv(\"LLM_BASE_URL\", \"http://10.164.84.47:8814/v1\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"LLM-ver01\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"xxx\")\n",
    "\n",
    "# Embedding Configuration\n",
    "EMBED_BASE_URL = os.getenv(\"EMBED_BASE_URL\", \"http://10.164.84.47:7893\")\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"embed-ver01\")\n",
    "\n",
    "# Vector Store Configuration\n",
    "QDRANT_ENDPOINT = os.getenv(\"QDRANT_ENDPOINT\", \"http://10.164.84.47:6933\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\", \"ailab2024\")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION\", None)\n",
    "QDRANT_HTTPS = os.getenv(\"QDRANT_HTTPS\", False)\n",
    "QDRANT_PORT = os.getenv(\"QDRANT_PORT\", 6933)\n",
    "\n",
    "# Reranker Configuration\n",
    "RERANKER_BASE_URL = os.getenv(\"RERANKER_BASE_URL\", \"http://10.164.84.47:7893/rerank\")\n",
    "RERANKER_MODEL = os.getenv(\"RERANKER_MODEL\", \"reranker-ver01\")\n",
    "\n",
    "# Thresholds\n",
    "RETRIEVAL_SCORE_THRESHOLD = float(os.getenv(\"RETRIEVAL_SCORE_THRESHOLD\", \"0.4\"))\n",
    "RERANKER_SCORE_THRESHOLD = float(os.getenv(\"RERANKER_SCORE_THRESHOLD\", \"0.25\"))\n",
    "RELATED_SCORE_THRESHOLD = float(os.getenv(\"RELATED_SCORE_THRESHOLD\", \"3.5\"))\n",
    "\n",
    "# Business Configuration\n",
    "BOT_NAME = os.getenv(\"bot_name\", \"Yuta\")\n",
    "BUSINESS_NAME = os.getenv(\"business_name\", \"Sushi Hokkaido Sachi\")\n",
    "INTRO_DOC = os.getenv(\"intro_doc\", \"Nhà hàng Nhật hàng đầu tại Việt Nam...\")\n",
    "WEB_LINK = os.getenv(\"web_link\", \"https://sushihokkaidosachi.com.vn/\")\n",
    "HOTLINE = os.getenv(\"hotline\", \"\")\n",
    "\n",
    "# Defaults\n",
    "DEFAULT_SUGGESTION_QUESTIONS = [\n",
    "    \"Bạn là ai?\",\n",
    "    \"Bạn cung cấp sản phẩm gì?\",\n",
    "    \"Làm sao để liên hệ?\",\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Classes and Functions\n",
    "# ============================================================================\n",
    "\n",
    "class ReasoningExtractor(Runnable):\n",
    "    \"\"\"Extracts reasoning content from LLM responses\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_token: str = \"`<think>`\",\n",
    "        end_token: str = \"`</think>`\",\n",
    "        return_full_output_only: bool = False\n",
    "    ):\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.return_full_output_only = return_full_output_only\n",
    "        self.reasoning_regex = re.compile(\n",
    "            rf\"{re.escape(start_token)}(.*?){re.escape(end_token)}\",\n",
    "            re.DOTALL\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: Union[str, dict], config: Optional[dict] = None) -> Union[str, dict]:\n",
    "        if not isinstance(input, AIMessage):\n",
    "            raise TypeError(\"ReasoningExtractor only supports AIMessage as input\")\n",
    "        \n",
    "        model_output = input.content or f\"{self.start_token}{self.end_token}\"\n",
    "        reasoning, content = self.extract_reasoning(model_output)\n",
    "\n",
    "        if self.return_full_output_only:\n",
    "            return content or \"\"\n",
    "\n",
    "        return {\n",
    "            \"reasoning\": reasoning,\n",
    "            \"content\": content\n",
    "        }\n",
    "\n",
    "    def extract_reasoning(self, model_output: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        if self.end_token not in model_output:\n",
    "            return None, model_output \n",
    "        if self.start_token not in model_output:\n",
    "            model_output = f\"{self.start_token}{model_output}\"\n",
    "\n",
    "        matches = self.reasoning_regex.findall(model_output)\n",
    "        if not matches:\n",
    "            return None, model_output\n",
    "\n",
    "        reasoning_content = matches[0]\n",
    "        end_index = len(f\"{self.start_token}{reasoning_content}{self.end_token}\")\n",
    "        final_output = model_output[end_index:]\n",
    "\n",
    "        if len(final_output.strip()) == 0:\n",
    "            return reasoning_content, None\n",
    "        return reasoning_content, final_output\n",
    "\n",
    "\n",
    "def convert_to_messages(history: List[Dict]) -> List[BaseMessage]:\n",
    "    \"\"\"Convert OpenAI format messages to LangChain messages\"\"\"\n",
    "    messages = []\n",
    "    for msg in history:\n",
    "        role = msg.get(\"role\", \"\")\n",
    "        content = msg.get(\"content\", \"\")\n",
    "        \n",
    "        if role == \"user\":\n",
    "            messages.append(HumanMessage(content=content))\n",
    "        elif role == \"assistant\":\n",
    "            messages.append(AIMessage(content=content))\n",
    "        elif role == \"system\":\n",
    "            messages.append(SystemMessage(content=content))\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    \"\"\"Check if a string is a valid URL\"\"\"\n",
    "    url_regex = re.compile(\n",
    "        r'^(https?|ftp)://'\n",
    "        r'(www\\.)?'\n",
    "        r'(?:(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,})'\n",
    "        r'(?:\\/[^\\s]*)?$'\n",
    "    )\n",
    "    return bool(url_regex.match(url))\n",
    "\n",
    "\n",
    "def extract_reference_from_response(response: str) -> Tuple[str, Dict]:\n",
    "    \"\"\"Extract reference URLs from response text\"\"\"\n",
    "    pattern = r\"\\[([^\\]]*)\\]\\((https?://[^\\)]+)\\)\"\n",
    "    try:\n",
    "        matches = re.findall(pattern, response)\n",
    "        matches = list(set(matches))\n",
    "\n",
    "        def replace_with_title(match):\n",
    "            title = match.group(1)\n",
    "            return title\n",
    "        \n",
    "        final_response = re.sub(pattern, replace_with_title, response)\n",
    "\n",
    "        reference_jsons = {\n",
    "            \"reference_url\": [\n",
    "                {\n",
    "                    \"title\": match[0] if match[0].lower() not in [\"đây\", \"tại đây\"] else \"Link\",\n",
    "                    \"payload\": {\n",
    "                        \"url\": match[1] + \"?utm_source=chatbot_AILab\"\n",
    "                    },\n",
    "                    \"type\": \"oa.open.url\",\n",
    "                }\n",
    "                for match in matches\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return final_response, reference_jsons\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting reference: {e}\")\n",
    "        return response, {\"reference_url\": []}\n",
    "\n",
    "\n",
    "def filter_url_in_response(response: str) -> str:\n",
    "    \"\"\"Filter and clean URLs in response\"\"\"\n",
    "    pattern = r\"\\[([^\\]]*)\\]\\((https?://[^\\)]+)\\)\"\n",
    "    try:\n",
    "        def replace_with_title(match):\n",
    "            title = match.group(1)\n",
    "            return title\n",
    "        final_response = re.sub(pattern, replace_with_title, response)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering URL: {e}\")\n",
    "        return response\n",
    "\n",
    "\n",
    "def check_prefix_tokens_with_questions(\n",
    "    response_string: str,\n",
    "    answer_prefix_string: str = \"Final Answer:\",\n",
    "    question_prefix_string: str = \"Bonus questions:\",\n",
    ") -> Tuple[bool, List[str], Dict, Dict]:\n",
    "    \"\"\"Extract answer and questions from response\"\"\"\n",
    "    answer_prefix = answer_prefix_string.lower()\n",
    "    question_prefix = question_prefix_string.lower()\n",
    "    \n",
    "    response_lower = response_string.lower()\n",
    "    \n",
    "    answer_pos = response_lower.find(answer_prefix)\n",
    "    question_pos = response_lower.find(question_prefix)\n",
    "    \n",
    "    check_answer = answer_pos != -1\n",
    "    check_question = question_pos != -1\n",
    "    \n",
    "    if check_answer:\n",
    "        answer_start = answer_pos + len(answer_prefix)\n",
    "        answer_text = response_string[answer_start:].strip()\n",
    "        \n",
    "        if check_question and question_pos < answer_pos:\n",
    "            # Questions come before answer\n",
    "            question_text = response_string[question_pos + len(question_prefix):answer_pos].strip()\n",
    "            try:\n",
    "                # Try to extract JSON\n",
    "                json_start = question_text.find('{')\n",
    "                json_end = question_text.rfind('}') + 1\n",
    "                if json_start != -1 and json_end > json_start:\n",
    "                    question_json = json.loads(question_text[json_start:json_end])\n",
    "                else:\n",
    "                    question_json = {\"question_list\": DEFAULT_SUGGESTION_QUESTIONS}\n",
    "            except:\n",
    "                question_json = {\"question_list\": DEFAULT_SUGGESTION_QUESTIONS}\n",
    "        else:\n",
    "            question_json = {\"question_list\": DEFAULT_SUGGESTION_QUESTIONS}\n",
    "        \n",
    "        return check_answer, [answer_text], question_json, {\"reference_url\": []}\n",
    "    \n",
    "    return False, [], {\"question_list\": DEFAULT_SUGGESTION_QUESTIONS}, {\"reference_url\": []}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Core Components\n",
    "# ============================================================================\n",
    "\n",
    "class RewriterOutputJsonFormat(BaseModel):\n",
    "    \"\"\"Output format for query rewriter\"\"\"\n",
    "    is_related_score: int = Field(description=\"Score from 0-5 indicating how related the query is to conversation\")\n",
    "    key_words: List[str] = Field(description=\"Important keywords from conversation and question\")\n",
    "    new_question: str = Field(description=\"Rewritten question that is more general\")\n",
    "    word_dup: float = Field(description=\"Ratio of keyword overlap between old and new question (0-1)\")\n",
    "    word_rewrite: float = Field(description=\"Semantic similarity between old and new question (0-1)\")\n",
    "\n",
    "\n",
    "class Rewriter:\n",
    "    \"\"\"Rewrites user queries to be more searchable\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            base_url=LLM_BASE_URL,\n",
    "            model=LLM_MODEL,\n",
    "            api_key=API_KEY,\n",
    "            temperature=0.0,\n",
    "            max_tokens=4096,\n",
    "            extra_body={\n",
    "                'repetition_penalty': 1.15,\n",
    "                'chat_template_kwargs': {\n",
    "                    'enable_thinking': False\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        self.reason_parser = ReasoningExtractor(return_full_output_only=True)\n",
    "        self.parser = JsonOutputParser(pydantic_object=RewriterOutputJsonFormat)\n",
    "        \n",
    "        # Simplified prompts (you can load from Langfuse in production)\n",
    "        system_prompt = \"\"\"You are a query rewriter. Your task is to rewrite user questions to be more searchable while maintaining the original intent.\"\"\"\n",
    "        \n",
    "        user_prompt_template = \"\"\"Given the conversation history and the last question, rewrite the question to be more general and searchable.\n",
    "\n",
    "Conversation:\n",
    "{conversation}\n",
    "\n",
    "Last Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Return a JSON object with the required fields.\"\"\"\n",
    "        \n",
    "        self.rewrite_chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessagePromptTemplate(\n",
    "                prompt=PromptTemplate(\n",
    "                    template=user_prompt_template,\n",
    "                    input_variables=[\"conversation\", \"question\"],\n",
    "                    partial_variables={\"format_instructions\": self.parser.get_format_instructions(\"\")}\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        self.chain = self.rewrite_chat_prompt_template | self.llm | self.reason_parser | self.parser\n",
    "    \n",
    "    def rewrite(self, query: str, history: List[BaseMessage]) -> Tuple[float, str]:\n",
    "        \"\"\"Rewrite query based on conversation history\"\"\"\n",
    "        reformatted_history = []\n",
    "        for h in history:\n",
    "            if isinstance(h, HumanMessage):\n",
    "                reformatted_history.append(f\"user: {h.content}\")\n",
    "            elif isinstance(h, AIMessage):\n",
    "                reformatted_history.append(f\"assistant: {h.content}\")\n",
    "        \n",
    "        history_context = \"\\n\".join(reformatted_history)\n",
    "        \n",
    "        result = self.chain.invoke({\n",
    "            \"conversation\": history_context,\n",
    "            \"question\": query\n",
    "        })\n",
    "        \n",
    "        is_related_score, new_question = self.calculate_result(result)\n",
    "        return is_related_score, new_question\n",
    "    \n",
    "    def calculate_result(self, result_json: Dict) -> Tuple[float, str]:\n",
    "        \"\"\"Calculate final relatedness score\"\"\"\n",
    "        is_related_score = float(result_json.get(\"is_related_score\", 0) / 5)\n",
    "        word_dup = float(result_json.get(\"word_dup\", 0))\n",
    "        word_rewrite = float(result_json.get(\"word_rewrite\", 0))\n",
    "        is_related_score_calculate = ((is_related_score * 2 + word_dup + word_rewrite) / 4) * 5\n",
    "        return is_related_score_calculate, result_json.get(\"new_question\", \"\")\n",
    "\n",
    "\n",
    "class LocalEmbeddings(Embeddings):\n",
    "    \"\"\"Local embeddings using Hugging Face transformers\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'Snowflake/snowflake-arctic-embed-l-v2.0', query_prefix: str = 'query: '):\n",
    "        \"\"\"\n",
    "        Initialize local embeddings model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model name or local path\n",
    "            query_prefix: Prefix to add to queries (not documents)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.query_prefix = query_prefix\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents (without query prefix)\"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Tokenize documents (no prefix)\n",
    "        tokens = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            max_length=8192\n",
    "        )\n",
    "        \n",
    "        # Move tokens to device\n",
    "        tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "        \n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**tokens)[0][:, 0]  # Take first token embedding\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Convert to list of lists\n",
    "        return embeddings.cpu().tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query (with query prefix)\"\"\"\n",
    "        # Add query prefix\n",
    "        query_with_prefix = f\"{self.query_prefix}{text}\"\n",
    "        \n",
    "        # Tokenize query\n",
    "        tokens = self.tokenizer(\n",
    "            [query_with_prefix],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            max_length=8192\n",
    "        )\n",
    "        \n",
    "        # Move tokens to device\n",
    "        tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "        \n",
    "        # Compute embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model(**tokens)[0][:, 0]  # Take first token embedding\n",
    "        \n",
    "        # Normalize embedding\n",
    "        embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        # Convert to list\n",
    "        return embedding.cpu().tolist()[0]\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Vector store for document retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize vector store with local embeddings\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model name or local path. \n",
    "                       Defaults to 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "                       or local path if model exists in model_hub/\n",
    "        \"\"\"\n",
    "        # Check for local model first, then use HuggingFace model name\n",
    "        local_model_path = 'model_hub/snowflake-arctic-embed-l-v2.0'\n",
    "        if model_name is None:\n",
    "            if os.path.exists(local_model_path):\n",
    "                model_name = local_model_path\n",
    "            else:\n",
    "                model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "        \n",
    "        vector_store_kwargs = {\n",
    "            \"url\": QDRANT_ENDPOINT,\n",
    "            \"api_key\": QDRANT_API_KEY,\n",
    "            \"https\": QDRANT_HTTPS,\n",
    "            \"port\": QDRANT_PORT\n",
    "        }\n",
    "        \n",
    "        # Use local embeddings instead of service\n",
    "        embedding_api = LocalEmbeddings(model_name=model_name)\n",
    "        self.client = QdrantClient(\n",
    "            url=vector_store_kwargs[\"url\"],\n",
    "            api_key=vector_store_kwargs[\"api_key\"],\n",
    "            https=vector_store_kwargs[\"https\"],\n",
    "            port=vector_store_kwargs[\"port\"]\n",
    "        )\n",
    "        self.vector_store = Qdrant(\n",
    "            embeddings=embedding_api,\n",
    "            collection_name=QDRANT_COLLECTION,\n",
    "            client=self.client\n",
    "        )\n",
    "    \n",
    "    def search_docs_with_score(self, query: str, top_k: int = 30) -> List[Document]:\n",
    "        \"\"\"Search documents with similarity scores\"\"\"\n",
    "        query_doc = Document(page_content=query)\n",
    "        results = self.vector_store.similarity_search_with_score(\n",
    "            query_doc,\n",
    "            k=top_k,\n",
    "            score_threshold=RETRIEVAL_SCORE_THRESHOLD,\n",
    "        )\n",
    "        matched_docs = []\n",
    "        for doc, score in results:\n",
    "            doc.metadata[\"score\"] = score\n",
    "            matched_docs.append(doc)\n",
    "        return matched_docs\n",
    "\n",
    "\n",
    "def rerank_documents(query: str, documents: List[Document], top_n: int = 8) -> List[Document]:\n",
    "    \"\"\"Rerank documents using reranker API\"\"\"\n",
    "    if len(documents) == 0:\n",
    "        return []\n",
    "    \n",
    "    res = requests.post(\n",
    "        RERANKER_BASE_URL,\n",
    "        json={\n",
    "            \"model\": RERANKER_MODEL,\n",
    "            \"query\": query,\n",
    "            \"documents\": [d.page_content for d in documents],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    res_data = json.loads(res.content)['results']\n",
    "    sorted_res = sorted(res_data, key=lambda x: x['relevance_score'], reverse=True)[:top_n]\n",
    "    \n",
    "    ranked_docs = []\n",
    "    for item in sorted_res:\n",
    "        index = item[\"index\"]\n",
    "        score = item[\"relevance_score\"]\n",
    "        \n",
    "        target_doc = documents[index]\n",
    "        target_doc.metadata[\"relevance_score\"] = score\n",
    "        if score >= RERANKER_SCORE_THRESHOLD:\n",
    "            ranked_docs.append(target_doc)\n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "class StructuredResponse(BaseModel):\n",
    "    \"\"\"Structured output for LLM response\"\"\"\n",
    "    rule: str = Field(description=\"Whether you are allowed to answer\")\n",
    "    thought: str = Field(description=\"Think about the user question\")\n",
    "    language: str = Field(description=\"The language that user uses\")\n",
    "    addressing_way: str = Field(description=\"Xưng em với Anh/chị\")\n",
    "    action: str = Field(description=\"What you gonna do to answer\")\n",
    "    bonus_questions: list[str] = Field(description=\"Follow-up questions\")\n",
    "    observation: str = Field(description=\"What you see user ask\")\n",
    "    reference_url: str = Field(description=\"Reference URL\")\n",
    "    final_answer: str = Field(description=\"The final answer to user\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Query Processing Function\n",
    "# ============================================================================\n",
    "\n",
    "def knowledge_base_chat(\n",
    "    user_query: str,\n",
    "    history: List[Dict],\n",
    "    force_disclaimer: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Main function for processing user queries through the complete AI pipeline\n",
    "    \n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        history: Conversation history in OpenAI format [{\"role\": \"user/assistant\", \"content\": \"...\"}]\n",
    "        force_disclaimer: Whether to force disclaimer in response\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with user_msg, raw_llm_msg, question_list, reference_list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    rewriter = Rewriter()\n",
    "    vector_store = VectorStore()\n",
    "    llm_model = ChatOpenAI(\n",
    "        base_url=LLM_BASE_URL,\n",
    "        model=LLM_MODEL,\n",
    "        api_key=API_KEY,\n",
    "        temperature=0.0,\n",
    "        max_tokens=4096,\n",
    "        extra_body={\n",
    "            'repetition_penalty': 1.15,\n",
    "            'chat_template_kwargs': {\n",
    "                'enable_thinking': True\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    parser = JsonOutputParser(pydantic_object=StructuredResponse)\n",
    "    reason_extractor = ReasoningExtractor()\n",
    "    \n",
    "    # Convert history\n",
    "    history_messages = convert_to_messages(history[-10:])  # Keep last 10 messages\n",
    "    \n",
    "    # Step 1: Query Rewriting\n",
    "    is_related_score, rewrited_query = rewriter.rewrite(user_query, history_messages)\n",
    "    print(f\"Rewriter: {is_related_score >= RELATED_SCORE_THRESHOLD} with score: {is_related_score}\")\n",
    "    is_related = is_related_score >= RELATED_SCORE_THRESHOLD\n",
    "    search_query = rewrited_query if is_related else user_query\n",
    "    \n",
    "    # Step 2: Vector Search\n",
    "    matched_docs = vector_store.search_docs_with_score(search_query, top_k=30)\n",
    "    \n",
    "    # Step 3: Reranking\n",
    "    reranked_docs = rerank_documents(search_query, matched_docs, top_n=8)\n",
    "    \n",
    "    # Step 4: Context Construction\n",
    "    if len(reranked_docs) != 0:\n",
    "        context_prompt = \"\".join(f\"{i}: {doc.page_content}\\n\\n\" for i, doc in enumerate(reranked_docs))\n",
    "    else:\n",
    "        context_prompt = \"No information found.\"\n",
    "    \n",
    "    enable_context = len(reranked_docs) != 0\n",
    "    response_tone = \"formal\"\n",
    "    \n",
    "    # Step 5: Prompt Selection (simplified - you can load from Langfuse)\n",
    "    if len(reranked_docs) == 0:\n",
    "        vib_system_prompt = \"\"\"You are a helpful assistant named {bot_name} for {business_name}. \n",
    "        {intro_doc}\n",
    "        You should be polite and use \"em\" to refer to yourself and \"Anh/Chị\" to refer to the user.\n",
    "        If you don't have information, politely say so.\"\"\"\n",
    "        query_instruction = \"\"\"Answer the following question based on your knowledge:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a helpful answer.\"\"\"\n",
    "    else:\n",
    "        vib_system_prompt = \"\"\"You are a helpful assistant named {bot_name} for {business_name}. \n",
    "        {intro_doc}\n",
    "        You should be polite and use \"em\" to refer to yourself and \"Anh/Chị\" to refer to the user.\n",
    "        Answer questions based on the provided context.\"\"\"\n",
    "        query_instruction = \"\"\"Answer the following question based on the provided context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a detailed answer based on the context. Include reference URLs if mentioned in the context.\"\"\"\n",
    "    \n",
    "    if force_disclaimer:\n",
    "        query_instruction += \"\\n\\nInclude a disclaimer about information accuracy.\"\n",
    "    \n",
    "    # Step 6: LLM Generation\n",
    "    final_chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=vib_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history_messages\"),\n",
    "        HumanMessagePromptTemplate.from_template(query_instruction),\n",
    "    ])\n",
    "    \n",
    "    chain = final_chat_prompt_template | llm_model | reason_extractor\n",
    "    \n",
    "    retry_attempt_questions = [\n",
    "        \"\",\n",
    "        \"Trả lời câu này: \",\n",
    "        \"Cho tôi hỏi: \",\n",
    "        \"Tôi muốn hỏi là: \",\n",
    "    ]\n",
    "    \n",
    "    # Contact info setup\n",
    "    contact_info = \"\"\n",
    "    if hotline and web_link:\n",
    "        contact_info = f\"The official website of {BUSINESS_NAME} is {WEB_LINK}, the Hotline is {HOTLINE}\"\n",
    "    elif web_link:\n",
    "        contact_info = f\"The official website of {BUSINESS_NAME} is {WEB_LINK}\"\n",
    "    elif hotline:\n",
    "        contact_info = f\"The Hotline of {BUSINESS_NAME} is {HOTLINE}\"\n",
    "    \n",
    "    # Retry loop for LLM generation\n",
    "    response_final = \"\"\n",
    "    question_list = []\n",
    "    check_final = False\n",
    "    \n",
    "    for ii, retry_question in enumerate(retry_attempt_questions):\n",
    "        chain_input = {\n",
    "            \"question\": retry_question + user_query,\n",
    "            \"history_messages\": history_messages,\n",
    "            \"bot_name\": BOT_NAME,\n",
    "            \"business_name\": BUSINESS_NAME,\n",
    "            \"web_link\": WEB_LINK,\n",
    "            \"hotline\": HOTLINE,\n",
    "            \"intro_doc\": INTRO_DOC,\n",
    "            \"contact_info\": contact_info,\n",
    "        }\n",
    "        if enable_context:\n",
    "            chain_input[\"context\"] = context_prompt\n",
    "        \n",
    "        response = chain.invoke(input=chain_input)\n",
    "        reasoning_content, final_output = response['reasoning'], response['content']\n",
    "        \n",
    "        # Clean up response\n",
    "        final_output = re.sub(r\"\\*\\*Final Answer\\*\\*:?\", \"Final Answer:\", final_output)\n",
    "        final_output = re.sub(r\"\\*\\*FINAL ANSWER\\*\\*\", \"Final Answer\", final_output)\n",
    "        final_output = re.sub(r'FINAL ANSWER:?', 'Final Answer:', final_output)\n",
    "        final_output = re.sub(r'## FINAL ANSWER:?', 'Final Answer:', final_output)\n",
    "        \n",
    "        try:\n",
    "            check_final, response_final_tokens, question_list_json, reference_json = check_prefix_tokens_with_questions(final_output)\n",
    "            response_final = ''.join(response_final_tokens)\n",
    "            question_list = question_list_json.get(\"question_list\", [])\n",
    "            \n",
    "            if check_final:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error at extract final answer: {e}\")\n",
    "            check_final = False\n",
    "    \n",
    "    if not check_final:\n",
    "        response_final = \"Em xin lỗi, em chưa hiểu ý của Anh/Chị. Mong Anh/Chị có thể hỏi lại lần nữa ạ.\"\n",
    "        question_list = []\n",
    "    \n",
    "    # Step 7: Response Processing\n",
    "    final_response, reference_json_valid = extract_reference_from_response(response_final)\n",
    "    final_reference_json_list = []\n",
    "    unique_links = []\n",
    "    \n",
    "    for item in reference_json_valid.get('reference_url', []):\n",
    "        if item['payload']['url'] not in unique_links and not is_valid_url(item['title']):\n",
    "            final_reference_json_list.append(item)\n",
    "            unique_links.append(item['payload']['url'])\n",
    "    \n",
    "    # Filter URLs from response\n",
    "    final_response = filter_url_in_response(final_response)\n",
    "    \n",
    "    # Final output\n",
    "    output_response = {\n",
    "        \"user_msg\": final_response.lstrip(),\n",
    "        \"raw_llm_msg\": response_final.lstrip(),\n",
    "        \"question_list\": (question_list + DEFAULT_SUGGESTION_QUESTIONS)[0:3],\n",
    "        \"reference_list\": final_reference_json_list,\n",
    "    }\n",
    "    \n",
    "    return output_response\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Example Usage\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    result = knowledge_base_chat(\n",
    "        user_query=\"Giá món sushi là bao nhiêu?\",\n",
    "        history=[\n",
    "            {\"role\": \"user\", \"content\": \"Xin chào\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Chào Anh/Chị! Em có thể giúp gì cho Anh/Chị?\"},\n",
    "        ],\n",
    "        force_disclaimer=False\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"FINAL RESPONSE:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"User Message: {result['user_msg']}\")\n",
    "    print(f\"\\nSuggested Questions: {result['question_list']}\")\n",
    "    print(f\"\\nReferences: {result['reference_list']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
