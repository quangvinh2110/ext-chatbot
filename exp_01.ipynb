{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d241c26c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "from functools import lru_cache, partial\n",
        "import json\n",
        "import json5\n",
        "import re\n",
        "import asyncio\n",
        "from datetime import datetime\n",
        "from typing import List, Optional, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import TypedDict\n",
        "from IPython.display import Image\n",
        "from operator import itemgetter\n",
        "from sqlglot import exp, parse_one\n",
        "\n",
        "from langchain_core.messages.tool import ToolCall\n",
        "from langchain_core.messages import (\n",
        "    AnyMessage,\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage,\n",
        ")\n",
        "\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_community.embeddings import InfinityEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.tools import tool\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, Runnable\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e306567",
      "metadata": {},
      "source": [
        "# Setup components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "89608e27",
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM_BASE_URL=os.getenv(\"LLM_BASE_URL\")\n",
        "LLM_MODEL=os.getenv(\"LLM_MODEL\")\n",
        "LLM_API_KEY=os.getenv(\"LLM_API_KEY\")\n",
        "\n",
        "EMBED_BASE_URL=os.getenv(\"EMBED_BASE_URL\")\n",
        "EMBED_MODEL=os.getenv(\"EMBED_MODEL\")\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def get_llm_model():\n",
        "    return ChatOpenAI(\n",
        "        model=LLM_MODEL,\n",
        "        base_url=LLM_BASE_URL,\n",
        "        api_key=LLM_API_KEY,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        presence_penalty=1,\n",
        "        extra_body = {\n",
        "            'chat_template_kwargs': {'enable_thinking': False},\n",
        "            \"top_k\": 20,\n",
        "            \"mip_p\": 0,\n",
        "        },\n",
        "    )\n",
        "\n",
        "@lru_cache()\n",
        "def get_thinking_llm_model():\n",
        "    return ChatOpenAI(\n",
        "        model=LLM_MODEL,\n",
        "        base_url=LLM_BASE_URL,\n",
        "        api_key=LLM_API_KEY,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        presence_penalty=1,\n",
        "        extra_body = {\n",
        "            'chat_template_kwargs': {'enable_thinking': True},\n",
        "            \"top_k\": 20,\n",
        "            \"mip_p\": 0,\n",
        "        },\n",
        "    )\n",
        "\n",
        "@lru_cache()\n",
        "def get_embedding_model():\n",
        "    return InfinityEmbeddings(\n",
        "        model=EMBED_MODEL,\n",
        "        infinity_api_url=EMBED_BASE_URL,\n",
        "    )\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def get_vector_store():\n",
        "    client = QdrantClient(\n",
        "        url=\"http://localhost\",\n",
        "        grpc_port=6334,\n",
        "        prefer_grpc=True,\n",
        "    )\n",
        "    embedding_model = get_embedding_model()\n",
        "    client.create_collection(\n",
        "        collection_name=\"demo\",\n",
        "        vectors_config=VectorParams(\n",
        "            size=len(embedding_model.embed_query(\"Hello\")), \n",
        "            distance=Distance.COSINE\n",
        "        ),\n",
        "    )\n",
        "    return QdrantVectorStore(\n",
        "        client=client,\n",
        "        collection_name=\"demo\",\n",
        "        embedding=embedding_model,\n",
        "    )\n",
        "\n",
        "\n",
        "# @lru_cache()\n",
        "# def get_sqlite_db():\n",
        "#     return SQLDatabase.from_uri(\"sqlite:////Users/vinhnguyen/Projects/ext-chatbot/resources/database/batdongsan.db\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ecab7f",
      "metadata": {},
      "source": [
        "# Process data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456d88dd",
      "metadata": {},
      "source": [
        "## Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b2979e68",
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "from src.tools.table import create_sqlite, create_faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6fd73e84",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['BĐS Cho thuê 500', 'BĐS Bán 500']\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "tables = []\n",
        "for filepath in glob.glob(\"/Users/vinhnguyen/Projects/ext-chatbot/resources/processed_data/batdongsan_1/*.json\"):\n",
        "    table_name = \".\".join(filepath.split(\"/\")[-1].split(\".\")[:-1])\n",
        "    with open(filepath, \"r\") as f:\n",
        "        table = json.load(f)\n",
        "        table[\"pydantic_schema\"][\"title\"] = table_name\n",
        "        if len(table[\"transformed_data\"]) > 100:\n",
        "            tables.append(table)\n",
        "\n",
        "print([table[\"pydantic_schema\"][\"title\"] for table in tables])\n",
        "print(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2a5586",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding BĐS Cho thuê 500.ID: 100%|██████████| 500/500 [00:02<00:00, 170.60values/s]\n",
            "Embedding BĐS Cho thuê 500.Dự án: 100%|██████████| 34/34 [00:00<00:00, 91.63values/s]\n",
            "Embedding BĐS Cho thuê 500.Giá/m²/tháng: 100%|██████████| 492/492 [00:02<00:00, 167.45values/s]\n",
            "Embedding BĐS Cho thuê 500.Ngày có thể chuyển vào: 100%|██████████| 61/61 [00:00<00:00, 119.56values/s]\n",
            "Embedding BĐS Cho thuê 500.Hướng: 100%|██████████| 8/8 [00:00<00:00, 66.63values/s]\n",
            "Embedding BĐS Cho thuê 500.Nội thất: 100%|██████████| 5/5 [00:00<00:00, 44.89values/s]\n",
            "Embedding BĐS Cho thuê 500.Phường/Xã: 100%|██████████| 15/15 [00:00<00:00, 82.35values/s]\n",
            "Embedding BĐS Cho thuê 500.Tình trạng: 100%|██████████| 4/4 [00:00<00:00, 39.47values/s]\n",
            "Embedding BĐS Cho thuê 500.Cho phép nuôi thú cưng: 100%|██████████| 3/3 [00:00<00:00, 33.10values/s]\n",
            "Embedding BĐS Cho thuê 500.Bếp: 100%|██████████| 4/4 [00:00<00:00, 41.04values/s]\n",
            "Embedding BĐS Cho thuê 500.Điều hòa: 100%|██████████| 5/5 [00:00<00:00, 46.14values/s]\n",
            "Embedding BĐS Cho thuê 500.Nóng lạnh: 100%|██████████| 3/3 [00:00<00:00, 37.06values/s]\n",
            "Embedding BĐS Cho thuê 500.Bãi đỗ xe: 100%|██████████| 5/5 [00:00<00:00, 48.14values/s]\n",
            "Embedding BĐS Cho thuê 500.Thời hạn thuê tối thiểu: 100%|██████████| 5/5 [00:00<00:00, 45.76values/s]\n",
            "Embedding BĐS Cho thuê 500.Phí dịch vụ: 100%|██████████| 21/21 [00:00<00:00, 103.22values/s]\n",
            "Embedding BĐS Cho thuê 500.Tiền điện nước: 100%|██████████| 5/5 [00:00<00:00, 33.11values/s]\n",
            "Embedding BĐS Cho thuê 500.Internet: 100%|██████████| 4/4 [00:00<00:00, 46.14values/s]\n",
            "Embedding BĐS Cho thuê 500.Người liên hệ: 100%|██████████| 16/16 [00:00<00:00, 97.03values/s]\n",
            "Embedding BĐS Cho thuê 500.SĐT liên hệ: 100%|██████████| 500/500 [00:02<00:00, 178.50values/s]\n",
            "Embedding BĐS Cho thuê 500.Zalo: 100%|██████████| 2/2 [00:00<00:00, 12.05values/s]\n",
            "Embedding BĐS Cho thuê 500.Email: 100%|██████████| 500/500 [00:02<00:00, 171.09values/s]\n",
            "Embedding BĐS Cho thuê 500.Ghi chú: 100%|██████████| 6/6 [00:00<00:00, 28.70values/s]\n",
            "Embedding BĐS Cho thuê 500.Loại BĐS: 100%|██████████| 13/13 [00:00<00:00, 69.63values/s]\n",
            "Embedding BĐS Cho thuê 500.Quận/Huyện: 100%|██████████| 38/38 [00:00<00:00, 151.92values/s]\n",
            "Embedding BĐS Cho thuê 500.Tiêu đề: 100%|██████████| 497/497 [00:02<00:00, 176.60values/s]\n",
            "Embedding BĐS Cho thuê 500.Tỉnh/TP: 100%|██████████| 8/8 [00:00<00:00, 40.21values/s]\n",
            "Embedding BĐS Cho thuê 500.Địa chỉ: 100%|██████████| 500/500 [00:02<00:00, 170.93values/s]\n",
            "Embedding BĐS Cho thuê 500.Địa chỉ_đường: 100%|██████████| 141/141 [00:01<00:00, 140.82values/s]\n",
            "Embedding BĐS Cho thuê 500.An ninh: 100%|██████████| 5/5 [00:00<00:00, 39.24values/s]\n",
            "Embedding BĐS Cho thuê 500.Thang máy: 100%|██████████| 3/3 [00:00<00:00, 32.59values/s]\n",
            "Embedding BĐS Cho thuê 500.Ban công/Sân thượng: 100%|██████████| 4/4 [00:00<00:00, 41.09values/s]\n",
            "Embedding BĐS Cho thuê 500.Tiện ích lân cận: 100%|██████████| 481/481 [00:02<00:00, 169.54values/s]\n",
            "Embedding BĐS Cho thuê 500.Khoảng cách tới trung tâm: 100%|██████████| 144/144 [00:00<00:00, 153.40values/s]\n",
            "Embedding BĐS Cho thuê 500.Tiền cọc: 100%|██████████| 474/474 [00:03<00:00, 151.31values/s]\n",
            "Embedding BĐS Cho thuê 500.Khoảng cách tới trung tâm_đơn vị: 100%|██████████| 1/1 [00:00<00:00, 13.00values/s]\n",
            "Embedding BĐS Cho thuê 500.Tiền cọc_Số tháng: 100%|██████████| 3/3 [00:00<00:00, 35.20values/s]\n",
            "Embedding BĐS Bán 500.ID: 100%|██████████| 500/500 [00:02<00:00, 168.87values/s]\n",
            "Embedding BĐS Bán 500.Dự án: 100%|██████████| 37/37 [00:00<00:00, 126.49values/s]\n",
            "Embedding BĐS Bán 500.Giá/m²: 100%|██████████| 451/451 [00:03<00:00, 146.64values/s]\n",
            "Embedding BĐS Bán 500.Giá/m²_don_vi: 100%|██████████| 1/1 [00:00<00:00,  9.67values/s]\n",
            "Embedding BĐS Bán 500.Ngày hết hạn: 100%|██████████| 202/202 [00:01<00:00, 137.40values/s]\n",
            "Embedding BĐS Bán 500.Hướng nhà: 100%|██████████| 8/8 [00:00<00:00, 65.68values/s]\n",
            "Embedding BĐS Bán 500.Hướng ban công: 100%|██████████| 9/9 [00:00<00:00, 71.57values/s]\n",
            "Embedding BĐS Bán 500.Nội thất: 100%|██████████| 5/5 [00:00<00:00, 46.03values/s]\n",
            "Embedding BĐS Bán 500.Pháp lý: 100%|██████████| 6/6 [00:00<00:00, 42.82values/s]\n",
            "Embedding BĐS Bán 500.Tình trạng: 100%|██████████| 6/6 [00:00<00:00, 42.48values/s]\n",
            "Embedding BĐS Bán 500.Năm xây dựng: 100%|██████████| 17/17 [00:00<00:00, 106.10values/s]\n",
            "Embedding BĐS Bán 500.Chủ đầu tư: 100%|██████████| 16/16 [00:00<00:00, 86.52values/s]\n",
            "Embedding BĐS Bán 500.Bãi đỗ xe: 100%|██████████| 5/5 [00:00<00:00, 45.02values/s]\n",
            "Embedding BĐS Bán 500.Email: 100%|██████████| 500/500 [00:02<00:00, 169.43values/s]\n",
            "Embedding BĐS Bán 500.Hoa hồng: 100%|██████████| 6/6 [00:00<00:00, 32.73values/s]\n",
            "Embedding BĐS Bán 500.Ghi chú: 100%|██████████| 6/6 [00:00<00:00, 51.58values/s]\n",
            "Embedding BĐS Bán 500.Loại BĐS: 100%|██████████| 14/14 [00:00<00:00, 84.42values/s]\n",
            "Embedding BĐS Bán 500.Phường/Xã: 100%|██████████| 15/15 [00:00<00:00, 111.96values/s]\n",
            "Embedding BĐS Bán 500.Quận/Huyện: 100%|██████████| 36/36 [00:00<00:00, 122.34values/s]\n",
            "Embedding BĐS Bán 500.Tiêu đề: 100%|██████████| 498/498 [00:03<00:00, 153.58values/s]\n",
            "Embedding BĐS Bán 500.Tỉnh/TP: 100%|██████████| 8/8 [00:00<00:00, 45.83values/s]\n",
            "Embedding BĐS Bán 500.Địa chỉ: 100%|██████████| 500/500 [00:02<00:00, 182.51values/s]\n",
            "Embedding BĐS Bán 500.Địa chỉ_Tên đường: 100%|██████████| 137/137 [00:01<00:00, 128.90values/s]\n",
            "Embedding BĐS Bán 500.View: 100%|██████████| 6/6 [00:00<00:00, 45.19values/s]\n",
            "Embedding BĐS Bán 500.Tiện ích: 100%|██████████| 500/500 [00:03<00:00, 152.79values/s]\n",
            "Embedding BĐS Bán 500.Ngày đăng: 100%|██████████| 170/170 [00:01<00:00, 149.35values/s]\n",
            "Embedding BĐS Bán 500.Người liên hệ: 100%|██████████| 16/16 [00:00<00:00, 79.29values/s]\n",
            "Embedding BĐS Bán 500.SĐT liên hệ: 100%|██████████| 500/500 [00:02<00:00, 174.18values/s]\n"
          ]
        }
      ],
      "source": [
        "# for table in tables:\n",
        "#     create_sqlite(\n",
        "#         schema=table[\"pydantic_schema\"],\n",
        "#         column_groups=table[\"column_groups\"],\n",
        "#         data=table[\"transformed_data\"],\n",
        "#         db_path=\"/Users/vinhnguyen/Projects/ext-chatbot/resources/database/batdongsan.db\",\n",
        "#     )\n",
        "\n",
        "\n",
        "for table in tables:\n",
        "    create_faiss(\n",
        "        schema=table[\"pydantic_schema\"],\n",
        "        db_path=\"/Users/vinhnguyen/Projects/ext-chatbot/resources/database/batdongsan.db\",\n",
        "        faiss_dir=\"/Users/vinhnguyen/Projects/ext-chatbot/resources/faiss/\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "abb6f635",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Request more files from PO to test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f81d17",
      "metadata": {},
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf394eb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_fn(text: str) -> tuple[str, str]:\n",
        "    \"\"\"Extract function name and arguments from tool call text.\"\"\"\n",
        "    fn_name, fn_args = '', ''\n",
        "    fn_name_s = '\"name\": \"'\n",
        "    fn_name_e = '\", \"'\n",
        "    fn_args_s = '\"arguments\": '\n",
        "    \n",
        "    i = text.find(fn_name_s)\n",
        "    k = text.find(fn_args_s)\n",
        "    \n",
        "    if i > 0:\n",
        "        _text = text[i + len(fn_name_s):]\n",
        "        j = _text.find(fn_name_e)\n",
        "        if j > -1:\n",
        "            fn_name = _text[:j]\n",
        "    \n",
        "    if k > 0:\n",
        "        fn_args = text[k + len(fn_args_s):]\n",
        "    \n",
        "    fn_args = fn_args.strip()\n",
        "    if len(fn_args) > 2:\n",
        "        fn_args = fn_args[:-1]\n",
        "    else:\n",
        "        fn_args = ''\n",
        "    \n",
        "    return fn_name, fn_args\n",
        "\n",
        "\n",
        "def postprocess_ai_message(\n",
        "    ai_message: AIMessage,\n",
        ") -> AIMessage:\n",
        "    \"\"\"\n",
        "    Convert AIMessage with <tool_call> tags to proper LangChain message with tool calls and leave it in a list to integrate with MessagesState.\n",
        "    Assumes all content is text (no multimodal).\n",
        "    \"\"\"\n",
        "    tool_id = 1\n",
        "    \n",
        "    content: str = ai_message.content if isinstance(ai_message.content, str) else str(ai_message.content)\n",
        "    \n",
        "    # Handle <think> tags - skip tool call parsing inside thinking\n",
        "    if '<think>' in content:\n",
        "        if '</think>' not in content:\n",
        "            # Incomplete thinking, add as regular message\n",
        "            return ai_message\n",
        "        \n",
        "        # Split thinking from rest of content\n",
        "        parts = content.split('</think>')\n",
        "        content = parts[-1]\n",
        "        \n",
        "    \n",
        "    # Find tool calls in content\n",
        "    if '<tool_call>' not in content:\n",
        "        # No tool calls, add as regular message\n",
        "        return AIMessage(content=content.strip())\n",
        "    \n",
        "    # Split content by tool calls\n",
        "    tool_call_list = content.split('<tool_call>')\n",
        "    pre_text = tool_call_list[0].strip()\n",
        "    tool_calls: List[ToolCall] = []\n",
        "    \n",
        "    # Process each tool call\n",
        "    for txt in tool_call_list[1:]:\n",
        "        if not txt.strip():\n",
        "            continue\n",
        "        \n",
        "        # Handle incomplete tool calls (no closing tag)\n",
        "        if '</tool_call>' not in txt:\n",
        "            fn_name, fn_args = extract_fn(txt)\n",
        "            if fn_name:\n",
        "                tool_calls.append(\n",
        "                    ToolCall(\n",
        "                        name=fn_name,\n",
        "                        args=json.loads(fn_args) if fn_args else {},\n",
        "                        id=str(tool_id),\n",
        "                    )\n",
        "                )\n",
        "                tool_id += 1\n",
        "                # new_messages.append(AIMessage(content='', tool_calls=tool_calls))\n",
        "            continue\n",
        "        \n",
        "        # Handle complete tool calls\n",
        "        one_tool_call_txt = txt.split('</tool_call>')[0].strip()\n",
        "        \n",
        "        try:\n",
        "            # Try to parse as JSON\n",
        "            fn = json5.loads(one_tool_call_txt)\n",
        "            if 'name' in fn and 'arguments' in fn:\n",
        "                tool_calls.append(\n",
        "                    ToolCall(\n",
        "                        name=fn['name'],\n",
        "                        args=fn['arguments'],\n",
        "                        id=str(tool_id),\n",
        "                    )\n",
        "                )\n",
        "                tool_id += 1\n",
        "                # new_messages.append(AIMessage(content='', tool_calls=tool_calls))\n",
        "        except Exception:\n",
        "            # Fallback to manual extraction\n",
        "            fn_name, fn_args = extract_fn(one_tool_call_txt)\n",
        "            if fn_name:\n",
        "                tool_calls.append(\n",
        "                    ToolCall(\n",
        "                        name=fn_name,\n",
        "                        args=json.loads(fn_args) if fn_args else {},\n",
        "                        id=str(tool_id),\n",
        "                    )\n",
        "                )\n",
        "                tool_id += 1\n",
        "                # new_messages.append(AIMessage(content='', tool_calls=tool_calls))\n",
        "        \n",
        "    if tool_calls:\n",
        "        return AIMessage(content=pre_text, tool_calls=tool_calls)\n",
        "    elif pre_text:\n",
        "        return AIMessage(content=pre_text)\n",
        "    else:\n",
        "        return AIMessage(content=content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5668ae44",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_today_date_en() -> str:\n",
        "    \"\"\"Get today's date formatted for system message.\"\"\"\n",
        "    today = datetime.today()\n",
        "    day_names = [\n",
        "        \"Monday\",\n",
        "        \"Tuesday\",\n",
        "        \"Wednesday\",\n",
        "        \"Thursday\",\n",
        "        \"Friday\",\n",
        "        \"Saturday\",\n",
        "        \"Sunday\",\n",
        "    ]\n",
        "    day_of_week = day_names[today.weekday()]\n",
        "    month_name_full = today.strftime(\"%B\")\n",
        "    if today.day % 10 == 1 and today.day != 11:\n",
        "        day_suffix = \"st\"\n",
        "    elif today.day % 10 == 2 and today.day != 12:\n",
        "        day_suffix = \"nd\"\n",
        "    elif today.day % 10 == 3 and today.day != 13:\n",
        "        day_suffix = \"rd\"\n",
        "    else:\n",
        "        day_suffix = \"th\"\n",
        "    return f\"{day_of_week}, {month_name_full} {today.day}{day_suffix}, {today.year}\"\n",
        "\n",
        "\n",
        "def get_today_date_vi() -> str:\n",
        "    today = datetime.today()\n",
        "    day_names = [\n",
        "        \"Thứ hai\",\n",
        "        \"Thứ ba\",\n",
        "        \"Thứ tư\",\n",
        "        \"Thứ năm\",\n",
        "        \"Thứ sáu\",\n",
        "        \"Thứ bảy\",\n",
        "        \"Chủ nhật\",\n",
        "    ]\n",
        "    day_of_week = day_names[today.weekday()]\n",
        "    return f\"{day_of_week}, ngày {today.day}, tháng {today.month}, năm {today.year}\"\n",
        "\n",
        "\n",
        "def preprocess_messages(\n",
        "    state: BaseModel,\n",
        "    system_prompt: str,\n",
        ") -> List[AnyMessage]:\n",
        "    \"\"\"\n",
        "    Convert LangChain messages with tool calls to plaintext format with <tool_call> tags.\n",
        "    Converts ToolMessages to <tool_response> tags.\n",
        "    Assumes all content is text (no multimodal).\n",
        "    \"\"\"\n",
        "    if \"messages\" not in state:\n",
        "        raise ValueError(\"messages not found in state\")\n",
        "    messages: List[AnyMessage] = state[\"messages\"]\n",
        "    new_messages = []\n",
        "\n",
        "    if messages[0].type == \"system\":\n",
        "        new_messages.append(messages[0])\n",
        "    else:\n",
        "        date_info = \"Hôm nay là {date}.\\n\".format(date=get_today_date_vi())\n",
        "        new_messages.append(SystemMessage(\n",
        "            content=date_info + system_prompt\n",
        "        ))\n",
        "        messages = [SystemMessage(content=date_info + system_prompt)] + messages\n",
        "\n",
        "    for msg in messages[1:]:\n",
        "        # Pass through human messages as-is\n",
        "        if msg.type == \"human\":\n",
        "            new_messages.append(msg)\n",
        "            continue\n",
        "        # Handle AI messages with tool calls\n",
        "        elif msg.type == \"ai\":\n",
        "            content = msg.content if isinstance(msg.content, str) else str(msg.content)\n",
        "            \n",
        "            # Convert tool calls to plaintext format\n",
        "            if msg.tool_calls:\n",
        "                for tool_call in msg.tool_calls:\n",
        "                    fc = {\n",
        "                        'name': tool_call['name'],\n",
        "                        'arguments': tool_call['args']\n",
        "                    }\n",
        "                    fc_str = json.dumps(fc, ensure_ascii=False)\n",
        "                    tool_call_text = f'<tool_call>\\n{fc_str}\\n</tool_call>'\n",
        "                    \n",
        "                    # Append to content\n",
        "                    if content:\n",
        "                        content += '\\n' + tool_call_text\n",
        "                    else:\n",
        "                        content = tool_call_text\n",
        "            \n",
        "            # Merge consecutive AI messages\n",
        "            if new_messages and new_messages[-1].type == \"ai\":\n",
        "                prev_content = new_messages[-1].content\n",
        "                if prev_content and not prev_content.endswith('\\n'):\n",
        "                    prev_content += '\\n'\n",
        "                new_messages[-1] = AIMessage(content=prev_content + content)\n",
        "            else:\n",
        "                new_messages.append(AIMessage(content=content))\n",
        "            continue\n",
        "        # Handle tool messages - convert to <tool_response> wrapped in HumanMessage\n",
        "        elif msg.type == \"tool\":\n",
        "            content = msg.content if isinstance(msg.content, str) else str(msg.content)\n",
        "            response_text = f'<tool_response>\\n{content}\\n</tool_response>'\n",
        "            if new_messages and new_messages[-1].type == \"human\":\n",
        "                prev_content = new_messages[-1].content\n",
        "                prev_content += '\\n' + response_text\n",
        "                new_messages[-1] = HumanMessage(content=prev_content)\n",
        "            else:\n",
        "                new_messages.append(HumanMessage(content=response_text))\n",
        "            continue\n",
        "    \n",
        "    return new_messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0fd1de5",
      "metadata": {},
      "source": [
        "# SQLite Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e4d5e6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import asyncio\n",
        "from typing import Any, Dict, Iterable, List, Literal, Sequence, Tuple, Union, Optional\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sqlalchemy import (\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    create_engine,\n",
        "    inspect,\n",
        "    text,\n",
        ")\n",
        "from sqlalchemy.engine import Engine, Result\n",
        "from sqlalchemy.exc import ProgrammingError, SQLAlchemyError\n",
        "from sqlalchemy.types import NullType\n",
        "\n",
        "from langchain_core.embeddings import Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "bbac486b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def truncate_word(content: Any, *, length: int, suffix: str = \"...\") -> str:\n",
        "    \"\"\"Truncate a string to a certain number of words, based on the max string length.\"\"\"\n",
        "    if not isinstance(content, str) or length <= 0:\n",
        "        return content\n",
        "    if len(content) <= length:\n",
        "        return content\n",
        "    return content[: length - len(suffix)].rsplit(\" \", 1)[0] + suffix\n",
        "\n",
        "\n",
        "def _safe_filename(name: str) -> str:\n",
        "    \"\"\"Make a reasonably safe filename from table/column names.\"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        name = str(name)\n",
        "    # Keep unicode but remove path separators and problematic chars\n",
        "    name = name.replace(os.sep, \"_\").replace(\"\\x00\", \"_\")\n",
        "    name = re.sub(r\"[<>:\\\"/\\\\|?*\\n\\r\\t]+\", \"_\", name).strip()\n",
        "    return name or \"unnamed\"\n",
        "\n",
        "\n",
        "class SQLiteDatabase:\n",
        "    \"\"\"SQLAlchemy wrapper around a SQLite database with column comments support.\"\"\"\n",
        "\n",
        "    def _render_type(self, col_type: Any, *, default: str = \"TEXT\") -> str:\n",
        "        \"\"\"Render SQLAlchemy type using this engine's dialect when possible.\"\"\"\n",
        "        if col_type is None or isinstance(col_type, NullType):\n",
        "            return default\n",
        "        try:\n",
        "            compiled = col_type.compile(dialect=self._engine.dialect)\n",
        "            if isinstance(compiled, str) and compiled.strip():\n",
        "                return compiled.strip()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            rendered = str(col_type)\n",
        "            return rendered.strip() if rendered.strip() else default\n",
        "        except Exception:\n",
        "            return default\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        engine: Engine,\n",
        "        ignore_tables: Optional[List[str]] = None,\n",
        "        include_tables: Optional[List[str]] = None,\n",
        "        indexes_in_table_info: bool = False,\n",
        "        max_string_length: int = 200,\n",
        "        lazy_table_reflection: bool = False,\n",
        "        faiss_dir: Optional[str] = None,\n",
        "        embeddings: Optional[Embeddings] = None,\n",
        "        concurrency_limit: int = 10,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create SQLite database wrapper.\n",
        "        \n",
        "        Args:\n",
        "            engine: SQLAlchemy engine connected to SQLite database\n",
        "            ignore_tables: List of table names to ignore\n",
        "            include_tables: List of table names to include (mutually exclusive with ignore_tables)\n",
        "            indexes_in_table_info: Whether to include index information in table info\n",
        "            max_string_length: Maximum string length for truncating values\n",
        "            lazy_table_reflection: Whether to lazily reflect tables\n",
        "            faiss_dir: Root directory that stores FAISS artifacts (see create_faiss)\n",
        "            embeddings: Optional pre-initialized InfinityEmbeddings instance to reuse\n",
        "            embed_model: Model name for InfinityEmbeddings (used when embeddings is None)\n",
        "            infinity_api_url: Infinity API endpoint (used when embeddings is None)\n",
        "        \"\"\"\n",
        "        self._engine = engine\n",
        "        if self._engine.dialect.name != \"sqlite\":\n",
        "            raise ValueError(\"SQLiteDatabase only supports SQLite databases\")\n",
        "        \n",
        "        if include_tables and ignore_tables:\n",
        "            raise ValueError(\"Cannot specify both include_tables and ignore_tables\")\n",
        "\n",
        "        self._inspector = inspect(self._engine)\n",
        "        self._all_tables = set(self._inspector.get_table_names())\n",
        "\n",
        "        self._include_tables = set(include_tables) if include_tables else set()\n",
        "        if self._include_tables:\n",
        "            missing_tables = self._include_tables - self._all_tables\n",
        "            if missing_tables:\n",
        "                raise ValueError(f\"include_tables {missing_tables} not found in database\")\n",
        "        \n",
        "        self._ignore_tables = set(ignore_tables) if ignore_tables else set()\n",
        "        if self._ignore_tables:\n",
        "            missing_tables = self._ignore_tables - self._all_tables\n",
        "            if missing_tables:\n",
        "                raise ValueError(f\"ignore_tables {missing_tables} not found in database\")\n",
        "        \n",
        "        usable_tables = self.get_usable_table_names()\n",
        "        self._usable_tables = set(usable_tables) if usable_tables else self._all_tables\n",
        "\n",
        "        self._indexes_in_table_info = indexes_in_table_info\n",
        "        self._max_string_length = max_string_length\n",
        "        self._faiss_dir = faiss_dir\n",
        "        self._faiss_indexes: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
        "        self._faiss_embeddings = embeddings\n",
        "        self._semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "\n",
        "        self._metadata = MetaData()\n",
        "        if not lazy_table_reflection:\n",
        "            self._metadata.reflect(\n",
        "                bind=self._engine,\n",
        "                only=list(self._usable_tables),\n",
        "            )\n",
        "\n",
        "        if self._faiss_dir:\n",
        "            self._load_faiss_indexes()\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_uri(\n",
        "        cls,\n",
        "        database_uri: str,\n",
        "        engine_args: Optional[dict] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"SQLiteDatabase\":\n",
        "        \"\"\"Construct a SQLiteDatabase from URI.\"\"\"\n",
        "        _engine_args = engine_args or {}\n",
        "        return cls(create_engine(database_uri, **_engine_args), **kwargs)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def dialect(self) -> str:\n",
        "        \"\"\"Return string representation of dialect to use.\"\"\"\n",
        "        return \"SQLite\"\n",
        "\n",
        "\n",
        "    def get_usable_table_names(self) -> Iterable[str]:\n",
        "        \"\"\"Get names of tables available.\"\"\"\n",
        "        if self._include_tables:\n",
        "            base = set(self._include_tables)\n",
        "        else:\n",
        "            base = self._all_tables - self._ignore_tables\n",
        "\n",
        "        # filter out metadata tables (companion EAV tables)\n",
        "        base = {tbl for tbl in base if not tbl.endswith(\"__metadata\")}\n",
        "        return sorted(base)\n",
        "\n",
        "\n",
        "    def get_column_datatype(\n",
        "        self,\n",
        "        table_name: str,\n",
        "        column_name: str,\n",
        "        default: str = \"TEXT\",\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Return SQL datatype for a column in a table.\n",
        "\n",
        "        Notes:\n",
        "        - Uses SQLAlchemy inspector, so it does not require table reflection.\n",
        "        - Returns `default` when the table/column is not found or the type is unknown.\n",
        "        \"\"\"\n",
        "        all_table_names = set(self.get_usable_table_names())\n",
        "        if table_name not in all_table_names:\n",
        "            raise ValueError(\n",
        "                f\"Table '{table_name}' not found in database. Available tables: {sorted(all_table_names)}\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            cols = self._inspector.get_columns(table_name)\n",
        "        except SQLAlchemyError:\n",
        "            return default\n",
        "\n",
        "        for col in cols:\n",
        "            if col.get(\"name\") != column_name:\n",
        "                continue\n",
        "            col_type = col.get(\"type\")\n",
        "            return self._render_type(col_type, default=default)\n",
        "\n",
        "        return default\n",
        "\n",
        "\n",
        "    def get_column_names(self, table_name: str) -> List[str] | None:\n",
        "        \"\"\"\n",
        "        Return the names of columns in a table.\n",
        "        \"\"\"\n",
        "        all_table_names = list(self.get_usable_table_names())\n",
        "        if table_name not in all_table_names:\n",
        "            raise ValueError(f\"Table '{table_name}' not found in database. Available tables: {all_table_names}\")\n",
        "        try:\n",
        "            column_names = []\n",
        "            for col in self._inspector.get_columns(table_name):\n",
        "                col_name = col.get(\"name\")\n",
        "                if isinstance(col_name, str):\n",
        "                    column_names.append(col_name)\n",
        "            return column_names\n",
        "        except SQLAlchemyError:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def get_table_info(\n",
        "        self,\n",
        "        table_name: str,\n",
        "        get_col_comments: bool = False,\n",
        "        allowed_col_names: Optional[List[str]] = None,\n",
        "        sample_count: Optional[int] = None,\n",
        "        column_sample_values: Optional[Dict[str, List[str]]] = None,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Get information about a specified table.\n",
        "\n",
        "        Args:\n",
        "            table_name: Name of the table to get info for\n",
        "            get_col_comments: Whether to include column comments in the output\n",
        "            allowed_col_names: If provided, only include these columns in the output.\n",
        "                              If None, include all columns.\n",
        "            sample_count: Number of distinct example values to include for each column.\n",
        "                          If None, no example values are included (unless provided via\n",
        "                          column_sample_values).\n",
        "            column_sample_values: Optional mapping from column name to a list of\n",
        "                          precomputed example values. For columns present in this\n",
        "                          mapping, these values are used directly. For remaining\n",
        "                          columns, example values are fetched via `_get_sample_values`\n",
        "                          when sample_count is provided.\n",
        "\n",
        "        Returns:\n",
        "            String containing table schema (CREATE TABLE statement) and optionally\n",
        "            column comments and sample rows.\n",
        "        \"\"\"\n",
        "        all_table_names = list(self.get_usable_table_names())\n",
        "        if table_name not in all_table_names:\n",
        "            raise ValueError(f\"Table '{table_name}' not found in database. Available tables: {all_table_names}\")\n",
        "\n",
        "        # Ensure table is reflected\n",
        "        metadata_table_names = [tbl.name for tbl in self._metadata.sorted_tables]\n",
        "        if table_name not in metadata_table_names:\n",
        "            self._metadata.reflect(\n",
        "                bind=self._engine,\n",
        "                only=[table_name],\n",
        "            )\n",
        "\n",
        "        # Find the table object\n",
        "        table = None\n",
        "        for tbl in self._metadata.sorted_tables:\n",
        "            if tbl.name == table_name:\n",
        "                table = tbl\n",
        "                break\n",
        "\n",
        "        if table is None:\n",
        "            raise ValueError(f\"Table '{table_name}' could not be reflected\")\n",
        "\n",
        "        # Remove NullType columns\n",
        "        try:\n",
        "            for _, v in table.columns.items():\n",
        "                if type(v.type) is NullType:\n",
        "                    table._columns.remove(v)\n",
        "        except AttributeError:\n",
        "            for _, v in dict(table.columns).items():\n",
        "                if type(v.type) is NullType:\n",
        "                    table._columns.remove(v)\n",
        "\n",
        "        # Filter columns if allowed_col_names is specified\n",
        "        display_columns = list(table.columns) if not allowed_col_names else [col for col in table.columns if col.name in allowed_col_names]\n",
        "        if not display_columns:\n",
        "            raise ValueError(f\"No matching columns found. Requested: {allowed_col_names}\")\n",
        "\n",
        "        # Get sample values for columns:\n",
        "        # - Prefer values passed in via column_sample_values for those columns.\n",
        "        # - For remaining columns, fetch values via _get_sample_values when sample_count is set.\n",
        "        provided_sample_values: Dict[str, List[str]] = column_sample_values or {}\n",
        "        fetched_sample_values: Dict[str, List[str]] = {}\n",
        "        if sample_count:\n",
        "            columns_to_fetch = [\n",
        "                col for col in display_columns if col.name not in provided_sample_values\n",
        "            ]\n",
        "            if columns_to_fetch:\n",
        "                fetched_sample_values = self._get_sample_values(\n",
        "                    table, columns_to_fetch, sample_count\n",
        "                )\n",
        "            provided_sample_values = {\n",
        "                col_name: values[:sample_count]\n",
        "                for col_name, values in provided_sample_values.items()\n",
        "            }\n",
        "\n",
        "        # Merge, giving precedence to explicitly provided sample values\n",
        "        column_sample_values = {**fetched_sample_values, **provided_sample_values}\n",
        "\n",
        "        # Build custom CREATE TABLE statement with filtered columns\n",
        "        col_defs = []\n",
        "        column_descriptions = (\n",
        "            self._get_column_descriptions_from_metadata(table_name)\n",
        "            if get_col_comments\n",
        "            else {}\n",
        "        )\n",
        "        for col in display_columns:\n",
        "            col_type = self._render_type(col.type, default=\"TEXT\")\n",
        "            col_def = f'\\t\"{col.name}\" {col_type}'\n",
        "            \n",
        "            # Build comment with description and example values\n",
        "            comment_parts = []\n",
        "            col_cmt = column_descriptions.get(col.name, \"\")\n",
        "            if col_cmt:\n",
        "                comment_parts.append(col_cmt)\n",
        "            \n",
        "            # Add sample values if available\n",
        "            if col.name in column_sample_values and column_sample_values[col.name]:\n",
        "                raw_sample_values = column_sample_values[col.name]\n",
        "                display_values: List[str] = []\n",
        "                for sample in raw_sample_values:\n",
        "                    # Normalize to string and ensure string values are quoted,\n",
        "                    # unless they already appear quoted, to match _get_sample_values.\n",
        "                    if isinstance(sample, str):\n",
        "                        val_str = sample\n",
        "                        if not (val_str.startswith('\"') and val_str.endswith('\"')):\n",
        "                            val_str = f'\"{val_str}\"'\n",
        "                    else:\n",
        "                        val_str = str(sample)\n",
        "                    display_values.append(val_str)\n",
        "\n",
        "                examples_str = \", \".join(display_values)\n",
        "                comment_parts.append(f\"Một vài (không phải tất cả) giá trị trong cột \\\"{col.name}\\\": {examples_str},...\")\n",
        "            \n",
        "            if comment_parts:\n",
        "                comment_text = \" \".join(comment_parts)\n",
        "                col_def = f\"{col_def}\\t/* {comment_text} */\"\n",
        "            \n",
        "            col_defs.append(col_def)\n",
        "\n",
        "        col_defs.sort()        \n",
        "        create_table = f'CREATE TABLE \"{table_name}\" (\\n' + \", \\n\".join(col_defs) + \"\\n)\"\n",
        "\n",
        "        table_info = f\"{create_table.rstrip()}\"\n",
        "            \n",
        "        # Add indexes if needed\n",
        "        if self._indexes_in_table_info:\n",
        "            table_info += \"\\n\\n/*\"\n",
        "            table_info += f\"\\n{self._get_table_indexes(table)}\\n\"\n",
        "            table_info += \"*/\"\n",
        "\n",
        "        return table_info\n",
        "\n",
        "\n",
        "    def _get_column_descriptions_from_metadata(\n",
        "        self, table_name: str\n",
        "    ) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Fetch column descriptions from the metadata EAV table created alongside the data table.\n",
        "\n",
        "        Expects a companion table named \"{table_name}__metadata\" with rows:\n",
        "            entity = column name\n",
        "            attribute = \"description\"\n",
        "            value = description text\n",
        "        \"\"\"\n",
        "        metadata_table = f\"{table_name}__metadata\"\n",
        "        if metadata_table not in self._all_tables:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            query = text(\n",
        "                f'SELECT entity, value FROM \"{metadata_table}\" WHERE attribute = :attr'\n",
        "            )\n",
        "            with self._engine.connect() as connection:\n",
        "                result: Result = connection.execute(query, {\"attr\": \"description\"})\n",
        "                return {row[0]: row[1] for row in result if row[1] is not None}\n",
        "        except (ProgrammingError, SQLAlchemyError):\n",
        "            return {}\n",
        "\n",
        "\n",
        "    def get_column_groups(self, table_name: str) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Return column groups for a table based on its metadata companion table.\n",
        "\n",
        "        Reads rows where attribute == \"group\" from \"{table_name}__metadata\" and\n",
        "        builds a list of column-name lists, ordered by group id.\n",
        "        \"\"\"\n",
        "        metadata_table = f\"{table_name}__metadata\"\n",
        "        if metadata_table not in self._all_tables:\n",
        "            return []\n",
        "\n",
        "        groups: Dict[int, List[str]] = {}\n",
        "        try:\n",
        "            query = text(\n",
        "                f'SELECT entity, value FROM \"{metadata_table}\" WHERE attribute = :attr'\n",
        "            )\n",
        "            with self._engine.connect() as connection:\n",
        "                result: Result = connection.execute(query, {\"attr\": \"group\"})\n",
        "                for entity, value in result:\n",
        "                    if value is None:\n",
        "                        continue\n",
        "                    try:\n",
        "                        group_id = int(value)\n",
        "                    except (TypeError, ValueError):\n",
        "                        continue\n",
        "                    groups.setdefault(group_id, []).append(entity)\n",
        "        except (ProgrammingError, SQLAlchemyError):\n",
        "            return []\n",
        "\n",
        "        if not groups:\n",
        "            return []\n",
        "\n",
        "        return [groups[idx] for idx in sorted(groups.keys())]\n",
        "\n",
        "\n",
        "    def _get_table_indexes(self, table: Table) -> str:\n",
        "        \"\"\"Get formatted index information for a table.\"\"\"\n",
        "        indexes = self._inspector.get_indexes(table.name)\n",
        "        indexes_formatted = \"\\n\".join(\n",
        "            f\"Name: {idx['name']}, Unique: {idx['unique']}, Columns: {idx['column_names']}\"\n",
        "            for idx in indexes\n",
        "        )\n",
        "        return f\"Table Indexes:\\n{indexes_formatted}\"\n",
        "\n",
        "\n",
        "    def _get_sample_values(\n",
        "        self,\n",
        "        table: Table,\n",
        "        columns: List[Column],\n",
        "        sample_count: int,\n",
        "    ) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Get up to sample_count distinct example values per column.\n",
        "\n",
        "        Strings are quoted to reflect their type. Values longer than 100 chars are skipped.\n",
        "        \"\"\"\n",
        "        if sample_count <= 0:\n",
        "            return {}\n",
        "\n",
        "        column_sample_values: Dict[str, List[str]] = {col.name: [] for col in columns}\n",
        "        for col in columns:\n",
        "            query = text(\n",
        "                f'SELECT DISTINCT \"{col.name}\" '\n",
        "                f'FROM \"{table.name}\" '\n",
        "                f'WHERE \"{col.name}\" IS NOT NULL '\n",
        "                f\"LIMIT {sample_count}\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                with self._engine.connect() as connection:\n",
        "                    result = connection.execute(query)\n",
        "                    remaining_length = 1000\n",
        "                    for val, in result:\n",
        "                        val_str = str(val)\n",
        "                        # Represent type: quote strings, leave others as-is\n",
        "                        display_val = f'\"{val_str}\"' if isinstance(val, str) else val_str\n",
        "                        column_sample_values[col.name].append(display_val)\n",
        "                        remaining_length -= len(display_val)\n",
        "                        if remaining_length <= 0:\n",
        "                            break\n",
        "\n",
        "            except ProgrammingError:\n",
        "                continue\n",
        "\n",
        "        return column_sample_values\n",
        "\n",
        "\n",
        "    def _execute(\n",
        "        self,\n",
        "        command: str,\n",
        "        fetch: Literal[\"all\", \"one\", \"cursor\"] = \"all\",\n",
        "        *,\n",
        "        parameters: Optional[Dict[str, Any]] = None,\n",
        "        execution_options: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Union[Sequence[Dict[str, Any]], Result]:\n",
        "        \"\"\"Execute SQL command through underlying engine.\"\"\"\n",
        "        parameters = parameters or {}\n",
        "        execution_options = execution_options or {}\n",
        "        \n",
        "        with self._engine.begin() as connection:\n",
        "            cursor = connection.execute(\n",
        "                text(command),\n",
        "                parameters,\n",
        "                execution_options=execution_options,\n",
        "            )\n",
        "\n",
        "            if cursor.returns_rows:\n",
        "                if fetch == \"all\":\n",
        "                    result = [x._asdict() for x in cursor.fetchall()]\n",
        "                elif fetch == \"one\":\n",
        "                    first_result = cursor.fetchone()\n",
        "                    result = [] if first_result is None else [first_result._asdict()]\n",
        "                elif fetch == \"cursor\":\n",
        "                    return cursor\n",
        "                else:\n",
        "                    raise ValueError(\"Fetch parameter must be either 'one', 'all', or 'cursor'\")\n",
        "                return result\n",
        "        return []\n",
        "\n",
        "\n",
        "    def _run_sync(\n",
        "        self,\n",
        "        command: str,\n",
        "        fetch: Literal[\"all\", \"one\", \"cursor\"],\n",
        "        include_columns: bool,\n",
        "        parameters: Optional[Dict[str, Any]],\n",
        "        execution_options: Optional[Dict[str, Any]],\n",
        "    ) -> Union[Sequence[Dict[str, Any]], Sequence[Tuple[Any, ...]], Result[Any]]:\n",
        "        \"\"\"\n",
        "        Helper method containing the synchronous logic for `run`.\n",
        "        This handles the CPU-bound result formatting after the IO-bound execution.\n",
        "        \"\"\"\n",
        "        result = self._execute(\n",
        "            command, fetch, parameters=parameters, execution_options=execution_options\n",
        "        )\n",
        "\n",
        "        if fetch == \"cursor\":\n",
        "            return result\n",
        "\n",
        "        if include_columns:\n",
        "            return [\n",
        "                {\n",
        "                    column: truncate_word(value, length=self._max_string_length)\n",
        "                    for column, value in r.items()\n",
        "                }\n",
        "                for r in result\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                tuple(\n",
        "                    truncate_word(value, length=self._max_string_length)\n",
        "                    for value in r.values()\n",
        "                )\n",
        "                for r in result\n",
        "            ]\n",
        "\n",
        "\n",
        "    async def run(\n",
        "        self,\n",
        "        command: str,\n",
        "        fetch: Literal[\"all\", \"one\", \"cursor\"] = \"all\",\n",
        "        include_columns: bool = False,\n",
        "        *,\n",
        "        parameters: Optional[Dict[str, Any]] = None,\n",
        "        execution_options: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Union[Sequence[Dict[str, Any]], Sequence[Tuple[Any, ...]], Result[Any]]:\n",
        "        \"\"\"\n",
        "        Execute a SQL command asynchronously.\n",
        "        Offloads the blocking SQLAlchemy call to a separate thread.\n",
        "        \"\"\"\n",
        "        await self._semaphore.acquire()\n",
        "        try:\n",
        "            return await asyncio.to_thread(\n",
        "                self._run_sync,\n",
        "                command,\n",
        "                fetch,\n",
        "                include_columns,\n",
        "                parameters,\n",
        "                execution_options,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            self._semaphore.release()\n",
        "\n",
        "\n",
        "    async def run_no_throw(\n",
        "        self,\n",
        "        command: str,\n",
        "        fetch: Literal[\"all\", \"one\"] = \"all\",\n",
        "        include_columns: bool = False,\n",
        "        *,\n",
        "        parameters: Optional[Dict[str, Any]] = None,\n",
        "        execution_options: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Execute a SQL command and return results or error message.\"\"\"\n",
        "        try:\n",
        "            res = await self.run(\n",
        "                command,\n",
        "                fetch,\n",
        "                parameters=parameters,\n",
        "                execution_options=execution_options,\n",
        "                include_columns=include_columns,\n",
        "            )\n",
        "            return {\n",
        "                \"result\": res,\n",
        "                \"error\": None,\n",
        "            }\n",
        "        except SQLAlchemyError as e:\n",
        "            return {\n",
        "                \"result\": [],\n",
        "                \"error\": f\"Error: {e}\",\n",
        "            }\n",
        "\n",
        "\n",
        "    def get_table_info_no_throw(\n",
        "        self,\n",
        "        table_name: str,\n",
        "        get_col_comments: bool = False,\n",
        "        allowed_col_names: Optional[List[str]] = None,\n",
        "        sample_count: Optional[int] = None,\n",
        "        column_sample_values: Optional[Dict[str, List[str]]] = None,\n",
        "    ) -> str:\n",
        "        \"\"\"Get table info without throwing exceptions.\"\"\"\n",
        "        try:\n",
        "            return self.get_table_info(\n",
        "                table_name,\n",
        "                get_col_comments=get_col_comments,\n",
        "                allowed_col_names=allowed_col_names,\n",
        "                sample_count=sample_count,\n",
        "                column_sample_values=column_sample_values,\n",
        "            )\n",
        "        except ValueError as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "    def get_context(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return db context that you may want in agent prompt.\"\"\"\n",
        "        table_names = list(self.get_usable_table_names())\n",
        "        # Get info for all tables\n",
        "        table_infos = []\n",
        "        for tbl in table_names:\n",
        "            table_infos.append(self.get_table_info_no_throw(tbl))\n",
        "        table_info = \"\\n\\n\".join(table_infos)\n",
        "        return {\"table_info\": table_info, \"table_names\": \", \".join(table_names)}\n",
        "\n",
        "\n",
        "    def _load_faiss_indexes(self) -> None:\n",
        "        \"\"\"Eagerly load FAISS indexes and their value mappings if the directory is present.\"\"\"\n",
        "        if not self._faiss_dir:\n",
        "            return\n",
        "\n",
        "        for table_name in self._usable_tables:\n",
        "            table_dir = os.path.join(\n",
        "                self._faiss_dir,\n",
        "                _safe_filename(table_name),\n",
        "            )\n",
        "            if not os.path.isdir(table_dir):\n",
        "                continue\n",
        "\n",
        "            # Build a lookup of column names to quickly check candidate files.\n",
        "            try:\n",
        "                columns = {c[\"name\"] for c in self._inspector.get_columns(table_name)}\n",
        "            except SQLAlchemyError:\n",
        "                continue\n",
        "\n",
        "            for col_name in columns:\n",
        "                index_path = os.path.join(\n",
        "                    table_dir, f\"{_safe_filename(col_name)}.faiss\"\n",
        "                )\n",
        "                values_path = os.path.join(\n",
        "                    table_dir, f\"{_safe_filename(col_name)}.json\"\n",
        "                )\n",
        "                if not (os.path.exists(index_path) and os.path.exists(values_path)):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    index = faiss.read_index(index_path)\n",
        "                    with open(values_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        values = json.load(f) or []\n",
        "                    if not isinstance(values, list):\n",
        "                        continue\n",
        "\n",
        "                    metric = getattr(index, \"metric_type\", None)\n",
        "                    normalize = metric == faiss.METRIC_INNER_PRODUCT\n",
        "\n",
        "                    # Guard against mismatched artifacts.\n",
        "                    if hasattr(index, \"ntotal\") and index.ntotal != len(values):\n",
        "                        # Skip inconsistent artifacts to avoid incorrect lookups.\n",
        "                        continue\n",
        "\n",
        "                    self._faiss_indexes.setdefault(table_name, {})[col_name] = {\n",
        "                        \"index\": index,\n",
        "                        \"values\": values,\n",
        "                        \"normalize\": normalize,\n",
        "                    }\n",
        "                except Exception:\n",
        "                    # Ignore malformed artifacts; consumers can still use SQL methods.\n",
        "                    continue\n",
        "\n",
        "\n",
        "    async def batch_search_similar_predicate_values(\n",
        "        self,\n",
        "        predicate_values: List[Tuple[str, str, str]],\n",
        "        k: int = 5,\n",
        "    ) -> Dict[str, Dict[str, List[str]]]:\n",
        "        \"\"\"\n",
        "        Asynchronously search for similar predicate values for a batch of predicate values.\n",
        "        \n",
        "        Args:\n",
        "            predicate_values: List of (table_name, column_name, value) tuples.\n",
        "            k: Number of nearest neighbors to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Mapping of table_name -> column_name -> list of similar values (no scores).\n",
        "            If an index is missing for a predicate value, that table/column entry is an\n",
        "            empty list.\n",
        "        \"\"\"\n",
        "        if not self._faiss_indexes:\n",
        "            raise ValueError(\"FAISS indexes are not loaded for this database\")\n",
        "        if self._faiss_embeddings is None:\n",
        "            raise ValueError(\"Embeddings client is not configured\")\n",
        "        if not predicate_values:\n",
        "            return {}\n",
        "\n",
        "        # 1. Validation and preparation\n",
        "        valid_queries = []\n",
        "        valid_indices = []\n",
        "        texts_to_embed = []\n",
        "        \n",
        "        # Initialize results mapping table -> column -> list of values\n",
        "        results: Dict[str, Dict[str, List[str]]] = {}\n",
        "        for table, col, _ in predicate_values:\n",
        "            results.setdefault(table, {}).setdefault(col, [])\n",
        "\n",
        "        for i, (table, col, val) in enumerate(predicate_values):\n",
        "            table_indexes = self._faiss_indexes.get(table)\n",
        "            if table_indexes and col in table_indexes:\n",
        "                valid_queries.append((table, col))\n",
        "                valid_indices.append(i)\n",
        "                texts_to_embed.append(str(val))\n",
        "            # Note: Invalid predicate values (no index found) remain [] in the results mapping\n",
        "\n",
        "        if not valid_queries:\n",
        "            return results\n",
        "\n",
        "        # 2. Batch Embedding (I/O Bound)\n",
        "        # Use the async batch embedding method from LangChain\n",
        "        embeddings = await self._get_batch_embeddings(texts_to_embed)\n",
        "\n",
        "        # 3. Parallel FAISS Search (CPU Bound, offloaded to threads)\n",
        "        tasks = []\n",
        "\n",
        "        for i, embedding in zip(valid_indices, embeddings):\n",
        "            table_name, column_name = predicate_values[i][0], predicate_values[i][1]\n",
        "            index_data = self._faiss_indexes[table_name][column_name]\n",
        "            \n",
        "            task = self._execute_search_with_semaphore(\n",
        "                semaphore=self._semaphore,\n",
        "                index_data=index_data,\n",
        "                vector=embedding,\n",
        "                k=k\n",
        "            )\n",
        "            tasks.append(task)\n",
        "\n",
        "        # Wait for all search tasks to complete\n",
        "        search_results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # 4. Map results back to table/column buckets (drop scores)\n",
        "        for original_idx, res in zip(valid_indices, search_results):\n",
        "            table_name, column_name, _ = predicate_values[original_idx]\n",
        "            value_list = [\n",
        "                str(r[\"value\"])\n",
        "                for r in res\n",
        "                if isinstance(r, dict) and \"value\" in r and r[\"value\"] is not None\n",
        "            ]\n",
        "            results[table_name][column_name].extend(value_list)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    async def search_similar_values_from_message(\n",
        "        self,\n",
        "        user_message: str,\n",
        "        linked_schema: Optional[Dict[str, Dict[str, str]]] = None,\n",
        "        k: int = 5,\n",
        "    ) -> Dict[str, Dict[str, List[str]]]:\n",
        "        \"\"\"\n",
        "        Search for similar values across text columns using a user message.\n",
        "        \n",
        "        Embeds the user message once and searches for top k similar values in each\n",
        "        text column that has a FAISS index.\n",
        "        \n",
        "        Args:\n",
        "            user_message: The user's message to search for similar values.\n",
        "            linked_schema: Optional dictionary mapping table_name -> column_name -> column_datatype.\n",
        "                           If None, the schema is automatically constructed from all usable tables.\n",
        "                           Only columns with text datatypes (containing \"TEXT\") will be searched.\n",
        "            k: Number of nearest neighbors to retrieve per column.\n",
        "\n",
        "        Returns:\n",
        "            Mapping of table_name -> column_name -> list of similar values (no scores).\n",
        "            Only includes columns that have text datatypes and FAISS indexes.\n",
        "        \"\"\"\n",
        "        if not self._faiss_indexes:\n",
        "            raise ValueError(\"FAISS indexes are not loaded for this database\")\n",
        "        if self._faiss_embeddings is None:\n",
        "            raise ValueError(\"Embeddings client is not configured\")\n",
        "        if not user_message:\n",
        "            return {}\n",
        "\n",
        "        # If no schema is provided, build it from all usable tables in the database\n",
        "        if linked_schema is None:\n",
        "            linked_schema = {}\n",
        "            for table_name in self.get_usable_table_names():\n",
        "                try:\n",
        "                    column_names = self.get_column_names(table_name) or []\n",
        "                except ValueError:\n",
        "                    # Skip tables that cannot be introspected\n",
        "                    continue\n",
        "\n",
        "                table_schema: Dict[str, str] = {}\n",
        "                for column_name in column_names:\n",
        "                    datatype = self.get_column_datatype(\n",
        "                        table_name, column_name, default=\"TEXT\"\n",
        "                    )\n",
        "                    table_schema[column_name] = datatype\n",
        "\n",
        "                if table_schema:\n",
        "                    linked_schema[table_name] = table_schema\n",
        "\n",
        "        # If schema (either provided or auto-built) is empty, nothing to search\n",
        "        if not linked_schema:\n",
        "            return {}\n",
        "\n",
        "        # 1. Filter columns to only text datatypes and check for FAISS indexes\n",
        "        valid_queries: List[Tuple[str, str]] = []  # (table_name, column_name)\n",
        "        \n",
        "        # Initialize results mapping table -> column -> list of values\n",
        "        results: Dict[str, Dict[str, List[str]]] = {}\n",
        "        \n",
        "        for table_name, columns in linked_schema.items():\n",
        "            for column_name, datatype in columns.items():\n",
        "                # Check if datatype is text (case-insensitive check for \"TEXT\")\n",
        "                if \"TEXT\" not in str(datatype).upper():\n",
        "                    continue\n",
        "                \n",
        "                # Check if FAISS index exists for this column\n",
        "                table_indexes = self._faiss_indexes.get(table_name)\n",
        "                if table_indexes and column_name in table_indexes:\n",
        "                    valid_queries.append((table_name, column_name))\n",
        "                    results.setdefault(table_name, {}).setdefault(column_name, [])\n",
        "\n",
        "        if not valid_queries:\n",
        "            return results\n",
        "\n",
        "        # 2. Embed the user message once (I/O Bound)\n",
        "        embedding = await self._get_batch_embeddings([user_message])\n",
        "        # Extract single embedding from batch result\n",
        "        message_embedding = embedding[0]\n",
        "\n",
        "        # 3. Parallel FAISS Search for all valid columns (CPU Bound, offloaded to threads)\n",
        "        tasks = []\n",
        "        for table_name, column_name in valid_queries:\n",
        "            index_data = self._faiss_indexes[table_name][column_name]\n",
        "            \n",
        "            task = self._execute_search_with_semaphore(\n",
        "                semaphore=self._semaphore,\n",
        "                index_data=index_data,\n",
        "                vector=message_embedding,\n",
        "                k=k\n",
        "            )\n",
        "            tasks.append(task)\n",
        "\n",
        "        # Wait for all search tasks to complete\n",
        "        search_results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # 4. Map results back to table/column buckets (drop scores)\n",
        "        for (table_name, column_name), res in zip(valid_queries, search_results):\n",
        "            value_list = [\n",
        "                str(r[\"value\"])\n",
        "                for r in res\n",
        "                if isinstance(r, dict) and \"value\" in r and r[\"value\"] is not None\n",
        "            ]\n",
        "            results[table_name][column_name].extend(value_list)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    async def _get_batch_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Helper to fetch embeddings asynchronously.\"\"\"\n",
        "        if self._faiss_embeddings is None:\n",
        "            raise ValueError(\"Infinity embeddings client is not configured\")\n",
        "        if not hasattr(self._faiss_embeddings, \"aembed_documents\"):\n",
        "            raise ValueError(\"Infinity embeddings client does not support aembed_documents\")\n",
        "        return await self._faiss_embeddings.aembed_documents(texts)\n",
        "\n",
        "\n",
        "    async def _execute_search_with_semaphore(\n",
        "        self,\n",
        "        semaphore: asyncio.Semaphore,\n",
        "        index_data: Dict[str, Any],\n",
        "        vector: List[float],\n",
        "        k: int\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Acquires semaphore and offloads CPU work to a thread.\"\"\"\n",
        "        await semaphore.acquire()\n",
        "        try:\n",
        "            return await asyncio.to_thread(\n",
        "                self._run_faiss_search_job,\n",
        "                index=index_data[\"index\"],\n",
        "                values=index_data[\"values\"],\n",
        "                normalize=index_data[\"normalize\"],\n",
        "                vector=vector,\n",
        "                k=k\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            semaphore.release()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _run_faiss_search_job(\n",
        "        index: Any,\n",
        "        values: List[str],\n",
        "        normalize: bool,\n",
        "        vector: List[float],\n",
        "        k: int\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Pure CPU-bound static method to perform the FAISS search.\n",
        "        Running this in a separate thread avoids blocking the asyncio event loop.\n",
        "        \"\"\"\n",
        "        x = np.asarray(vector, dtype=\"float32\")\n",
        "        \n",
        "        # Ensure correct shape (1, embedding_dim)\n",
        "        if x.ndim == 1:\n",
        "            x = x.reshape(1, -1)\n",
        "\n",
        "        if normalize:\n",
        "            faiss.normalize_L2(x)\n",
        "\n",
        "        distances, indices = index.search(x, min(k, len(values)))\n",
        "        \n",
        "        results: List[Dict[str, Any]] = []\n",
        "        found_indices = indices[0]\n",
        "        found_distances = distances[0]\n",
        "\n",
        "        for idx, score in zip(found_indices, found_distances):\n",
        "            if idx < 0 or idx >= len(values):\n",
        "                continue\n",
        "            results.append({\n",
        "                \"value\": values[int(idx)],\n",
        "                \"score\": float(score)\n",
        "            })\n",
        "            \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "44b151e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "@lru_cache()\n",
        "def get_sqlite_db(business_name: str):\n",
        "    return SQLiteDatabase.from_uri(\n",
        "        f\"sqlite:////Users/vinhnguyen/Projects/ext-chatbot/resources/database/{business_name}.db\",\n",
        "        faiss_dir=f\"/Users/vinhnguyen/Projects/ext-chatbot/resources/faiss/{business_name}/\",\n",
        "        concurrency_limit=10,\n",
        "        embeddings=get_embedding_model()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "05f79041",
      "metadata": {},
      "outputs": [],
      "source": [
        "db = get_sqlite_db(\"batdongsan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "3b5e1acb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# db.run(\"\"\"\n",
        "# select * from \"BĐS Bán 500\" limit 5;\n",
        "# \"\"\", include_columns=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "3bfc7513",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SQLite'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.dialect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e27e57b8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['BĐS Bán 500', 'BĐS Cho thuê 500']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.get_usable_table_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "353e271d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# db.get_column_groups(\"BĐS Bán 500\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c2b02da9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CREATE TABLE \"BĐS Bán 500\" (\n",
            "\t\"Bãi đỗ xe\" TEXT\t/* Thông tin về khả năng đỗ xe (bao gồm số lượng và loại phương tiện). Một vài (không phải tất cả) giá trị trong cột \"Bãi đỗ xe\": \"Có\", \"Không\", \"Nhiều xe máy\", \"1 ô tô\", \"2 ô tô\",... */, \n",
            "\t\"Chiều dài (m)\" REAL\t/* Độ dài chiều dài của bất động sản tính theo mét. Một vài (không phải tất cả) giá trị trong cột \"Chiều dài (m)\": \"3.9\", \"8.6\", \"16.4\", \"7.7\", \"32.6\",... */\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(db.get_table_info_no_throw(\n",
        "    \"BĐS Bán 500\",\n",
        "    get_col_comments=True,\n",
        "    allowed_col_names=[\"Bãi đỗ xe\", \"Chiều dài (m)\"],\n",
        "    sample_count=5\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fc0b38",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac33b2ff",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f7a7e0",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26208608",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161630a6",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "37598a24",
      "metadata": {},
      "source": [
        "# Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be91324",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = get_llm_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ddf23d",
      "metadata": {},
      "source": [
        "## SQL Chain Components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0f6a4c",
      "metadata": {},
      "source": [
        "### Schema Linking Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e59b6ad6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# table and column selection (run async for multi table / run one time for all tables)\n",
        "# tools: retrieve_values_in_columns, query_database, return_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e7c52bfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "SCHEMA_LINKING_TEMPLATE = \"\"\"\n",
        "You are an expert in SQL schema linking. \n",
        "Given a {dialect} table schema (DDL) and a user query, determine if the table is relevant to the query.\n",
        "\n",
        "Your task:\n",
        "1. Analyze the table schema and the user query to decide if they are related.\n",
        "2. Answer \"Y\" (Yes) or \"N\" (No).\n",
        "3. If the answer is \"Y\", list ALL columns that are semantically related to the query topics. \n",
        "   - You do NOT need to identify the exact columns for the final SQL query. \n",
        "   - You SHOULD include any columns that provide context, identifiers, or potential join keys related to the entities in the query.\n",
        "\n",
        "Output must be a valid JSON object inside a ```json code block using this format:\n",
        "```json\n",
        "{{\n",
        "    \"reasoning\": \"Reasoning of the decision\",\n",
        "    \"is_related\": \"Y or N\",\n",
        "    \"columns\": [\"column name 1\", \"column name 2\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "Table Schema (DDL):\n",
        "{table_info}\n",
        "\n",
        "User Query:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "\n",
        "schema_linking_chain = (\n",
        "    ChatPromptTemplate([(\"human\", SCHEMA_LINKING_TEMPLATE)])\n",
        "    | llm\n",
        "    | JsonOutputParser()\n",
        ")\n",
        "\n",
        "async def _link_schema_one(\n",
        "    query: str,\n",
        "    table_name: str,\n",
        "    allowed_col_names: Optional[List[str]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    try:\n",
        "        table_info = db.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=allowed_col_names,\n",
        "            sample_count=3\n",
        "        )\n",
        "        result = await schema_linking_chain.ainvoke(\n",
        "            {\"table_info\": table_info, \"query\": query, \"dialect\": db.dialect}\n",
        "        )\n",
        "        if \"is_related\" not in result or result[\"is_related\"] not in [\"Y\", \"N\"]:\n",
        "            raise ValueError(\"Invalid response from schema linking chain\")\n",
        "        if result[\"is_related\"] == \"Y\" and not result.get(\"columns\"):\n",
        "            result[\"columns\"] = [\"ROWID\"]\n",
        "\n",
        "        if result[\"is_related\"] == \"N\":\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"query\": query,\n",
        "                    \"allowed_col_names\": allowed_col_names\n",
        "                },\n",
        "                \"filtered_schema\": None,\n",
        "                \"error\": None\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"input_item\": {\"table_name\": table_name, \"query\": query, \"allowed_col_names\": allowed_col_names},\n",
        "                \"filtered_schema\": (table_name, result[\"columns\"]),\n",
        "                \"error\": None\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"input_item\": {\"table_name\": table_name, \"query\": query},\n",
        "            \"filtered_schema\": None,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "async def link_schema(\n",
        "    _input: dict,\n",
        ") -> Dict[str, Dict[str, str]]:\n",
        "    query = _input.get(\"query\")\n",
        "    if not query:\n",
        "        raise ValueError(\"query is required\")\n",
        "    max_retries = _input.get(\"max_retries\", 1)\n",
        "    # queue = []\n",
        "    # for table in  db.get_usable_table_names():\n",
        "    #     for col_group in db.get_column_groups(table):\n",
        "    #         queue.append({\n",
        "    #             \"table_name\": table,\n",
        "    #             \"allowed_col_names\": col_group,\n",
        "    #             \"query\": query\n",
        "    #         })\n",
        "    queue = [{\"table_name\": table_name, \"query\": query} for table_name in db.get_usable_table_names()]\n",
        "    successful_results = []\n",
        "    for _ in range(max_retries):\n",
        "        tasks = [_link_schema_one(**input_item) for input_item in queue]\n",
        "        results = await tqdm_asyncio.gather(*tasks)\n",
        "        successful_results.extend([\n",
        "            res for res in results if res[\"error\"] is None\n",
        "        ])\n",
        "        failed_items = [\n",
        "            res[\"input_item\"] for res in results if res[\"error\"] is not None\n",
        "        ]\n",
        "        queue = failed_items\n",
        "        if not queue:\n",
        "            break\n",
        "    \n",
        "    linked_schema = [\n",
        "        result[\"filtered_schema\"] for result in successful_results if result[\"filtered_schema\"]\n",
        "    ]\n",
        "\n",
        "    # Return per-table mapping: column_name -> datatype\n",
        "    final_schema: Dict[str, Dict[str, str]] = {}\n",
        "    for table_name, col_names in linked_schema:\n",
        "        table_schema = final_schema.setdefault(table_name, {})\n",
        "        for col_name in col_names:\n",
        "            col_type = db.get_column_datatype(\n",
        "                table_name,\n",
        "                col_name,\n",
        "                default=\"NULL\",\n",
        "            )\n",
        "            if col_type != \"NULL\":\n",
        "                table_schema[col_name] = col_type\n",
        "\n",
        "    return final_schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "dffb8b2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:04<00:00,  2.13s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BĐS Cho thuê 500': {'Địa chỉ_đường': 'TEXT',\n",
              "  'Địa chỉ': 'TEXT',\n",
              "  'Phường/Xã': 'TEXT',\n",
              "  'Quận/Huyện': 'TEXT',\n",
              "  'Tỉnh/TP': 'TEXT'}}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Tìm danh sách nhà cho thuê ở trên đường Láng\"\n",
        "linked_schema = await link_schema({\"query\": query})\n",
        "linked_schema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ffda9d",
      "metadata": {},
      "source": [
        "### SQL Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4133f013",
      "metadata": {},
      "outputs": [],
      "source": [
        "SQL_AGENT_PROMPT_TEMPLATE = \"\"\"\n",
        "### DATE INFORMATION:\n",
        "Today is {date}\n",
        "\n",
        "### INSTRUCTIONS:\n",
        "You write SQL queries for a {dialect} database. Users are querying their company database, and your task is to assist by generating valid SQL queries strictly adhering to the database schema provided.\n",
        "\n",
        "**Table Schema**:\n",
        "{table_infos}\n",
        "\n",
        "Translate the user's request into one valid {dialect} query. SQL should be written as a markdown code block:\n",
        "For example:\n",
        "```sql\n",
        "SELECT * FROM table WHERE condition;\n",
        "```\n",
        "\n",
        "### GUIDELINES:\n",
        "\n",
        "1.  **Schema Adherence**:\n",
        "    *   Use only tables, columns, and relationships explicitly listed in the provided schema.\n",
        "    *   Do not make assumptions about missing or inferred columns/tables.\n",
        "\n",
        "2.  **{dialect}-Specific Syntax**:\n",
        "    *   Use only {dialect} syntax. Be aware that {dialect} has limited built-in date/time functions compared to other sql dialects.\n",
        "\n",
        "3.  **Conditions**:\n",
        "    *   Always include default conditions for filtering invalid data, e.g., `deleted_at IS NULL` and `status != 'cancelled'` if relevant.\n",
        "    *   Ensure these conditions match the query's intent unless explicitly omitted in the user request.\n",
        "\n",
        "4.  **Output Consistency**:\n",
        "    *   The output fields must match the query's intent exactly. Do not add extra columns or omit requested fields.\n",
        "\n",
        "5.  **Reserved Keywords and Case Sensitivity**:\n",
        "    *   Escape reserved keywords or case-sensitive identifiers using double quotes (\" \"), e.g., \"order\".\n",
        "\n",
        "If the user's question is ambiguous or unclear, you must make your best reasonable guess based on the schema.\n",
        "Translate the user's intent into a **single valid {dialect} query** based on the schema provided.\n",
        "Ensure the query is optimized, precise, and error-free.\n",
        "\n",
        "**You must ONLY output ONE SINGLE valid SQL query as markdown codeblock.**\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "_sql_markdown_re = re.compile(r\"```sql\\s*([\\s\\S]*?)\\s*```\", re.DOTALL)\n",
        "def parse_sql_output(msg_content: str) -> str:\n",
        "    try:\n",
        "        match = _sql_markdown_re.search(msg_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        else:\n",
        "            raise ValueError(\"No SQL query found in the content\")\n",
        "    except Exception:\n",
        "        return msg_content\n",
        "\n",
        "\n",
        "def preprocess_for_sql_query_generation(\n",
        "    _input: dict,\n",
        ") -> List[AnyMessage]:\n",
        "    linked_schema: Dict[str, Dict[str, str]] = _input.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        db.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=_input.get(\"tbl_col_sample_values\", {}).get(table_name, None),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    system_prompt = SystemMessage(SQL_AGENT_PROMPT_TEMPLATE.format(\n",
        "        table_infos=table_infos,\n",
        "        date=get_today_date_en(),\n",
        "        dialect=db.dialect\n",
        "    ))\n",
        "    human_message = HumanMessage(content=_input[\"query\"])\n",
        "    return [system_prompt, human_message]\n",
        "\n",
        "\n",
        "def get_sql_query_from_content(content: str) -> str:\n",
        "    sql_block_pattern = re.compile(r\"```sql\\s*([\\s\\S]*?)\\s*```\", re.MULTILINE)\n",
        "    match = sql_block_pattern.search(content)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        raise ValueError(\"No SQL query found in the content\")\n",
        "\n",
        "sql_query_generation_chain = (\n",
        "    preprocess_for_sql_query_generation\n",
        "    | get_llm_model()\n",
        "    | StrOutputParser()\n",
        "    | parse_sql_output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "54f5697e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT * FROM \"BĐS Cho thuê 500\" \n",
            "WHERE \"Địa chỉ_đường\" = \"Láng\" OR \"Địa chỉ_đường\" = \"Cầu Giấy\";\n"
          ]
        }
      ],
      "source": [
        "print(sql_query_generation_chain.invoke({\n",
        "    \"query\": \"Tìm danh sách nhà cho thuê ở trên đường Láng hoặc Cầu Giấy\",\n",
        "    \"linked_schema\": linked_schema\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02801d6e",
      "metadata": {},
      "source": [
        "### SQL Chain without Retry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "20acb415",
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "sql_chain_without_retry = RunnablePassthrough.assign(\n",
        "    linked_schema=link_schema\n",
        ") | RunnablePassthrough.assign(\n",
        "    sql_query=sql_query_generation_chain\n",
        ") | RunnablePassthrough.assign(\n",
        "    db_output=(\n",
        "        RunnableLambda(itemgetter(\"sql_query\")) \n",
        "        | functools.partial(\n",
        "            db.run_no_throw, include_columns=True\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "caf21465",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"query\": \"Tìm danh sách nhà cho thuê ở trên đường Láng hoặc Cầu Giấy\",\n",
            "  \"linked_schema\": {\n",
            "    \"BĐS Bán 500\": {\n",
            "      \"Địa chỉ_Tên đường\": \"TEXT\",\n",
            "      \"Địa chỉ\": \"TEXT\"\n",
            "    },\n",
            "    \"BĐS Cho thuê 500\": {\n",
            "      \"Địa chỉ\": \"TEXT\",\n",
            "      \"Địa chỉ_đường\": \"TEXT\",\n",
            "      \"Phường/Xã\": \"TEXT\",\n",
            "      \"Quận/Huyện\": \"TEXT\",\n",
            "      \"Tỉnh/TP\": \"TEXT\"\n",
            "    }\n",
            "  },\n",
            "  \"sql_query\": \"SELECT \\\"Địa chỉ\\\", \\\"Địa chỉ_đường\\\", \\\"Phường/Xã\\\", \\\"Quận/Huyện\\\", \\\"Tỉnh/TP\\\"\\nFROM \\\"BĐS Cho thuê 500\\\"\\nWHERE \\\"Địa chỉ_đường\\\" = 'Láng' OR \\\"Địa chỉ_đường\\\" = 'Cầu Giấy';\",\n",
            "  \"db_output\": {\n",
            "    \"result\": [],\n",
            "    \"error\": null\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "_output = await sql_chain_without_retry.ainvoke({\n",
        "    \"query\": \"Tìm danh sách nhà cho thuê ở trên đường Láng hoặc Cầu Giấy\",\n",
        "})\n",
        "print(json.dumps(_output, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e562b35",
      "metadata": {},
      "source": [
        "### Parse SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9652cd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sqlglot import exp, parse_one\n",
        "\n",
        "\n",
        "def get_predicate_values(\n",
        "    _input: dict\n",
        ") -> List[Dict[str, Any]]:\n",
        "    sql_query: str = _input.get(\"sql_query\")\n",
        "    if not sql_query:\n",
        "        raise ValueError(\"\")\n",
        "    schema: Dict[str, Dict[str, str]] = _input.get(\"linked_schema\")\n",
        "    if not schema:\n",
        "        raise ValueError(\"\")\n",
        "    parsed = parse_one(sql_query, read=db.dialect.lower())\n",
        "    \n",
        "    # --- Step A: Resolve Aliases (c -> customers) ---\n",
        "    alias_map = {}\n",
        "    \n",
        "    # 1. Check FROM\n",
        "    for node in parsed.find_all(exp.From):\n",
        "        for table in node.find_all(exp.Table):\n",
        "            real_name = table.name\n",
        "            alias = table.alias if table.alias else real_name\n",
        "            alias_map[alias] = real_name\n",
        "\n",
        "    # 2. Check JOINs\n",
        "    for node in parsed.find_all(exp.Join):\n",
        "        table = node.this\n",
        "        real_name = table.name\n",
        "        alias = table.alias if table.alias else real_name\n",
        "        alias_map[alias] = real_name\n",
        "\n",
        "    print(f\"DEBUG: Found Aliases: {alias_map}\")\n",
        "\n",
        "    extracted_data = []\n",
        "\n",
        "    # --- Step B: Recursive Visitor ---\n",
        "    def visit_node(node):\n",
        "        if not node:\n",
        "            return\n",
        "\n",
        "        # 1. Handle Binary Logic (AND, OR)\n",
        "        # sqlglot stores left side in 'this' and right side in 'expression'\n",
        "        if isinstance(node, (exp.And, exp.Or)):\n",
        "            visit_node(node.this)\n",
        "            visit_node(node.expression)\n",
        "            return\n",
        "\n",
        "        # 2. Handle Wrappers (Parenthesis, NOT, WHERE)\n",
        "        # These only have one child stored in 'this'\n",
        "        if isinstance(node, (exp.Paren, exp.Not, exp.Where)):\n",
        "            visit_node(node.this)\n",
        "            return\n",
        "\n",
        "        # 3. Handle Comparisons (Column = 'Value', !=, LIKE)\n",
        "        if isinstance(node, (exp.EQ, exp.NEQ, exp.Like, exp.ILike)):\n",
        "            # We look for: Column op Literal\n",
        "            if isinstance(node.left, exp.Column) and isinstance(node.right, exp.Literal):\n",
        "                if node.right.is_string:\n",
        "                    process_extraction(node.left, node.right.this, node.key)\n",
        "            return\n",
        "\n",
        "        # 4. Handle IN (Column IN ('A', 'B'))\n",
        "        if isinstance(node, exp.In):\n",
        "            if isinstance(node.this, exp.Column):\n",
        "                # The list of values is in args['expressions']\n",
        "                for item in node.args.get('expressions', []):\n",
        "                    if isinstance(item, exp.Literal) and item.is_string:\n",
        "                        process_extraction(node.this, item.this, \"IN\")\n",
        "            return\n",
        "\n",
        "    # Helper to validate and store\n",
        "    def process_extraction(col_node, value_str, operator):\n",
        "        col_name = col_node.name\n",
        "        table_alias = col_node.table\n",
        "        \n",
        "        real_table_name = None\n",
        "\n",
        "        # Resolve Alias\n",
        "        if table_alias:\n",
        "            real_table_name = alias_map.get(table_alias)\n",
        "        else:\n",
        "            # Try to guess table from schema if no alias provided\n",
        "            matches = [t for t, cols in schema.items() if col_name in cols]\n",
        "            if len(matches) == 1:\n",
        "                real_table_name = matches[0]\n",
        "\n",
        "        # Validation\n",
        "        if real_table_name and real_table_name in schema:\n",
        "            cols = schema[real_table_name]\n",
        "            if col_name in cols:\n",
        "                if cols[col_name] == \"TEXT\":\n",
        "                    extracted_data.append({\n",
        "                        \"table_name\": real_table_name,\n",
        "                        \"column_name\": col_name,\n",
        "                        \"value\": value_str,\n",
        "                        \"operator\": operator\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"DEBUG: Skipped {col_name} (Not TEXT)\")\n",
        "            else:\n",
        "                print(f\"DEBUG: Skipped {col_name} (Not in {real_table_name})\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Skipped {col_name} (Unknown table/alias)\")\n",
        "\n",
        "    # --- Step C: Start Traversal ---\n",
        "    where_clause = parsed.find(exp.Where)\n",
        "    if where_clause:\n",
        "        # Crucial Fix: Pass where_clause.this (the content) OR rely on the updated visitor handling exp.Where\n",
        "        visit_node(where_clause)\n",
        "    \n",
        "    return extracted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "100f8c61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Found Aliases: {'c': 'customers', 'o': 'orders', 'p': 'products'}\n",
            "DEBUG: Skipped full_name (Unknown table/alias)\n",
            "\n",
            "--- FINAL EXTRACTED PAIRS FOR VECTOR SEARCH ---\n",
            "{'table_name': 'customers', 'column_name': 'country', 'value': 'USA', 'operator': 'IN'}\n",
            "{'table_name': 'customers', 'column_name': 'country', 'value': 'Canada', 'operator': 'IN'}\n",
            "{'table_name': 'customers', 'column_name': 'country', 'value': 'UK', 'operator': 'IN'}\n",
            "{'table_name': 'customers', 'column_name': 'country', 'value': 'Germany', 'operator': 'IN'}\n",
            "{'table_name': 'orders', 'column_name': 'status', 'value': 'Cancelled', 'operator': 'neq'}\n",
            "{'table_name': 'products', 'column_name': 'category', 'value': 'Electronics', 'operator': 'eq'}\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. SETUP\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "SQL_QUERY = \"\"\"\n",
        "SELECT c.id, c.first_name || ' ' || c.last_name AS full_name \n",
        "FROM customers c\n",
        "INNER JOIN orders o ON c.id = o.customer_id\n",
        "INNER JOIN products p ON o.product_id = p.id\n",
        "WHERE \n",
        "    c.country IN ('USA', 'Canada', 'UK', 'Germany')\n",
        "    AND o.order_date BETWEEN '2023-01-01' AND '2023-12-31'\n",
        "    AND o.status != 'Cancelled'\n",
        "    AND p.category = 'Electronics'\n",
        "    AND full_name = 'David Jones' \n",
        "\"\"\"\n",
        "\n",
        "DB_SCHEMA = {\n",
        "    \"customers\": {\"id\": \"INTEGER\", \"first_name\": \"TEXT\", \"last_name\": \"TEXT\", \"country\": \"TEXT\"},\n",
        "    \"orders\": {\"id\": \"INTEGER\", \"customer_id\": \"INTEGER\", \"order_date\": \"TEXT\", \"status\": \"TEXT\"},\n",
        "    \"products\": {\"id\": \"INTEGER\", \"product_name\": \"TEXT\", \"category\": \"TEXT\"}\n",
        "}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ROBUST PARSER LOGIC\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. EXECUTION\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "results = get_predicate_values({\"sql_query\": SQL_QUERY, \"linked_schema\": DB_SCHEMA})\n",
        "\n",
        "print(\"\\n--- FINAL EXTRACTED PAIRS FOR VECTOR SEARCH ---\")\n",
        "for res in results:\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f0a664",
      "metadata": {},
      "source": [
        "### Retry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17592131",
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "sql_chain_without_retry = RunnablePassthrough.assign(\n",
        "    linked_schema=link_schema\n",
        ") | RunnablePassthrough.assign(\n",
        "    sql_query=sql_query_generation_chain\n",
        ") | RunnablePassthrough.assign(\n",
        "    db_output=(\n",
        "        RunnableLambda(itemgetter(\"sql_query\")) \n",
        "        | functools.partial(\n",
        "            db.run_no_throw, include_columns=True\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e38626",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_query_regeneration_chain = RunnablePassthrough.assign(\n",
        "    predicate_values=get_predicate_values\n",
        ") | RunnablePassthrough.assign(\n",
        "    tbl_col_sample_values=(\n",
        "        RunnableLambda(itemgetter(\"predicate_values\"))\n",
        "        | RunnableLambda(lambda predicate_values: [\n",
        "            (v[\"table_name\"], v[\"column_name\"], v[\"value\"])\n",
        "            for v in predicate_values\n",
        "        ])\n",
        "        | functools.partial(\n",
        "            db.batch_search_similar_values, k=5\n",
        "        )\n",
        "    )\n",
        ") | RunnablePassthrough.assign(\n",
        "    sql_query_retry=sql_query_generation_chain\n",
        ") | RunnablePassthrough.assign(\n",
        "    db_output=(\n",
        "        RunnableLambda(itemgetter(\"sql_query_retry\")) \n",
        "        | functools.partial(db.run_no_throw, include_columns=True))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "2274a272",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Found Aliases: {'BĐS Cho thuê 500': 'BĐS Cho thuê 500'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Tìm danh sách nhà cho thuê ở trên đường Láng hoặc Cầu Giấy',\n",
              " 'linked_schema': {'BĐS Bán 500': {'Địa chỉ_Tên đường': 'TEXT',\n",
              "   'Địa chỉ': 'TEXT'},\n",
              "  'BĐS Cho thuê 500': {'Địa chỉ': 'TEXT',\n",
              "   'Địa chỉ_đường': 'TEXT',\n",
              "   'Phường/Xã': 'TEXT',\n",
              "   'Quận/Huyện': 'TEXT',\n",
              "   'Tỉnh/TP': 'TEXT'}},\n",
              " 'sql_query': 'SELECT \"Địa chỉ\", \"Địa chỉ_đường\", \"Phường/Xã\", \"Quận/Huyện\", \"Tỉnh/TP\"\\nFROM \"BĐS Cho thuê 500\"\\nWHERE \"Địa chỉ_đường\" = \\'Láng\\' OR \"Địa chỉ_đường\" = \\'Cầu Giấy\\';',\n",
              " 'db_output': {'result': [{'Địa chỉ': 'Park Hill, Láng Hạ, Hoàn Kiếm',\n",
              "    'Địa chỉ_đường': 'Láng Hạ',\n",
              "    'Phường/Xã': 'Phường 4',\n",
              "    'Quận/Huyện': 'Hoàn Kiếm',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': '787 Láng Hạ, Cầu Giấy',\n",
              "    'Địa chỉ_đường': 'Láng Hạ',\n",
              "    'Phường/Xã': 'Phường 4',\n",
              "    'Quận/Huyện': 'Cầu Giấy',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': 'Ecopark, Láng Hạ, Đống Đa',\n",
              "    'Địa chỉ_đường': 'Láng Hạ',\n",
              "    'Phường/Xã': 'Phường 1',\n",
              "    'Quận/Huyện': 'Đống Đa',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': 'BRG Grand Plaza, Láng Hạ, Hà Đông',\n",
              "    'Địa chỉ_đường': 'Láng Hạ',\n",
              "    'Phường/Xã': 'Phường 11',\n",
              "    'Quận/Huyện': 'Hà Đông',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': '874 Cầu Giấy, Hai Bà Trưng',\n",
              "    'Địa chỉ_đường': '874 Cầu Giấy',\n",
              "    'Phường/Xã': 'Phường 6',\n",
              "    'Quận/Huyện': 'Hai Bà Trưng',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': '509 Láng Hạ, Sơn Tây',\n",
              "    'Địa chỉ_đường': 'Láng Hạ',\n",
              "    'Phường/Xã': 'Phường 10',\n",
              "    'Quận/Huyện': 'Sơn Tây',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': '464 Láng Hạ, Đống Đa',\n",
              "    'Địa chỉ_đường': '464 Láng Hạ',\n",
              "    'Phường/Xã': 'Phường 12',\n",
              "    'Quận/Huyện': 'Đống Đa',\n",
              "    'Tỉnh/TP': 'Hà Nội'},\n",
              "   {'Địa chỉ': '994 Cầu Giấy, Hoàn Kiếm',\n",
              "    'Địa chỉ_đường': '994 Cầu Giấy',\n",
              "    'Phường/Xã': 'Phường 9',\n",
              "    'Quận/Huyện': 'Hoàn Kiếm',\n",
              "    'Tỉnh/TP': 'Hà Nội'}],\n",
              "  'error': None},\n",
              " 'predicate_values': [{'table_name': 'BĐS Cho thuê 500',\n",
              "   'column_name': 'Địa chỉ_đường',\n",
              "   'value': 'Láng',\n",
              "   'operator': 'eq'},\n",
              "  {'table_name': 'BĐS Cho thuê 500',\n",
              "   'column_name': 'Địa chỉ_đường',\n",
              "   'value': 'Cầu Giấy',\n",
              "   'operator': 'eq'}],\n",
              " 'tbl_col_sample_values': {'BĐS Cho thuê 500': {'Địa chỉ_đường': ['Láng Hạ',\n",
              "    '464 Láng Hạ',\n",
              "    'Lạc Long Quân',\n",
              "    'Lê Lợi',\n",
              "    'Phan Xích Long',\n",
              "    '994 Cầu Giấy',\n",
              "    '874 Cầu Giấy',\n",
              "    'Giảng Võ',\n",
              "    'Điện Biên Phủ',\n",
              "    'Trần Hưng Đạo']}},\n",
              " 'sql_query_retry': 'SELECT \"Địa chỉ\", \"Địa chỉ_đường\", \"Phường/Xã\", \"Quận/Huyện\", \"Tỉnh/TP\"\\nFROM \"BĐS Cho thuê 500\"\\nWHERE \"Địa chỉ_đường\" LIKE \\'%Láng%\\' OR \"Địa chỉ_đường\" LIKE \\'%Cầu Giấy%\\';'}"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp = await sql_chain_with_retry.ainvoke(result)\n",
        "tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6ab0e8",
      "metadata": {},
      "source": [
        "## Full Langgraph Workflow (Worked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "df59fc73",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SQLAssistantState(TypedDict):\n",
        "    user_query: str\n",
        "    linked_schema: Dict[str, Dict[str, str]]\n",
        "    sql_queries: List[str]\n",
        "    predicate_values: List[Dict[str, Any]]\n",
        "    tbl_col_sample_values: Dict[str, Dict[str, List[Any]]]\n",
        "    db_output: Dict[str, Any]\n",
        "    final_answer: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "id": "eea89c94",
      "metadata": {},
      "outputs": [],
      "source": [
        "SCHEMA_LINKING_TEMPLATE = \"\"\"\n",
        "You are an expert in SQL schema linking. \n",
        "Given a {dialect} table schema (DDL) and a user query, determine if the table is relevant to the query.\n",
        "\n",
        "Your task:\n",
        "1. Analyze the table schema and the user query to decide if they are related.\n",
        "2. Answer \"Y\" (Yes) or \"N\" (No).\n",
        "3. If the answer is \"Y\", list ALL columns that are semantically related to the query topics. \n",
        "   - You do NOT need to identify the exact columns for the final SQL query. \n",
        "   - You SHOULD include any columns that provide context, identifiers, or potential join keys related to the entities in the query.\n",
        "\n",
        "Output must be a valid JSON object inside a ```json code block using this format:\n",
        "```json\n",
        "{{\n",
        "    \"reasoning\": \"Reasoning of the decision\",\n",
        "    \"is_related\": \"Y or N\",\n",
        "    \"columns\": [\"column name 1\", \"column name 2\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "Table Schema (DDL):\n",
        "{table_info}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# Cache for schema linking chains keyed by model instance ID\n",
        "_schema_linking_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_schema_linking_chain(chat_model: BaseChatModel) -> Runnable:\n",
        "    # Use model instance ID as cache key (since ChatOpenAI objects aren't hashable)\n",
        "    chat_model_id = id(chat_model)\n",
        "    \n",
        "    if chat_model_id not in _schema_linking_chain_cache:\n",
        "        _schema_linking_chain_cache[chat_model_id] = (\n",
        "            ChatPromptTemplate([(\"human\", SCHEMA_LINKING_TEMPLATE)])\n",
        "            | chat_model\n",
        "            | JsonOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _schema_linking_chain_cache[chat_model_id]\n",
        "\n",
        "\n",
        "async def _link_schema_one(\n",
        "    user_query: str,\n",
        "    table_name: str,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        "    allowed_col_names: Optional[List[str]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    try:\n",
        "        column_names = database.get_column_names(table_name)\n",
        "        if isinstance(column_names, list) and len(column_names) <= 5:\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"user_query\": user_query,\n",
        "                    \"allowed_col_names\": allowed_col_names\n",
        "                },\n",
        "                \"filtered_schema\": (table_name, column_names),\n",
        "                \"error\": None\n",
        "            }\n",
        "    \n",
        "        table_info = database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=allowed_col_names,\n",
        "            sample_count=3\n",
        "        )\n",
        "        result = await get_schema_linking_chain(chat_model).ainvoke(\n",
        "            {\"table_info\": table_info, \"user_query\": user_query, \"dialect\": database.dialect}\n",
        "        )\n",
        "        if \"is_related\" not in result or result[\"is_related\"] not in [\"Y\", \"N\"]:\n",
        "            raise ValueError(\"Invalid response from schema linking chain\")\n",
        "        if result[\"is_related\"] == \"Y\" and not result.get(\"columns\"):\n",
        "            result[\"columns\"] = [\"ROWID\"]\n",
        "\n",
        "        if result[\"is_related\"] == \"N\":\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"user_query\": user_query,\n",
        "                    \"allowed_col_names\": allowed_col_names\n",
        "                },\n",
        "                \"filtered_schema\": None,\n",
        "                \"error\": None\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"input_item\": {\"table_name\": table_name, \"user_query\": user_query, \"allowed_col_names\": allowed_col_names},\n",
        "                \"filtered_schema\": (table_name, result[\"columns\"]),\n",
        "                \"error\": None\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"input_item\": {\"table_name\": table_name, \"user_query\": user_query},\n",
        "            \"filtered_schema\": None,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "async def link_schema(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> Dict[str, Dict[str, str]]:\n",
        "    user_query = state.get(\"user_query\")\n",
        "    if not user_query:\n",
        "        raise ValueError(\"user_query is required\")\n",
        "    max_retries = state.get(\"max_retries\", 1)\n",
        "    # queue = []\n",
        "    # for table in  database.get_usable_table_names():\n",
        "    #     for col_group in database.get_column_groups(table):\n",
        "    #         queue.append({\n",
        "    #             \"table_name\": table,\n",
        "    #             \"allowed_col_names\": col_group,\n",
        "    #             \"user_query\": user_query\n",
        "    #         })\n",
        "    queue = [\n",
        "        {\"table_name\": table_name, \"user_query\": user_query} \n",
        "        for table_name in database.get_usable_table_names()\n",
        "    ]\n",
        "    successful_results = []\n",
        "    for _ in range(max_retries):\n",
        "        tasks = [_link_schema_one(chat_model=chat_model, database=database, **input_item) for input_item in queue]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        successful_results.extend([\n",
        "            res for res in results if res[\"error\"] is None\n",
        "        ])\n",
        "        failed_items = [\n",
        "            res[\"input_item\"] for res in results if res[\"error\"] is not None\n",
        "        ]\n",
        "        queue = failed_items\n",
        "        if not queue:\n",
        "            break\n",
        "    \n",
        "    linked_schema = [\n",
        "        result[\"filtered_schema\"] \n",
        "        for result in successful_results \n",
        "        if result[\"filtered_schema\"]\n",
        "    ]\n",
        "    # Return per-table mapping: column_name -> datatype\n",
        "    final_schema: Dict[str, Dict[str, str]] = {}\n",
        "    for table_name, col_names in linked_schema:\n",
        "        table_schema = final_schema.setdefault(table_name, {})\n",
        "        for col_name in col_names:\n",
        "            col_type = database.get_column_datatype(\n",
        "                table_name,\n",
        "                col_name,\n",
        "                default=\"NULL\",\n",
        "            )\n",
        "            if col_type != \"NULL\":\n",
        "                table_schema[col_name] = col_type\n",
        "\n",
        "    state[\"linked_schema\"] = final_schema\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "id": "35ed24a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "SQL_GEN_TEMPLATE = \"\"\"\n",
        "### DATE INFORMATION:\n",
        "Today is {date}\n",
        "\n",
        "### INSTRUCTIONS:\n",
        "You write SQL queries for a {dialect} database. Users are querying their company database, and your task is to assist by generating valid SQL queries strictly adhering to the database schema provided.\n",
        "\n",
        "**Table Schema**:\n",
        "{table_infos}\n",
        "\n",
        "Translate the user's request into one valid {dialect} query. SQL should be written as a markdown code block:\n",
        "For example:\n",
        "```sql\n",
        "SELECT column1, column2 FROM table WHERE condition;\n",
        "```\n",
        "\n",
        "### GUIDELINES:\n",
        "\n",
        "1.  **Schema Adherence**:\n",
        "    *   Use only tables, columns, and relationships explicitly listed in the provided schema.\n",
        "    *   Do not make assumptions about missing or inferred columns/tables.\n",
        "\n",
        "2.  **{dialect}-Specific Syntax**:\n",
        "    *   Use only {dialect} syntax. Be aware that {dialect} has limited built-in date/time functions compared to other sql dialects.\n",
        "\n",
        "3.  **Conditions**:\n",
        "    *   Always include default conditions for filtering invalid data, e.g., `deleted_at IS NULL` and `status != 'cancelled'` if relevant.\n",
        "    *   Ensure these conditions match the query's intent unless explicitly omitted in the user request.\n",
        "\n",
        "4.  **Output Consistency**:\n",
        "    *   The output fields must match the query's intent exactly. Do not add extra columns or omit requested fields.\n",
        "\n",
        "5.  **Reserved Keywords and Case Sensitivity**:\n",
        "    *   Escape reserved keywords or case-sensitive identifiers using double quotes (\" \"), e.g., \"order\".\n",
        "\n",
        "If the user's question is ambiguous or unclear, you must make your best reasonable guess based on the schema.\n",
        "Translate the user's intent into a **single valid {dialect} query** based on the schema provided.\n",
        "Ensure the query is optimized, precise, and error-free.\n",
        "\n",
        "**You must ONLY output ONE SINGLE valid SQL query as markdown codeblock.**\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "_sql_markdown_re = re.compile(r\"```sql\\s*([\\s\\S]*?)\\s*```\", re.DOTALL)\n",
        "def parse_sql_output(msg_content: str) -> str:\n",
        "    try:\n",
        "        match = _sql_markdown_re.search(msg_content)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        else:\n",
        "            raise ValueError(\"No SQL query found in the content\")\n",
        "    except Exception:\n",
        "        return msg_content\n",
        "\n",
        "\n",
        "def preprocess_for_sql_query_generation(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> List[AnyMessage]:\n",
        "    linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    user_query = state.get(\"user_query\")\n",
        "    if not user_query:\n",
        "        raise ValueError(\"user_query not found in the input\")\n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=state.get(\"tbl_col_sample_values\", {}).get(table_name, None),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    system_prompt = SystemMessage(SQL_GEN_TEMPLATE.format(\n",
        "        table_infos=table_infos,\n",
        "        date=get_today_date_en(),\n",
        "        dialect=database.dialect\n",
        "    ))\n",
        "    human_message = HumanMessage(content=user_query)\n",
        "    return [system_prompt, human_message]\n",
        "\n",
        "\n",
        "_sql_query_generation_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_sql_query_generation_chain(\n",
        "    chat_model: BaseChatModel, database: SQLiteDatabase\n",
        ") -> Runnable:\n",
        "    chat_model_id, database_id = id(chat_model), id(database)\n",
        "\n",
        "    if (chat_model_id, database_id) not in _sql_query_generation_chain_cache:\n",
        "        _sql_query_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "            RunnableLambda(partial(preprocess_for_sql_query_generation, database=database))\n",
        "            | chat_model\n",
        "            | StrOutputParser()\n",
        "            | parse_sql_output\n",
        "        )\n",
        "    \n",
        "    return _sql_query_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "async def generate_sql_query(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    if not state.get(\"sql_queries\"):\n",
        "        state[\"sql_queries\"] = []\n",
        "    sql_gen_chain = get_sql_query_generation_chain(chat_model, database)\n",
        "    sql_query = await sql_gen_chain.ainvoke(state)\n",
        "    state[\"sql_queries\"].append(sql_query)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "id": "e0b6b811",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predicate_values(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\")\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if not sql_query:\n",
        "        raise ValueError(\"SQL query is required\")\n",
        "    schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not schema:\n",
        "        raise ValueError(\"Schema is required\")\n",
        "    parsed = parse_one(sql_query, read=database.dialect.lower())\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Map Aliases AND Track Active Tables\n",
        "    # ---------------------------------------------------------\n",
        "    alias_map = {}\n",
        "    \n",
        "    # Helper to register tables found in FROM/JOIN\n",
        "    def register_table(table_node):\n",
        "        real_name = table_node.name\n",
        "        alias = table_node.alias if table_node.alias else real_name\n",
        "        alias_map[alias] = real_name\n",
        "\n",
        "    for from_node in parsed.find_all(exp.From):\n",
        "        for table in from_node.find_all(exp.Table):\n",
        "            register_table(table)\n",
        "\n",
        "    for join_node in parsed.find_all(exp.Join):\n",
        "        register_table(join_node.this)\n",
        "\n",
        "    print(f\"DEBUG: Found Aliases: {alias_map}\")\n",
        "\n",
        "    extracted_data = []\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Logic to Resolve Table for a Column\n",
        "    # ---------------------------------------------------------\n",
        "    def resolve_table(col_node):\n",
        "        col_name = col_node.name\n",
        "        table_alias = col_node.table\n",
        "        \n",
        "        # Case A: Alias is explicit (e.g., c.country)\n",
        "        if table_alias:\n",
        "            return alias_map.get(table_alias)\n",
        "        \n",
        "        # Case B: No alias (e.g., country). \n",
        "        # FIX: Check only tables present in the current query (alias_map.values())\n",
        "        active_tables = set(alias_map.values())\n",
        "        \n",
        "        candidates = []\n",
        "        for table in active_tables:\n",
        "            # Check if table exists in schema AND column exists in that table\n",
        "            if table in schema and col_name in schema[table]:\n",
        "                candidates.append(table)\n",
        "        \n",
        "        if len(candidates) == 1:\n",
        "            return candidates[0]\n",
        "        elif len(candidates) > 1:\n",
        "            print(f\"DEBUG: Ambiguous column '{col_name}' found in multiple active tables: {candidates}\")\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Recursive Visitor\n",
        "    # ---------------------------------------------------------\n",
        "    def visit_node(node):\n",
        "        if not node: \n",
        "            return\n",
        "\n",
        "        if isinstance(node, (exp.And, exp.Or)):\n",
        "            visit_node(node.this)\n",
        "            visit_node(node.expression)\n",
        "            return\n",
        "\n",
        "        if isinstance(node, (exp.Paren, exp.Not, exp.Where)):\n",
        "            visit_node(node.this)\n",
        "            return\n",
        "\n",
        "        # Handle Binary Comparisons (=, !=, LIKE)\n",
        "        if isinstance(node, (exp.EQ, exp.NEQ, exp.Like, exp.ILike)):\n",
        "            if isinstance(node.left, exp.Column) and isinstance(node.right, exp.Literal):\n",
        "                if node.right.is_string:\n",
        "                    process_extraction(node.left, node.right.this, node.key)\n",
        "            return\n",
        "\n",
        "        # Handle IN clause\n",
        "        if isinstance(node, exp.In) and isinstance(node.this, exp.Column):\n",
        "            for item in node.args.get('expressions', []):\n",
        "                if isinstance(item, exp.Literal) and item.is_string:\n",
        "                    process_extraction(node.this, item.this, \"IN\")\n",
        "            return\n",
        "\n",
        "    def process_extraction(col_node, value_str, operator):\n",
        "        col_name = col_node.name\n",
        "        real_table_name = resolve_table(col_node)\n",
        "\n",
        "        if real_table_name:\n",
        "            # Verify data type is TEXT\n",
        "            col_type = schema[real_table_name].get(col_name)\n",
        "            if col_type == \"TEXT\":\n",
        "                extracted_data.append({\n",
        "                    \"table_name\": real_table_name,\n",
        "                    \"column_name\": col_name,\n",
        "                    \"value\": value_str,\n",
        "                    \"operator\": operator\n",
        "                })\n",
        "            else:\n",
        "                print(f\"DEBUG: Skipped {col_name} (Type is {col_type}, not TEXT)\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Skipped {col_name} (Could not resolve table)\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. Execution\n",
        "    # ---------------------------------------------------------\n",
        "    where_clause = parsed.find(exp.Where)\n",
        "    if where_clause:\n",
        "        visit_node(where_clause)\n",
        "    state[\"predicate_values\"] = extracted_data\n",
        "    return state\n",
        "\n",
        "\n",
        "async def get_similar_predicate_values(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    predicate_values = state.get(\"predicate_values\")\n",
        "    if not predicate_values:\n",
        "        state[\"tbl_col_sample_values\"] = {}\n",
        "        return state\n",
        "    state[\"tbl_col_sample_values\"] = await database.batch_search_similar_values(\n",
        "        [\n",
        "            (v[\"table_name\"], v[\"column_name\"], v[\"value\"])\n",
        "            for v in predicate_values\n",
        "        ], \n",
        "        k=5\n",
        "    )\n",
        "    return state\n",
        "\n",
        "\n",
        "def restrict_select_columns(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    \"\"\"\n",
        "    Replaces SELECT * with SELECT t.col1, t.col2 based on filtered_schema.\n",
        "    \"\"\"\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\")\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if not sql_query:\n",
        "        raise ValueError(\"SQL query is required\")\n",
        "    schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not schema:\n",
        "        raise ValueError(\"Schema is required\")\n",
        "    parsed = parse_one(sql_query, read=database.dialect.lower())\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Build Alias Map (Map Alias -> Real Table Name)\n",
        "    # ---------------------------------------------------------\n",
        "    # We need to know the order of tables to expand * correctly\n",
        "    active_tables_ordered = [] \n",
        "    alias_map = {}\n",
        "\n",
        "    def register_table(table_node):\n",
        "        real_name = table_node.name\n",
        "        alias = table_node.alias if table_node.alias else real_name\n",
        "        \n",
        "        # Only register if we haven't seen this alias yet\n",
        "        if alias not in alias_map:\n",
        "            alias_map[alias] = real_name\n",
        "            active_tables_ordered.append(alias)\n",
        "\n",
        "    # Scan FROM\n",
        "    for from_node in parsed.find_all(exp.From):\n",
        "        for table in from_node.find_all(exp.Table):\n",
        "            register_table(table)\n",
        "\n",
        "    # Scan JOINs\n",
        "    for join_node in parsed.find_all(exp.Join):\n",
        "        register_table(join_node.this)\n",
        "\n",
        "    print(f\"DEBUG: Active Tables: {alias_map}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Helper to Generate Column Expressions\n",
        "    # ---------------------------------------------------------\n",
        "    def get_columns_for_table(table_alias):\n",
        "        real_name = alias_map.get(table_alias)\n",
        "        if not real_name or real_name not in schema:\n",
        "            return [] # Table not in our allowed schema, return nothing (or handle error)\n",
        "        \n",
        "        # Create sqlglot Column objects: alias.column_name\n",
        "        cols = schema[real_name].keys()\n",
        "        return [\n",
        "            exp.Column(\n",
        "                this=exp.Identifier(this=col, quoted=True),\n",
        "                table=exp.Identifier(this=table_alias, quoted=True)\n",
        "            ) for col in cols\n",
        "        ]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Rewrite SELECT Expressions\n",
        "    # ---------------------------------------------------------\n",
        "    # We only want to transform the main SELECT statement(s)\n",
        "    for select_node in parsed.find_all(exp.Select):\n",
        "        new_expressions = []\n",
        "        \n",
        "        for expression in select_node.expressions:\n",
        "            # Case A: Naked * (SELECT *)\n",
        "            if isinstance(expression, exp.Star) and not isinstance(expression, exp.Count):\n",
        "                # Expand columns for ALL active tables in the query\n",
        "                for alias in active_tables_ordered:\n",
        "                    expanded_cols = get_columns_for_table(alias)\n",
        "                    new_expressions.extend(expanded_cols)\n",
        "            \n",
        "            # Case B: Qualified * (SELECT t.*)\n",
        "            elif isinstance(expression, exp.Column) and isinstance(expression.this, exp.Star):\n",
        "                # Extract the table alias (e.g., 't' from 't.*')\n",
        "                table_alias = expression.table\n",
        "                expanded_cols = get_columns_for_table(table_alias)\n",
        "                new_expressions.extend(expanded_cols)\n",
        "                \n",
        "            # Case C: Regular column or other expression (Keep it)\n",
        "            else:\n",
        "                new_expressions.append(expression)\n",
        "\n",
        "        # Replace the old expressions with the new expanded list\n",
        "        if new_expressions:\n",
        "            select_node.set(\"expressions\", new_expressions)\n",
        "\n",
        "    restricted_sql_query = parsed.sql(dialect=database.dialect.lower())\n",
        "    def normalize_sql_query(sql_query: str) -> str:\n",
        "        return re.sub(r\"\\s+\", \" \", sql_query).strip().lower()\n",
        "    if normalize_sql_query(restricted_sql_query) != normalize_sql_query(sql_query):\n",
        "        state[\"sql_queries\"].append(restricted_sql_query)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "id": "a3f4faf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "ANSWER_GEN_TEMPLATE = \"\"\"\n",
        "### THÔNG TIN NGÀY THÁNG:\n",
        "Hôm nay là {date}\n",
        "\n",
        "### NHIỆM VỤ:\n",
        "Bạn là một trợ lý phân tích dữ liệu chuyên nghiệp. Nhiệm vụ của bạn là đưa ra câu trả lời bằng **Tiếng Việt** rõ ràng, chính xác và súc tích cho câu hỏi của người dùng, dựa hoàn toàn vào kết quả cơ sở dữ liệu (database results) được cung cấp.\n",
        "\n",
        "**Dữ liệu đầu vào**:\n",
        "1. **Lược đồ bảng (Table Schema)**:\n",
        "{table_infos}\n",
        "\n",
        "2. **Câu hỏi người dùng (User Question)**:\n",
        "{user_query}\n",
        "\n",
        "3. **Truy vấn SQL (SQL Query)**:\n",
        "```sql\n",
        "{sql_query}\n",
        "```\n",
        "\n",
        "4. **Kết quả từ Database (Database Results)**:\n",
        "{db_result}\n",
        "\n",
        "### CÁC NGUYÊN TẮC HƯỚNG DẪN:\n",
        "\n",
        "1.  **Chính xác và Tuân thủ dữ liệu**:\n",
        "    *   Câu trả lời phải dựa **TUYỆT ĐỐI** vào phần \"Kết quả từ Database\".\n",
        "    *   Không được tự suy diễn hoặc đưa vào các kiến thức bên ngoài không có trong dữ liệu.\n",
        "    *   Nếu kết quả trả về là rỗng (empty), hãy thông báo lịch sự rằng không tìm thấy dữ liệu phù hợp với yêu cầu.\n",
        "\n",
        "2.  **Định dạng câu trả lời**:\n",
        "    *   **Trả lời trực tiếp**: Đi thẳng vào vấn đề.\n",
        "    *   **Danh sách/Bảng**: Nếu kết quả có nhiều dòng, hãy trình bày dưới dạng danh sách gạch đầu dòng hoặc bảng Markdown cho dễ đọc.\n",
        "    *   **Số liệu tổng hợp**: Nếu kết quả là một con số duy nhất (tổng, đếm, trung bình), hãy viết thành một câu hoàn chỉnh (Ví dụ: \"Tổng doanh thu là 50.000.000 VNĐ\").\n",
        "\n",
        "3.  **Trình bày dữ liệu (Formatting)**:\n",
        "    *   **Con số**: Sử dụng dấu phân cách hàng nghìn (ví dụ: 1.000 hoặc 1,000 tùy theo ngữ cảnh, nhưng phải nhất quán).\n",
        "    *   **Tiền tệ**: Thêm đơn vị tiền tệ phù hợp nếu có (ví dụ: VNĐ, $, USD).\n",
        "    *   **Ngày tháng**: Chuyển đổi sang định dạng ngày tháng Tiếng Việt tự nhiên (ví dụ: \"Ngày 01 tháng 01 năm 2024\").\n",
        "\n",
        "4.  **Ngữ cảnh và Thuật ngữ**:\n",
        "    *   Sử dụng \"Truy vấn SQL\" để hiểu ngữ cảnh lọc dữ liệu (ví dụ: nếu SQL có `WHERE status = 'active'`, hãy nói rõ đây là các đơn hàng có trạng thái là \"đang hoạt động\").\n",
        "    *   Sử dụng ngôn ngữ kinh doanh/đời thường. **Không** nhắc đến tên bảng kỹ thuật (như `tbl_users`, `col_price`) hoặc cú pháp code trong câu trả lời cuối cùng.\n",
        "\n",
        "5.  **Văn phong**:\n",
        "    *   Chuyên nghiệp, khách quan và hữu ích.\n",
        "    *   Tránh các câu máy móc như mà hãy trả lời tự nhiên như một con người.\n",
        "\n",
        "**Đầu ra**:\n",
        "Chỉ xuất ra câu trả lời cuối cùng bằng Tiếng Việt (sử dụng Markdown).\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def preprocess_for_answer_generation(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> List[AnyMessage]:\n",
        "    user_query = state.get(\"user_query\")\n",
        "    if not user_query:\n",
        "        raise ValueError(\"user_query not found in the input\")\n",
        "    linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    db_output: Dict[str, Any] = state.get(\"db_output\", {})\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\", [])\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"sql_queries not found in the input\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if db_output.get(\"error\", \"Error\") is not None:\n",
        "        raise ValueError(\"No valid database result found\")\n",
        "    db_result = db_output.get(\"result\", [])\n",
        "    \n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=state.get(\"tbl_col_sample_values\", {}).get(table_name, None),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    \n",
        "    human_message = HumanMessage(content=ANSWER_GEN_TEMPLATE.format(\n",
        "        date=get_today_date_vi(),\n",
        "        table_infos=table_infos,\n",
        "        user_query=user_query,\n",
        "        sql_query=sql_query,\n",
        "        db_result=db_result\n",
        "    ))\n",
        "    return [human_message]\n",
        "\n",
        "\n",
        "_answer_generation_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_answer_generation_chain(chat_model: BaseChatModel, database: SQLiteDatabase) -> Runnable:\n",
        "    chat_model_id, database_id = id(chat_model), id(database)\n",
        "\n",
        "    if (chat_model_id, database_id) not in _answer_generation_chain_cache:\n",
        "        _answer_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "            RunnableLambda(partial(preprocess_for_answer_generation, database=database))\n",
        "            | chat_model\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _answer_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "async def generate_answer(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    answer_chain = get_answer_generation_chain(chat_model, database)\n",
        "    answer = await answer_chain.ainvoke(state)\n",
        "    state[\"final_answer\"] = answer\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "id": "c448cb63",
      "metadata": {},
      "outputs": [],
      "source": [
        "# add condition for rewrite sql query: dont rewrite if we can't find predicate values or similar predicate values contains original\n",
        "\n",
        "def retry_condition(\n",
        "    state: SQLAssistantState\n",
        ") -> Literal[\"gen_sql_query_2\", \"restrict_select_columns\"]:\n",
        "    predicate_values = state.get(\"predicate_values\")\n",
        "    if not predicate_values:\n",
        "        return \"restrict_select_columns\"\n",
        "    similar_predicate_values = state.get(\"tbl_col_sample_values\")\n",
        "    if not similar_predicate_values:\n",
        "        return \"restrict_select_columns\"\n",
        "    \n",
        "    # Check if all original predicate values are found in the similar values\n",
        "    all_found = True\n",
        "    for pred_value in predicate_values:\n",
        "        table_name = pred_value[\"table_name\"]\n",
        "        column_name = pred_value[\"column_name\"]\n",
        "        original_value = pred_value[\"value\"]\n",
        "        \n",
        "        # Get the list of similar values for this table/column pair\n",
        "        similar_values = similar_predicate_values.get(table_name, {}).get(column_name, [])\n",
        "        \n",
        "        # If the original value is NOT found in similar values, we need to rewrite\n",
        "        if original_value not in similar_values:\n",
        "            all_found = False\n",
        "            break\n",
        "    \n",
        "    # If all original values were found in similar values, we don't need to rewrite\n",
        "    if all_found:\n",
        "        return \"restrict_select_columns\"\n",
        "    \n",
        "    # If any original value was not found in similar values, we should rewrite\n",
        "    return \"gen_sql_query_2\"\n",
        "    \n",
        "\n",
        "\n",
        "async def sql_execution(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    sql_queries = state.get(\"sql_queries\", [])\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    state[\"db_output\"] = await database.run_no_throw(sql_query, include_columns=True)\n",
        "    return state\n",
        "\n",
        "\n",
        "def build_sql_assistant_pipeline(\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> CompiledStateGraph:\n",
        "    builder = StateGraph(SQLAssistantState)\n",
        "    # Add nodes\n",
        "    builder.add_node(\n",
        "        \"link_schema\",\n",
        "        partial(\n",
        "            link_schema,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"gen_sql_query_1\",\n",
        "        partial(\n",
        "            generate_sql_query, \n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"get_predicate_values\", \n",
        "        partial(\n",
        "            get_predicate_values, \n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"get_similar_predicate_values\", \n",
        "        partial(\n",
        "            get_similar_predicate_values, \n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"gen_sql_query_2\",\n",
        "        partial(\n",
        "            generate_sql_query,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"restrict_select_columns\",\n",
        "        partial(\n",
        "            restrict_select_columns,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"sql_execution\", \n",
        "        partial(\n",
        "            sql_execution,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"answer_generation\", \n",
        "        partial(\n",
        "            generate_answer,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add edges\n",
        "    builder.add_edge(START, \"link_schema\")\n",
        "    builder.add_edge(\"link_schema\", \"gen_sql_query_1\")\n",
        "    builder.add_edge(\"gen_sql_query_1\", \"get_predicate_values\")\n",
        "    builder.add_edge(\"get_predicate_values\", \"get_similar_predicate_values\")\n",
        "    builder.add_conditional_edges(\n",
        "        \"get_similar_predicate_values\",\n",
        "        retry_condition,\n",
        "    )\n",
        "    builder.add_edge(\"gen_sql_query_2\", \"restrict_select_columns\")\n",
        "    builder.add_edge(\"restrict_select_columns\", \"sql_execution\")\n",
        "    builder.add_edge(\"sql_execution\", \"answer_generation\")\n",
        "    builder.add_edge(\"answer_generation\", END)\n",
        "\n",
        "    return builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "id": "46cd2da6",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_assistant = build_sql_assistant_pipeline(llm, db)\n",
        "\n",
        "# display(Image(sql_assistant.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "id": "862945df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Found Aliases: {'BĐS Cho thuê 500': 'BĐS Cho thuê 500'}\n",
            "DEBUG: Active Tables: {'BĐS Cho thuê 500': 'BĐS Cho thuê 500'}\n",
            "Đã tìm thấy 1 căn hộ phù hợp với yêu cầu của bạn:\n",
            "\n",
            "- Diện tích: 52 m²  \n",
            "- Giá thuê: 5.6 triệu đồng/tháng  \n",
            "- Giá/m²/tháng: 107.692 VNĐ  \n",
            "- Số phòng ngủ: 2 phòng\n"
          ]
        }
      ],
      "source": [
        "user_query = \"tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng\"\n",
        "state = await sql_assistant.ainvoke({\"user_query\": user_query})\n",
        "print(state[\"final_answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "id": "7252b954",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'user_query': 'tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng',\n",
              " 'linked_schema': {'BĐS Bán 500': {'Số phòng ngủ': 'INTEGER',\n",
              "   'Giá (tỷ VNĐ)': 'REAL',\n",
              "   'Giá/m²_so_sanh': 'REAL',\n",
              "   'Diện tích (m²)': 'INTEGER',\n",
              "   'Loại BĐS': 'TEXT'},\n",
              "  'BĐS Cho thuê 500': {'Số phòng ngủ': 'INTEGER',\n",
              "   'Giá thuê (triệu/tháng)': 'REAL',\n",
              "   'Giá/m²/tháng_số': 'REAL',\n",
              "   'Diện tích (m²)': 'INTEGER'}},\n",
              " 'sql_queries': ['SELECT \"Diện tích (m²)\", \"Giá thuê (triệu/tháng)\", \"Giá/m²/tháng_số\", \"Số phòng ngủ\" \\nFROM \"BĐS Cho thuê 500\" \\nWHERE \"Số phòng ngủ\" = 2 AND \"Giá thuê (triệu/tháng)\" < 6;',\n",
              "  'SELECT \"Diện tích (m²)\", \"Giá thuê (triệu/tháng)\", \"Giá/m²/tháng_số\", \"Số phòng ngủ\" FROM \"BĐS Cho thuê 500\" WHERE \"Số phòng ngủ\" = 2 AND \"Giá thuê (triệu/tháng)\" < 6'],\n",
              " 'predicate_values': [],\n",
              " 'tbl_col_sample_values': {},\n",
              " 'db_output': {'result': [{'Diện tích (m²)': 52,\n",
              "    'Giá thuê (triệu/tháng)': 5.6,\n",
              "    'Giá/m²/tháng_số': 107692.0,\n",
              "    'Số phòng ngủ': 2}],\n",
              "  'error': None},\n",
              " 'final_answer': 'Đã tìm thấy 1 căn hộ phù hợp với yêu cầu của bạn:\\n\\n- Diện tích: 52 m²  \\n- Giá thuê: 5.6 triệu đồng/tháng  \\n- Giá/m²/tháng: 107.692 VNĐ  \\n- Số phòng ngủ: 2 phòng'}"
            ]
          },
          "execution_count": 295,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "029c82f0",
      "metadata": {},
      "source": [
        "## Full Langgraph workflow - Multi-turn (Worked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "0dd364b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SQLAssistantState(TypedDict):\n",
        "    conversation: List[AnyMessage]\n",
        "    linked_schema: Dict[str, Dict[str, str]]\n",
        "    sql_queries: List[str]\n",
        "    predicate_values: List[Dict[str, Any]]\n",
        "    tbl_col_sample_values: Dict[str, Dict[str, List[Any]]]\n",
        "    db_output: Dict[str, Any]\n",
        "    final_answer: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "5c2cfe11",
      "metadata": {},
      "outputs": [],
      "source": [
        "SCHEMA_LINKING_TEMPLATE = \"\"\"\n",
        "You are an expert in SQL schema linking. \n",
        "Given a {dialect} table schema (DDL) and a conversation history, determine if the table is relevant to the latest customer query.\n",
        "\n",
        "Your task:\n",
        "1. Analyze the table schema and the conversation history. Focus on the latest customer message, using previous messages for context (e.g., to resolve references). Evaluate the Table Name and Table Comment to see if the general topic matches the query. Answer \"Y\" (Yes) or \"N\" (No) regarding the table's relevance to the latest query.\n",
        "2. If the answer is \"Y\", list ALL columns that are semantically related. \n",
        "   - You do NOT need to identify the exact columns for the final SQL query. \n",
        "   - You MUST include all columns that provide context, identifiers, or potential join keys related to the entities in the query.\n",
        "\n",
        "Output must be a valid JSON object inside a ```json code block using this format:\n",
        "```json\n",
        "{{\n",
        "    \"explanation\": \"Explanation of the decision\",\n",
        "    \"is_related\": \"Y or N\",\n",
        "    \"columns\": [\"column name 1\", \"column name 2\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "Table Schema (DDL):\n",
        "{table_info}\n",
        "\n",
        "Conversation History:\n",
        "{formatted_conversation}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# Cache for schema linking chains keyed by model instance ID\n",
        "_schema_linking_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_schema_linking_chain(chat_model: BaseChatModel) -> Runnable:\n",
        "    # Use model instance ID as cache key (since ChatOpenAI objects aren't hashable)\n",
        "    chat_model_id = id(chat_model)\n",
        "    \n",
        "    if chat_model_id not in _schema_linking_chain_cache:\n",
        "        _schema_linking_chain_cache[chat_model_id] = (\n",
        "            ChatPromptTemplate([(\"human\", SCHEMA_LINKING_TEMPLATE)])\n",
        "            | chat_model\n",
        "            | JsonOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _schema_linking_chain_cache[chat_model_id]\n",
        "\n",
        "\n",
        "def format_conversation(conversation: List[AnyMessage]) -> str:\n",
        "    formatted_conversation = \"\"\n",
        "    end_index = len(conversation) - 1 \n",
        "    for ind in range(len(conversation) - 1, -1, -1):\n",
        "        if conversation[ind].type == \"human\":\n",
        "            end_index = ind\n",
        "            break\n",
        "    for message in conversation[:end_index]:\n",
        "        if message.type == \"human\":\n",
        "            formatted_conversation += f\"Customer: {message.content}\\n\"\n",
        "        elif message.type == \"ai\":\n",
        "            formatted_conversation += f\"Support Team: {message.content}\\n\"\n",
        "    \n",
        "    formatted_conversation += f\"\\nLatest Customer Message: {conversation[end_index].content}\"\n",
        "    return formatted_conversation\n",
        "\n",
        "\n",
        "async def _link_schema_one(\n",
        "    conversation: List[AnyMessage],\n",
        "    table_name: str,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        "    allowed_col_names: Optional[List[str]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    try:\n",
        "        column_names = database.get_column_names(table_name)\n",
        "        if isinstance(column_names, list) and len(column_names) <= 5:\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"conversation\": conversation,\n",
        "                    \"allowed_col_names\": allowed_col_names\n",
        "                },\n",
        "                \"filtered_schema\": (table_name, column_names),\n",
        "                \"error\": None\n",
        "            }\n",
        "        table_info = database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=allowed_col_names,\n",
        "            sample_count=3\n",
        "        )\n",
        "        result = await get_schema_linking_chain(chat_model).ainvoke({\n",
        "            \"table_info\": table_info, \n",
        "            \"formatted_conversation\": format_conversation(conversation), \n",
        "            \"dialect\": database.dialect\n",
        "        })\n",
        "        print(result[\"explanation\"])\n",
        "        if \"is_related\" not in result or result[\"is_related\"] not in [\"Y\", \"N\"]:\n",
        "            raise ValueError(\"Invalid response from schema linking chain\")\n",
        "        if result[\"is_related\"] == \"Y\" and not result.get(\"columns\"):\n",
        "            result[\"columns\"] = [\"ROWID\"]\n",
        "\n",
        "        if result[\"is_related\"] == \"N\":\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"conversation\": conversation,\n",
        "                    \"allowed_col_names\": allowed_col_names\n",
        "                },\n",
        "                \"filtered_schema\": None,\n",
        "                \"error\": None\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"input_item\": {\"table_name\": table_name, \"conversation\": conversation, \"allowed_col_names\": allowed_col_names},\n",
        "                \"filtered_schema\": (table_name, result[\"columns\"]),\n",
        "                \"error\": None\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"input_item\": {\"table_name\": table_name, \"conversation\": conversation},\n",
        "            \"filtered_schema\": None,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "async def link_schema(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> Dict[str, Dict[str, str]]:\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation is required\")\n",
        "    max_retries = 1\n",
        "    # queue = []\n",
        "    # for table in  database.get_usable_table_names():\n",
        "    #     for col_group in database.get_column_groups(table):\n",
        "    #         queue.append({\n",
        "    #             \"table_name\": table,\n",
        "    #             \"allowed_col_names\": col_group,\n",
        "    #             \"conversation\": conversation\n",
        "    #         })\n",
        "    queue = [\n",
        "        {\"table_name\": table_name, \"conversation\": conversation} \n",
        "        for table_name in database.get_usable_table_names()\n",
        "    ]\n",
        "    successful_results = []\n",
        "    for _ in range(max_retries):\n",
        "        tasks = [_link_schema_one(chat_model=chat_model, database=database, **input_item) for input_item in queue]\n",
        "        results = await tqdm_asyncio.gather(*tasks)\n",
        "        successful_results.extend([\n",
        "            res for res in results if res[\"error\"] is None\n",
        "        ])\n",
        "        failed_items = [\n",
        "            res[\"input_item\"] for res in results if res[\"error\"] is not None\n",
        "        ]\n",
        "        queue = failed_items\n",
        "        if not queue:\n",
        "            break\n",
        "    \n",
        "    linked_schema = [\n",
        "        result[\"filtered_schema\"] \n",
        "        for result in successful_results \n",
        "        if result[\"filtered_schema\"]\n",
        "    ]\n",
        "    # Return per-table mapping: column_name -> datatype\n",
        "    final_schema: Dict[str, Dict[str, str]] = {}\n",
        "    for table_name, col_names in linked_schema:\n",
        "        table_schema = final_schema.setdefault(table_name, {})\n",
        "        for col_name in col_names:\n",
        "            col_type = database.get_column_datatype(\n",
        "                table_name,\n",
        "                col_name,\n",
        "                default=\"NULL\",\n",
        "            )\n",
        "            if col_type != \"NULL\":\n",
        "                table_schema[col_name] = col_type\n",
        "\n",
        "    state[\"linked_schema\"] = final_schema\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "94e64be8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(db.get_table_info_no_throw(\n",
        "#     db.get_usable_table_names()[1],\n",
        "#     get_col_comments=True,\n",
        "#     sample_count=3\n",
        "# ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "d54f6d5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# tmp = await link_schema({\"conversation\": [\n",
        "#     HumanMessage(\"Hello\"),\n",
        "#     AIMessage(\"Em là trợ lý ảo Guso có thể hỗ trợ anh/chị về các sản phẩm và dịch vụ của BDS Guru. Chào anh/chị ạ! Nếu anh/chị có bất kỳ câu hỏi hay nhu cầu nào, em rất sẵn sàng giúp đỡ ạ!\"),\n",
        "#     HumanMessage(\"Có bao nhiêu nhà đang được cho thuê nhỉ\"),\n",
        "#     AIMessage(\"Tổng số nhà đang được cho thuê là 111 nhà ạ.\"),\n",
        "#     HumanMessage(\"Tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng\"),\n",
        "#     AIMessage(\"Có 1 căn hộ 2 phòng ngủ với giá thuê dưới 6 triệu đồng/tháng ạ.\"),\n",
        "#     HumanMessage(\"Thế có văn phòng nào giá dưới 6tr ở HN không\"),\n",
        "# ]}, get_llm_model(), db)\n",
        "# tmp[\"linked_schema\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f2926a7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# tmp = await link_schema({\"conversation\": [\n",
        "#     HumanMessage(\"Hello\"),\n",
        "#     AIMessage(\"Em là trợ lý ảo Guso có thể hỗ trợ anh/chị về các sản phẩm và dịch vụ của BDS Guru. Chào anh/chị ạ! Nếu anh/chị có bất kỳ câu hỏi hay nhu cầu nào, em rất sẵn sàng giúp đỡ ạ!\"),\n",
        "#     HumanMessage(\"Có bao nhiêu nhà đang được cho thuê nhỉ\"),\n",
        "#     AIMessage(\"Tổng số nhà đang được cho thuê là 111 nhà ạ.\"),\n",
        "#     HumanMessage(content=\"Tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng\"),\n",
        "# ]}, llm, db)\n",
        "# tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "a8651f56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(format_conversation([\n",
        "#     HumanMessage(\"Hello\"),\n",
        "#     AIMessage(\"Em là trợ lý ảo Guso có thể hỗ trợ anh/chị về các sản phẩm và dịch vụ của BDS Guru. Chào anh/chị ạ! Nếu anh/chị có bất kỳ câu hỏi hay nhu cầu nào, em rất sẵn sàng giúp đỡ ạ!\"),\n",
        "#     HumanMessage(\"Có bao nhiêu nhà đang được cho thuê nhỉ\"),\n",
        "#     AIMessage(\"Tổng số nhà đang được cho thuê là 111 nhà ạ.\"),\n",
        "#     HumanMessage(\"Tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng\"),\n",
        "# ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "00365d74",
      "metadata": {},
      "outputs": [],
      "source": [
        "SQL_GEN_TEMPLATE = \"\"\"\n",
        "### DATE INFORMATION:\n",
        "Today is {date}\n",
        "\n",
        "### INSTRUCTIONS:\n",
        "You write SQL queries for a {dialect} database. The Support Team is querying the database to answer Customer questions, and your task is to assist by generating valid SQL queries strictly adhering to the database schema provided.\n",
        "\n",
        "**Table Schema**:\n",
        "{table_infos}\n",
        "\n",
        "\n",
        "Translate the latest customer message into one valid {dialect} query, using the conversation history for context (e.g., resolving pronouns or follow-up filters). SQL should be written as a markdown code block:\n",
        "For example:\n",
        "```sql\n",
        "SELECT column1, column2 FROM table WHERE condition;\n",
        "```\n",
        "\n",
        "### GUIDELINES:\n",
        "\n",
        "1.  **Schema Adherence**:\n",
        "    *   Use only tables, columns, and relationships explicitly listed in the provided schema.\n",
        "    *   Do not make assumptions about missing or inferred columns/tables.\n",
        "\n",
        "2.  **{dialect}-Specific Syntax**:\n",
        "    *   Use only {dialect} syntax. Be aware that {dialect} has limited built-in date/time functions compared to other sql dialects.\n",
        "\n",
        "3.  **Conditions**:\n",
        "    *   Always include default conditions for filtering invalid data, e.g., `deleted_at IS NULL` and `status != 'cancelled'` if relevant.\n",
        "    *   Ensure these conditions match the query's intent unless explicitly omitted in the customer's request.\n",
        "\n",
        "4.  **Output Consistency**:\n",
        "    *   The output fields must match the query's intent exactly. Do not add extra columns or omit requested fields.\n",
        "\n",
        "5.  **Reserved Keywords and Case Sensitivity**:\n",
        "    *   Escape reserved keywords or case-sensitive identifiers using double quotes (\" \"), e.g., \"order\".\n",
        "\n",
        "If the customer's question is ambiguous or unclear, you must make your best reasonable guess based on the schema.\n",
        "Translate the customer's intent into a **single valid {dialect} query** based on the schema provided.\n",
        "Ensure the query is optimized, precise, and error-free.\n",
        "\n",
        "**You must ONLY output ONE SINGLE valid SQL query as markdown codeblock.**\n",
        "\n",
        "### CONVERSATION HISTORY:\n",
        "{formatted_conversation}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "_sql_markdown_re = re.compile(r\"```sql\\s*([\\s\\S]*?)\\s*```\", re.DOTALL)\n",
        "def parse_sql_output(msg_content: str) -> str:\n",
        "    try:\n",
        "        match = _sql_markdown_re.findall(msg_content)\n",
        "        if match:\n",
        "            return match[-1].strip()\n",
        "        else:\n",
        "            raise ValueError(\"No SQL query found in the content\")\n",
        "    except Exception:\n",
        "        return msg_content\n",
        "\n",
        "\n",
        "def preprocess_for_sql_query_generation(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> List[AnyMessage]:\n",
        "    linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation not found in the input\")\n",
        "    formatted_conversation = format_conversation(conversation)\n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=state.get(\"tbl_col_sample_values\", {}).get(table_name, None),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    return [HumanMessage(SQL_GEN_TEMPLATE.format(\n",
        "        date=get_today_date_en(),\n",
        "        dialect=database.dialect,\n",
        "        table_infos=table_infos,\n",
        "        formatted_conversation=formatted_conversation,\n",
        "    ))]\n",
        "\n",
        "\n",
        "_sql_query_generation_chain_cache: Dict[tuple[int, int], Runnable] = {}\n",
        "def get_sql_query_generation_chain(\n",
        "    chat_model: BaseChatModel, database: SQLiteDatabase\n",
        ") -> Runnable:\n",
        "    chat_model_id, database_id = id(chat_model), id(database)\n",
        "    if (chat_model_id, database_id) not in _sql_query_generation_chain_cache:\n",
        "        _sql_query_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "            RunnableLambda(partial(preprocess_for_sql_query_generation, database=database))\n",
        "            | chat_model\n",
        "            | StrOutputParser()\n",
        "            | parse_sql_output\n",
        "        )\n",
        "    \n",
        "    return _sql_query_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "async def generate_sql_query(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    if not state.get(\"sql_queries\"):\n",
        "        state[\"sql_queries\"] = []\n",
        "    sql_gen_chain = get_sql_query_generation_chain(chat_model, database)\n",
        "    sql_query = await sql_gen_chain.ainvoke(state)\n",
        "    state[\"sql_queries\"].append(sql_query)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "3341255c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQL_GEN_TEMPLATE = \"\"\"\n",
        "# ### DATE INFORMATION:\n",
        "# Today is {date}\n",
        "\n",
        "# ### INSTRUCTIONS:\n",
        "# You write SQL queries for a {dialect} database. Users are querying their company database, and your task is to assist by generating valid SQL queries strictly adhering to the database schema provided and calling the tool to execute them.\n",
        "\n",
        "# **Table Schema**:\n",
        "# {table_infos}\n",
        "\n",
        "# ### GUIDELINES:\n",
        "# 1.  **Schema Adherence**:\n",
        "#     *   Use only tables, columns, and relationships explicitly listed in the provided schema.\n",
        "#     *   Do not make assumptions about missing or inferred columns/tables.\n",
        "\n",
        "# 2.  **{dialect}-Specific Syntax**:\n",
        "#     *   Use only {dialect} syntax. Be aware that {dialect} has limited built-in date/time functions compared to other sql dialects.\n",
        "\n",
        "# 3.  **Conditions**:\n",
        "#     *   Always include default conditions for filtering invalid data, e.g., `deleted_at IS NULL` and `status != 'cancelled'` if relevant.\n",
        "#     *   Ensure these conditions match the query's intent unless explicitly omitted in the user request.\n",
        "\n",
        "# 4.  **Output Consistency**:\n",
        "#     *   The output fields must match the query's intent exactly. Do not add extra columns or omit requested fields.\n",
        "\n",
        "# 5.  **Reserved Keywords and Case Sensitivity**:\n",
        "#     *   Escape reserved keywords or case-sensitive identifiers using double quotes (\" \"), e.g., \"order\".\n",
        "\n",
        "# If the user's intent is ambiguous or unclear, you must make your best reasonable guess based on the schema. Translate the user's intent into a **single valid {dialect} query** based on the schema provided.\n",
        "# Ensure the query is optimized, precise, and error-free.\n",
        "# \"\"\".strip()\n",
        "\n",
        "\n",
        "# QUERY_DATABASE_TOOL = json.dumps({\n",
        "#     'type': 'function',\n",
        "#     'function': {\n",
        "#         'name': 'query_database',\n",
        "#         'description': 'Thực hiện câu truy vấn {{dialect}} và trả về kết quả',\n",
        "#         'parameters': {\n",
        "#             'properties': {\n",
        "#                 'sql_query': {\n",
        "#                     'description': 'Câu truy vấn {{dialect}}',\n",
        "#                     'type': 'string'\n",
        "#                 }\n",
        "#             },\n",
        "#             'required': ['sql_query'],\n",
        "#             'type': 'object'\n",
        "#         }\n",
        "#     }\n",
        "# })\n",
        "\n",
        "\n",
        "# def preprocess_for_sql_query_generation(\n",
        "#     state: SQLAssistantState,\n",
        "#     database: SQLiteDatabase,\n",
        "# ) -> List[AnyMessage]:\n",
        "#     linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "#     if not linked_schema:\n",
        "#         raise ValueError(\"linked_schema not found in the input\")\n",
        "#     conversation = state.get(\"conversation\")\n",
        "#     if not conversation:\n",
        "#         raise ValueError(\"conversation not found in the input\")\n",
        "#     table_infos = \"\\n\\n\".join([\n",
        "#         database.get_table_info_no_throw(\n",
        "#             table_name,\n",
        "#             get_col_comments=True,\n",
        "#             allowed_col_names=list(col_types.keys()),\n",
        "#             sample_count=5,\n",
        "#             column_sample_values=state.get(\"tbl_col_sample_values\", {}).get(table_name, None),\n",
        "#         )\n",
        "#         for table_name, col_types in linked_schema.items()\n",
        "#     ])\n",
        "#     system_prompt = SystemMessage(SQL_GEN_TEMPLATE.format(\n",
        "#         table_infos=table_infos,\n",
        "#         date=get_today_date_en(),\n",
        "#         dialect=database.dialect\n",
        "#     ))\n",
        "#     return [system_prompt] + conversation\n",
        "\n",
        "\n",
        "# _sql_query_generation_chain_cache: Dict[tuple[int, int], Runnable] = {}\n",
        "# def get_sql_query_generation_chain(\n",
        "#     chat_model: BaseChatModel, database: SQLiteDatabase\n",
        "# ) -> Runnable:\n",
        "#     chat_model_id, database_id = id(chat_model), id(database)\n",
        "#     tool = QUERY_DATABASE_TOOL.replace(\"{{dialect}}\", database.dialect)\n",
        "#     if (chat_model_id, database_id) not in _sql_query_generation_chain_cache:\n",
        "#         _sql_query_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "#             RunnableLambda(partial(preprocess_for_sql_query_generation, database=database))\n",
        "#             | chat_model.bind(tools=[json.loads(tool)])\n",
        "#             | postprocess_ai_message\n",
        "#             | RunnableLambda(\n",
        "#                 lambda ai_message: ai_message.tool_calls[-1].get(\"args\", {}).get(\"sql_query\", \"\")\n",
        "#             )\n",
        "#         )\n",
        "    \n",
        "#     return _sql_query_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "# async def generate_sql_query(\n",
        "#     state: SQLAssistantState,\n",
        "#     chat_model: BaseChatModel,\n",
        "#     database: SQLiteDatabase,\n",
        "# ) -> SQLAssistantState:\n",
        "#     if not state.get(\"sql_queries\"):\n",
        "#         state[\"sql_queries\"] = []\n",
        "#     sql_gen_chain = get_sql_query_generation_chain(chat_model, database)\n",
        "#     sql_query = await sql_gen_chain.ainvoke(state)\n",
        "#     state[\"sql_queries\"].append(sql_query)\n",
        "#     return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "9ad69d5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predicate_values(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\")\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if not sql_query:\n",
        "        raise ValueError(\"SQL query is required\")\n",
        "    schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not schema:\n",
        "        raise ValueError(\"Schema is required\")\n",
        "    parsed = parse_one(sql_query, read=database.dialect.lower())\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Map Aliases AND Track Active Tables\n",
        "    # ---------------------------------------------------------\n",
        "    alias_map = {}\n",
        "    \n",
        "    # Helper to register tables found in FROM/JOIN\n",
        "    def register_table(table_node):\n",
        "        real_name = table_node.name\n",
        "        alias = table_node.alias if table_node.alias else real_name\n",
        "        alias_map[alias] = real_name\n",
        "\n",
        "    for from_node in parsed.find_all(exp.From):\n",
        "        for table in from_node.find_all(exp.Table):\n",
        "            register_table(table)\n",
        "\n",
        "    for join_node in parsed.find_all(exp.Join):\n",
        "        register_table(join_node.this)\n",
        "\n",
        "    print(f\"DEBUG: Found Aliases: {alias_map}\")\n",
        "\n",
        "    extracted_data = []\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Logic to Resolve Table for a Column\n",
        "    # ---------------------------------------------------------\n",
        "    def resolve_table(col_node):\n",
        "        col_name = col_node.name\n",
        "        table_alias = col_node.table\n",
        "        \n",
        "        # Case A: Alias is explicit (e.g., c.country)\n",
        "        if table_alias:\n",
        "            return alias_map.get(table_alias)\n",
        "        \n",
        "        # Case B: No alias (e.g., country). \n",
        "        # FIX: Check only tables present in the current query (alias_map.values())\n",
        "        active_tables = set(alias_map.values())\n",
        "        \n",
        "        candidates = []\n",
        "        for table in active_tables:\n",
        "            # Check if table exists in schema AND column exists in that table\n",
        "            if table in schema and col_name in schema[table]:\n",
        "                candidates.append(table)\n",
        "        \n",
        "        if len(candidates) == 1:\n",
        "            return candidates[0]\n",
        "        elif len(candidates) > 1:\n",
        "            print(f\"DEBUG: Ambiguous column '{col_name}' found in multiple active tables: {candidates}\")\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Recursive Visitor\n",
        "    # ---------------------------------------------------------\n",
        "    def visit_node(node):\n",
        "        if not node: \n",
        "            return\n",
        "\n",
        "        if isinstance(node, (exp.And, exp.Or)):\n",
        "            visit_node(node.this)\n",
        "            visit_node(node.expression)\n",
        "            return\n",
        "\n",
        "        if isinstance(node, (exp.Paren, exp.Not, exp.Where)):\n",
        "            visit_node(node.this)\n",
        "            return\n",
        "\n",
        "        # Handle Binary Comparisons (=, !=, LIKE)\n",
        "        if isinstance(node, (exp.EQ, exp.NEQ, exp.Like, exp.ILike)):\n",
        "            if isinstance(node.left, exp.Column) and isinstance(node.right, exp.Literal):\n",
        "                if node.right.is_string:\n",
        "                    process_extraction(node.left, node.right.this, node.key)\n",
        "            return\n",
        "\n",
        "        # Handle IN clause\n",
        "        if isinstance(node, exp.In) and isinstance(node.this, exp.Column):\n",
        "            for item in node.args.get('expressions', []):\n",
        "                if isinstance(item, exp.Literal) and item.is_string:\n",
        "                    process_extraction(node.this, item.this, \"IN\")\n",
        "            return\n",
        "\n",
        "    def process_extraction(col_node, value_str, operator):\n",
        "        col_name = col_node.name\n",
        "        real_table_name = resolve_table(col_node)\n",
        "\n",
        "        if real_table_name:\n",
        "            # Verify data type is TEXT\n",
        "            col_type = schema[real_table_name].get(col_name)\n",
        "            if col_type == \"TEXT\":\n",
        "                extracted_data.append({\n",
        "                    \"table_name\": real_table_name,\n",
        "                    \"column_name\": col_name,\n",
        "                    \"value\": value_str,\n",
        "                    \"operator\": operator\n",
        "                })\n",
        "            else:\n",
        "                print(f\"DEBUG: Skipped {col_name} (Type is {col_type}, not TEXT)\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Skipped {col_name} (Could not resolve table)\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. Execution\n",
        "    # ---------------------------------------------------------\n",
        "    where_clause = parsed.find(exp.Where)\n",
        "    if where_clause:\n",
        "        visit_node(where_clause)\n",
        "    state[\"predicate_values\"] = extracted_data\n",
        "    return state\n",
        "\n",
        "\n",
        "async def get_similar_predicate_values(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    predicate_values = state.get(\"predicate_values\")\n",
        "    if not predicate_values:\n",
        "        state[\"tbl_col_sample_values\"] = {}\n",
        "        return state\n",
        "    state[\"tbl_col_sample_values\"] = await database.batch_search_similar_values(\n",
        "        [\n",
        "            (v[\"table_name\"], v[\"column_name\"], v[\"value\"])\n",
        "            for v in predicate_values\n",
        "        ], \n",
        "        k=5\n",
        "    )\n",
        "    return state\n",
        "\n",
        "\n",
        "def restrict_select_columns(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    \"\"\"\n",
        "    Replaces SELECT * with SELECT t.col1, t.col2 based on filtered_schema.\n",
        "    \"\"\"\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\")\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if not sql_query:\n",
        "        raise ValueError(\"SQL query is required\")\n",
        "    schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not schema:\n",
        "        raise ValueError(\"Schema is required\")\n",
        "    parsed = parse_one(sql_query, read=database.dialect.lower())\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Build Alias Map (Map Alias -> Real Table Name)\n",
        "    # ---------------------------------------------------------\n",
        "    # We need to know the order of tables to expand * correctly\n",
        "    active_tables_ordered = [] \n",
        "    alias_map = {}\n",
        "\n",
        "    def register_table(table_node):\n",
        "        real_name = table_node.name\n",
        "        alias = table_node.alias if table_node.alias else real_name\n",
        "        \n",
        "        # Only register if we haven't seen this alias yet\n",
        "        if alias not in alias_map:\n",
        "            alias_map[alias] = real_name\n",
        "            active_tables_ordered.append(alias)\n",
        "\n",
        "    # Scan FROM\n",
        "    for from_node in parsed.find_all(exp.From):\n",
        "        for table in from_node.find_all(exp.Table):\n",
        "            register_table(table)\n",
        "\n",
        "    # Scan JOINs\n",
        "    for join_node in parsed.find_all(exp.Join):\n",
        "        register_table(join_node.this)\n",
        "\n",
        "    print(f\"DEBUG: Active Tables: {alias_map}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Helper to Generate Column Expressions\n",
        "    # ---------------------------------------------------------\n",
        "    def get_columns_for_table(table_alias):\n",
        "        real_name = alias_map.get(table_alias)\n",
        "        if not real_name or real_name not in schema:\n",
        "            return [] # Table not in our allowed schema, return nothing (or handle error)\n",
        "        \n",
        "        # Create sqlglot Column objects: alias.column_name\n",
        "        cols = schema[real_name].keys()\n",
        "        return [\n",
        "            exp.Column(\n",
        "                this=exp.Identifier(this=col, quoted=True),\n",
        "                table=exp.Identifier(this=table_alias, quoted=True)\n",
        "            ) for col in cols\n",
        "        ]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Rewrite SELECT Expressions\n",
        "    # ---------------------------------------------------------\n",
        "    # We only want to transform the main SELECT statement(s)\n",
        "    for select_node in parsed.find_all(exp.Select):\n",
        "        new_expressions = []\n",
        "        \n",
        "        for expression in select_node.expressions:\n",
        "            # Case A: Naked * (SELECT *)\n",
        "            if isinstance(expression, exp.Star) and not isinstance(expression, exp.Count):\n",
        "                # Expand columns for ALL active tables in the query\n",
        "                for alias in active_tables_ordered:\n",
        "                    expanded_cols = get_columns_for_table(alias)\n",
        "                    new_expressions.extend(expanded_cols)\n",
        "            \n",
        "            # Case B: Qualified * (SELECT t.*)\n",
        "            elif isinstance(expression, exp.Column) and isinstance(expression.this, exp.Star):\n",
        "                # Extract the table alias (e.g., 't' from 't.*')\n",
        "                table_alias = expression.table\n",
        "                expanded_cols = get_columns_for_table(table_alias)\n",
        "                new_expressions.extend(expanded_cols)\n",
        "                \n",
        "            # Case C: Regular column or other expression (Keep it)\n",
        "            else:\n",
        "                new_expressions.append(expression)\n",
        "\n",
        "        # Replace the old expressions with the new expanded list\n",
        "        if new_expressions:\n",
        "            select_node.set(\"expressions\", new_expressions)\n",
        "\n",
        "    restricted_sql_query = parsed.sql(dialect=database.dialect.lower())\n",
        "    def normalize_sql_query(sql_query: str) -> str:\n",
        "        return re.sub(r\"\\s+\", \" \", sql_query).strip().strip(\";\").lower()\n",
        "    if normalize_sql_query(restricted_sql_query) != normalize_sql_query(sql_query):\n",
        "        state[\"sql_queries\"].append(restricted_sql_query)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "1539abdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "ANSWER_GEN_TEMPLATE = \"\"\"\n",
        "### THÔNG TIN NGÀY THÁNG:\n",
        "Hôm nay là {date}\n",
        "\n",
        "### NHIỆM VỤ:\n",
        "Bạn là một trợ lý phân tích dữ liệu chuyên nghiệp. Nhiệm vụ của bạn là đưa ra câu trả lời bằng **Tiếng Việt** rõ ràng, chính xác và súc tích cho câu hỏi của người dùng, dựa hoàn toàn vào kết quả cơ sở dữ liệu (database results) được cung cấp.\n",
        "\n",
        "**Lược đồ bảng (Table Schema)**:\n",
        "{table_infos}\n",
        "\n",
        "### CÁC NGUYÊN TẮC HƯỚNG DẪN:\n",
        "\n",
        "1.  **Chính xác và Tuân thủ dữ liệu**:\n",
        "    *   Câu trả lời phải dựa **TUYỆT ĐỐI** vào phần \"Kết quả từ Database\".\n",
        "    *   Không được tự suy diễn hoặc đưa vào các kiến thức bên ngoài không có trong dữ liệu.\n",
        "    *   Nếu kết quả trả về là rỗng (empty), hãy thông báo lịch sự rằng không tìm thấy dữ liệu phù hợp với yêu cầu.\n",
        "\n",
        "2.  **Định dạng câu trả lời**:\n",
        "    *   **Trả lời trực tiếp**: Đi thẳng vào vấn đề.\n",
        "    *   **Danh sách/Bảng**: Nếu kết quả có nhiều dòng, hãy trình bày dưới dạng danh sách gạch đầu dòng hoặc bảng Markdown cho dễ đọc.\n",
        "    *   **Số liệu tổng hợp**: Nếu kết quả là một con số duy nhất (tổng, đếm, trung bình), hãy viết thành một câu hoàn chỉnh (Ví dụ: \"Tổng doanh thu là 50.000.000 VNĐ\").\n",
        "\n",
        "3.  **Trình bày dữ liệu (Formatting)**:\n",
        "    *   **Con số**: Sử dụng dấu phân cách hàng nghìn (ví dụ: 1.000 hoặc 1,000 tùy theo ngữ cảnh, nhưng phải nhất quán).\n",
        "    *   **Tiền tệ**: Thêm đơn vị tiền tệ phù hợp nếu có (ví dụ: VNĐ, $, USD).\n",
        "    *   **Ngày tháng**: Chuyển đổi sang định dạng ngày tháng Tiếng Việt tự nhiên (ví dụ: \"Ngày 01 tháng 01 năm 2024\").\n",
        "\n",
        "4.  **Ngữ cảnh và Thuật ngữ**:\n",
        "    *   Sử dụng \"Truy vấn SQL\" để hiểu ngữ cảnh lọc dữ liệu (ví dụ: nếu SQL có `WHERE status = 'active'`, hãy nói rõ đây là các đơn hàng có trạng thái là \"đang hoạt động\").\n",
        "    *   Sử dụng ngôn ngữ kinh doanh/đời thường. **Không** nhắc đến tên bảng kỹ thuật (như `tbl_users`, `col_price`) hoặc cú pháp code trong câu trả lời cuối cùng.\n",
        "\n",
        "5.  **Văn phong**:\n",
        "    *   Chuyên nghiệp, khách quan và hữu ích.\n",
        "    *   Tránh các câu máy móc như mà hãy trả lời tự nhiên như một con người.\n",
        "\n",
        "**Đầu ra**:\n",
        "Chỉ xuất ra câu trả lời cuối cùng bằng Tiếng Việt (sử dụng Markdown).\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "class QueryDatabaseInput(BaseModel):\n",
        "    sql_query: str = Field(description=\"Câu truy vấn SQLite\")\n",
        "\n",
        "\n",
        "@tool(\"query_database\", args_schema=QueryDatabaseInput)\n",
        "def query_database(sql_query: str) -> str:\n",
        "    \"\"\"Thực hiện câu truy vấn SQLite và trả về kết quả\"\"\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def preprocess_for_answer_generation(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> List[AnyMessage]:\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation not found in the input\")\n",
        "    linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    db_output: Dict[str, Any] = state.get(\"db_output\", {})\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\", [])\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"sql_queries not found in the input\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if db_output.get(\"error\", \"Error\") is not None:\n",
        "        raise ValueError(\"No valid database result found\")\n",
        "    db_result = db_output.get(\"result\", [])\n",
        "    \n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=state.get(\"tbl_col_sample_values\", {}).get(table_name, None),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    tool_call = {\n",
        "        \"name\": \"query_database\",\n",
        "        \"arguments\": {\"sql_query\": sql_query}\n",
        "    }\n",
        "    \n",
        "    system_message = SystemMessage(content=ANSWER_GEN_TEMPLATE.format(\n",
        "        date=get_today_date_vi(),\n",
        "        table_infos=table_infos,\n",
        "    ))\n",
        "    sql_conversation = [system_message] + conversation\n",
        "    sql_conversation.append(AIMessage('<tool_call>\\n' + json.dumps(tool_call, ensure_ascii=False) + '\\n</tool_call>'))\n",
        "    sql_conversation.append(HumanMessage(content=str(db_result)))\n",
        "    return sql_conversation\n",
        "\n",
        "\n",
        "_answer_generation_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_answer_generation_chain(chat_model: BaseChatModel, database: SQLiteDatabase) -> Runnable:\n",
        "    chat_model_id, database_id = id(chat_model), id(database)\n",
        "    openai_tool_schema = {\n",
        "        'type': 'function',\n",
        "        'function': {\n",
        "            'name': 'query_database',\n",
        "            'description': f'Thực hiện câu truy vấn {database.dialect} và trả về kết quả',\n",
        "            'parameters': {\n",
        "                'properties': {\n",
        "                    'sql_query': {\n",
        "                        'description': f'Câu truy vấn {database.dialect}',\n",
        "                        'type': 'string'\n",
        "                    }\n",
        "                },\n",
        "                'required': ['sql_query'],\n",
        "                'type': 'object'\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    if (chat_model_id, database_id) not in _answer_generation_chain_cache:\n",
        "        _answer_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "            RunnableLambda(partial(preprocess_for_answer_generation, database=database))\n",
        "            | chat_model.bind(tools=[openai_tool_schema])\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _answer_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "async def generate_answer(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    answer_chain = get_answer_generation_chain(chat_model, database)\n",
        "    answer = await answer_chain.ainvoke(state)\n",
        "    state[\"final_answer\"] = answer\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "16756feb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# add condition for rewrite sql query: dont rewrite if we can't find predicate values or similar predicate values contains original\n",
        "\n",
        "def retry_condition(\n",
        "    state: SQLAssistantState\n",
        ") -> Literal[\"gen_sql_query_2\", \"restrict_select_columns\"]:\n",
        "    predicate_values = state.get(\"predicate_values\")\n",
        "    if not predicate_values:\n",
        "        return \"restrict_select_columns\"\n",
        "    similar_predicate_values = state.get(\"tbl_col_sample_values\")\n",
        "    if not similar_predicate_values:\n",
        "        return \"restrict_select_columns\"\n",
        "    \n",
        "    # Check if all original predicate values are found in the similar values\n",
        "    all_found = True\n",
        "    for pred_value in predicate_values:\n",
        "        table_name = pred_value[\"table_name\"]\n",
        "        column_name = pred_value[\"column_name\"]\n",
        "        original_value = pred_value[\"value\"]\n",
        "        \n",
        "        # Get the list of similar values for this table/column pair\n",
        "        similar_values = similar_predicate_values.get(table_name, {}).get(column_name, [])\n",
        "        \n",
        "        # If the original value is NOT found in similar values, we need to rewrite\n",
        "        if original_value not in similar_values:\n",
        "            all_found = False\n",
        "            break\n",
        "    \n",
        "    # If all original values were found in similar values, we don't need to rewrite\n",
        "    if all_found:\n",
        "        return \"restrict_select_columns\"\n",
        "    \n",
        "    print(predicate_values)\n",
        "    print(similar_predicate_values)\n",
        "    # If any original value was not found in similar values, we should rewrite\n",
        "    return \"gen_sql_query_2\"\n",
        "    \n",
        "\n",
        "\n",
        "async def sql_execution(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    sql_queries = state.get(\"sql_queries\", [])\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    state[\"db_output\"] = await database.run_no_throw(sql_query, include_columns=True)\n",
        "    return state\n",
        "\n",
        "\n",
        "def build_sql_assistant_pipeline(\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> CompiledStateGraph:\n",
        "    builder = StateGraph(SQLAssistantState)\n",
        "    # Add nodes\n",
        "    builder.add_node(\n",
        "        \"link_schema\",\n",
        "        partial(\n",
        "            link_schema,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"gen_sql_query_1\",\n",
        "        partial(\n",
        "            generate_sql_query, \n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"get_predicate_values\", \n",
        "        partial(\n",
        "            get_predicate_values, \n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"get_similar_predicate_values\", \n",
        "        partial(\n",
        "            get_similar_predicate_values, \n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"gen_sql_query_2\",\n",
        "        partial(\n",
        "            generate_sql_query,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"restrict_select_columns\",\n",
        "        partial(\n",
        "            restrict_select_columns,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"sql_execution\", \n",
        "        partial(\n",
        "            sql_execution,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"answer_generation\", \n",
        "        partial(\n",
        "            generate_answer,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add edges\n",
        "    builder.add_edge(START, \"link_schema\")\n",
        "    builder.add_edge(\"link_schema\", \"gen_sql_query_1\")\n",
        "    builder.add_edge(\"gen_sql_query_1\", \"get_predicate_values\")\n",
        "    builder.add_edge(\"get_predicate_values\", \"get_similar_predicate_values\")\n",
        "    builder.add_conditional_edges(\n",
        "        \"get_similar_predicate_values\",\n",
        "        retry_condition,\n",
        "    )\n",
        "    builder.add_edge(\"gen_sql_query_2\", \"restrict_select_columns\")\n",
        "    builder.add_edge(\"restrict_select_columns\", \"sql_execution\")\n",
        "    builder.add_edge(\"sql_execution\", \"answer_generation\")\n",
        "    builder.add_edge(\"answer_generation\", END)\n",
        "\n",
        "    return builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "2e279b50",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_assistant = build_sql_assistant_pipeline(get_llm_model(), db)\n",
        "\n",
        "# display(Image(sql_assistant.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "882e6b2f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:02<00:02,  2.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The latest customer query is asking about office spaces available for rent under 30 million VND. The table 'BĐS Bán 500' contains information about real estate properties for sale, not for rent. Therefore, the table is not relevant to the query about renting office space.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:05<00:00,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The latest customer query is asking about office spaces for rent under 30 million VND per month. The table 'BĐS Cho thuê 500' contains information about rental properties, including 'Giá thuê (triệu/tháng)' which represents the monthly rent in millions of VND. Since the customer is inquiring about rental prices and the table includes relevant price data, the table is related to the query. Additional columns such as 'Loại BĐS' (property type) and 'Diện tích (m²)' (area) are also relevant for context.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Found Aliases: {'BĐS Cho thuê 500': 'BĐS Cho thuê 500'}\n",
            "DEBUG: Active Tables: {'BĐS Cho thuê 500': 'BĐS Cho thuê 500'}\n"
          ]
        },
        {
          "ename": "InternalServerError",
          "evalue": "Error code: 500 - {'detail': '400: {\"object\":\"error\",\"message\":\"This model\\'s maximum context length is 8092 tokens. However, you requested 13472 tokens in the messages, Please reduce the length of the messages. None\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}'}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# state = await sql_assistant.ainvoke({\"conversation\": [\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#     HumanMessage(\"Hello\"),\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     AIMessage(\"Em là trợ lý ảo Guso có thể hỗ trợ anh/chị về các sản phẩm và dịch vụ của BDS Guru. Chào anh/chị ạ! Nếu anh/chị có bất kỳ câu hỏi hay nhu cầu nào, em rất sẵn sàng giúp đỡ ạ!\"),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#     HumanMessage(\"Thế có văn phòng nào giá dưới 6tr ở HN không\"),\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ]})\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m state = \u001b[38;5;28;01mawait\u001b[39;00m sql_assistant.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     12\u001b[39m     HumanMessage(\u001b[33m\"\u001b[39m\u001b[33mhello có văn phòng cho thuê dưới 10tr ko, văn phòng nhé\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     13\u001b[39m     AIMessage(\u001b[33m\"\u001b[39m\u001b[33mKhông tìm thấy văn phòng cho thuê có giá dưới 10 triệu đồng/tháng trong cơ sở dữ liệu.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     14\u001b[39m     HumanMessage(\u001b[33m\"\u001b[39m\u001b[33mthế dưới 30tr thì sao\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     15\u001b[39m ]})\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langgraph/pregel/main.py:3158\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3156\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3160\u001b[39m     config,\n\u001b[32m   3161\u001b[39m     context=context,\n\u001b[32m   3162\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3164\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3165\u001b[39m     print_mode=print_mode,\n\u001b[32m   3166\u001b[39m     output_keys=output_keys,\n\u001b[32m   3167\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3168\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3169\u001b[39m     durability=durability,\n\u001b[32m   3170\u001b[39m     **kwargs,\n\u001b[32m   3171\u001b[39m ):\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3173\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langgraph/pregel/main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mgenerate_answer\u001b[39m\u001b[34m(state, chat_model, database)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_answer\u001b[39m(\n\u001b[32m    126\u001b[39m     state: SQLAssistantState,\n\u001b[32m    127\u001b[39m     chat_model: BaseChatModel,\n\u001b[32m    128\u001b[39m     database: SQLiteDatabase,\n\u001b[32m    129\u001b[39m ) -> SQLAssistantState:\n\u001b[32m    130\u001b[39m     answer_chain = get_answer_generation_chain(chat_model, database)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     answer = \u001b[38;5;28;01mawait\u001b[39;00m answer_chain.ainvoke(state)\n\u001b[32m    132\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mfinal_answer\u001b[39m\u001b[33m\"\u001b[39m] = answer\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:3183\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3181\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3182\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3183\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3184\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3185\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:5561\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5554\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5555\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5556\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5559\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5560\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5562\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5563\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5564\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5565\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    418\u001b[39m     **kwargs: Any,\n\u001b[32m    419\u001b[39m ) -> AIMessage:\n\u001b[32m    420\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    422\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    423\u001b[39m         stop=stop,\n\u001b[32m    424\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    425\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    426\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    427\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    428\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    429\u001b[39m         **kwargs,\n\u001b[32m    430\u001b[39m     )\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    432\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    433\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1128\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     **kwargs: Any,\n\u001b[32m   1126\u001b[39m ) -> LLMResult:\n\u001b[32m   1127\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1129\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1130\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1086\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1073\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1074\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1075\u001b[39m             *[\n\u001b[32m   1076\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m             ]\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1087\u001b[39m flattened_outputs = [\n\u001b[32m   1088\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1090\u001b[39m ]\n\u001b[32m   1091\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1339\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1339\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1340\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1341\u001b[39m     )\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1630\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1628\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1629\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1632\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1633\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1634\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1635\u001b[39m ):\n\u001b[32m   1636\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1623\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1616\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1617\u001b[39m             response,\n\u001b[32m   1618\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1619\u001b[39m             metadata=generation_info,\n\u001b[32m   1620\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1621\u001b[39m         )\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m         raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.with_raw_response.create(\n\u001b[32m   1624\u001b[39m             **payload\n\u001b[32m   1625\u001b[39m         )\n\u001b[32m   1626\u001b[39m         response = raw_response.parse()\n\u001b[32m   1627\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2678\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2675\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2676\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2677\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2680\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2681\u001b[39m             {\n\u001b[32m   2682\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2683\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2684\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2685\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2686\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2687\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2688\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2689\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2690\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2691\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2692\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2693\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2694\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2695\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2696\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2697\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2698\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2699\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2700\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2701\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2702\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2703\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2704\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2705\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2706\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2707\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2708\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2709\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2710\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2711\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2712\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2713\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2714\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2715\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2716\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2717\u001b[39m             },\n\u001b[32m   2718\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2719\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2720\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2721\u001b[39m         ),\n\u001b[32m   2722\u001b[39m         options=make_request_options(\n\u001b[32m   2723\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2724\u001b[39m         ),\n\u001b[32m   2725\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2726\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2727\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2728\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-env/lib/python3.11/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mInternalServerError\u001b[39m: Error code: 500 - {'detail': '400: {\"object\":\"error\",\"message\":\"This model\\'s maximum context length is 8092 tokens. However, you requested 13472 tokens in the messages, Please reduce the length of the messages. None\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}'}",
            "During task with name 'answer_generation' and id '5375c5ee-70d3-e93b-68c3-025e9f7e2e3e'"
          ]
        }
      ],
      "source": [
        "# state = await sql_assistant.ainvoke({\"conversation\": [\n",
        "#     HumanMessage(\"Hello\"),\n",
        "#     AIMessage(\"Em là trợ lý ảo Guso có thể hỗ trợ anh/chị về các sản phẩm và dịch vụ của BDS Guru. Chào anh/chị ạ! Nếu anh/chị có bất kỳ câu hỏi hay nhu cầu nào, em rất sẵn sàng giúp đỡ ạ!\"),\n",
        "#     HumanMessage(\"Có bao nhiêu nhà đang được cho thuê nhỉ\"),\n",
        "#     AIMessage(\"Tổng số nhà đang được cho thuê là 111 nhà ạ.\"),\n",
        "#     HumanMessage(\"Tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng\"),\n",
        "#     AIMessage(\"Có 1 căn hộ 2 phòng ngủ với giá thuê dưới 6 triệu đồng/tháng ạ.\"),\n",
        "#     HumanMessage(\"Thế có văn phòng nào giá dưới 6tr ở HN không\"),\n",
        "# ]})\n",
        "\n",
        "state = await sql_assistant.ainvoke({\"conversation\": [\n",
        "    HumanMessage(\"hello có văn phòng cho thuê dưới 10tr ko, văn phòng nhé\"),\n",
        "    AIMessage(\"Không tìm thấy văn phòng cho thuê có giá dưới 10 triệu đồng/tháng trong cơ sở dữ liệu.\"),\n",
        "    HumanMessage(\"thế dưới 30tr thì sao\"),\n",
        "]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "f5e76b1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Không tìm thấy văn phòng cho thuê có giá dưới 10 triệu đồng/tháng trong cơ sở dữ liệu.\n"
          ]
        }
      ],
      "source": [
        "print(state[\"final_answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "45c830fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['SELECT \"Giá thuê (triệu/tháng)\", \"ID\", \"Loại BĐS\", \"Địa chỉ\" \\nFROM \"BĐS Cho thuê 500\" \\nWHERE \"Loại BĐS\" = \\'Văn phòng\\' \\n  AND \"Giá thuê (triệu/tháng)\" < 10;']\n"
          ]
        }
      ],
      "source": [
        "print(state[\"sql_queries\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a1f016",
      "metadata": {},
      "source": [
        "## Full Langgraph workflow - Multi-turn V1 (Beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "255fa4c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SQLAssistantState(TypedDict):\n",
        "    conversation: List[AnyMessage]\n",
        "    rewritten_message: str\n",
        "    sample_values: Dict[str, Dict[str, List[Any]]]\n",
        "    linked_schema: Dict[str, Dict[str, str]]\n",
        "    sql_queries: List[str]\n",
        "    db_output: Dict[str, Any]\n",
        "    final_answer: str\n",
        "\n",
        "\n",
        "def format_conversation(conversation: List[AnyMessage]) -> str:\n",
        "    formatted_conversation = \"\"\n",
        "    end_index = len(conversation) - 1 \n",
        "    for ind in range(len(conversation) - 1, -1, -1):\n",
        "        if conversation[ind].type == \"human\":\n",
        "            end_index = ind\n",
        "            break\n",
        "    for message in conversation[:end_index]:\n",
        "        if message.type == \"human\":\n",
        "            formatted_conversation += f\"Customer: {message.content}\\n\"\n",
        "        elif message.type == \"ai\":\n",
        "            formatted_conversation += f\"Support Team: {message.content}\\n\"\n",
        "    \n",
        "    formatted_conversation += f\"\\nLatest Customer Message: {conversation[end_index].content}\"\n",
        "    return formatted_conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de91c2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "MESSAGE_REWRITING_TEMPLATE = \"\"\"\n",
        "### Role\n",
        "You are an expert Query Reformulator. Your task is to take a conversation between a \"Customer\" and a \"Support Team\" and rewrite the Customer's LATEST message into a single, standalone, and self-contained Vietnamese query.\n",
        "\n",
        "### Objective\n",
        "The rewritten query will be sent to a Text-to-SQL engine. It must contain all the necessary constraints, entities, and filters mentioned earlier in the conversation, while removing conversational filler.\n",
        "\n",
        "### Rules\n",
        "1. **Self-Containment**: The output must be understandable without looking at the history.\n",
        "2. **Resolve Pronouns**: Replace words like \"them\", \"it\", \"those\", or \"their\" with the actual entities mentioned previously.\n",
        "3. **Handle Refinements**: If the user adds a filter (e.g., \"only the ones in New York\"), combine it with the previous subject (e.g., \"List all customers in New York\").\n",
        "4. **No Redundancy**: Do not include conversational fluff like \"Thanks,\" \"That's helpful,\" or \"Can you tell me...\"\n",
        "5. **Detect Topic Shifts**: If the user asks a question that is completely unrelated to the previous context, do not include old information.\n",
        "6. **Stay Brief**: Change the customer message as less as possible. Only add the minimum amount of context required to resolve context.\n",
        "7. **Output Format**: Output ONLY the rewritten text. Do not provide explanations or labels.\n",
        "\n",
        "### Conversation:\n",
        "{formatted_conversation}\n",
        "\n",
        "### Rewritten Query:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "_message_rewriting_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_message_rewriting_chain(chat_model: BaseChatModel) -> Runnable:\n",
        "    # Use model instance ID as cache key (since ChatOpenAI objects aren't hashable)\n",
        "    chat_model_id = id(chat_model)\n",
        "    \n",
        "    if chat_model_id not in _message_rewriting_chain_cache:\n",
        "        _message_rewriting_chain_cache[chat_model_id] = (\n",
        "            ChatPromptTemplate([(\"human\", MESSAGE_REWRITING_TEMPLATE)])\n",
        "            | chat_model\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _message_rewriting_chain_cache[chat_model_id]\n",
        "\n",
        "\n",
        "async def rewrite_message(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        ") -> Dict[str, Dict[str, str]]:\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation is required\")\n",
        "    rewritten_message = await get_message_rewriting_chain(chat_model).ainvoke({\n",
        "        \"formatted_conversation\": format_conversation(conversation)\n",
        "    })\n",
        "    state[\"rewritten_message\"] = rewritten_message\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "688ec20e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt = MESSAGE_REWRITING_TEMPLATE.format(formatted_conversation=format_conversation([\n",
        "#     HumanMessage(\"Hello\"),\n",
        "#     AIMessage(\"Em là trợ lý ảo Guso có thể hỗ trợ anh/chị về các sản phẩm và dịch vụ của BDS Guru. Chào anh/chị ạ! Nếu anh/chị có bất kỳ câu hỏi hay nhu cầu nào, em rất sẵn sàng giúp đỡ ạ!\"),\n",
        "#     HumanMessage(\"Có bao nhiêu nhà đang được cho thuê nhỉ\"),\n",
        "#     AIMessage(\"Tổng số nhà đang được cho thuê là 111 nhà ạ.\"),\n",
        "#     HumanMessage(\"Tôi muốn thuê một căn 2 phòng ngủ, giá dưới 6tr 1 tháng\"),\n",
        "#     AIMessage(\"Có 1 căn hộ 2 phòng ngủ với giá thuê dưới 6 triệu đồng/tháng ạ.\"),\n",
        "#     HumanMessage(\"ok xin thêm thông tin với\"),\n",
        "# ]))\n",
        "# tmp = llm.invoke(prompt)\n",
        "# print(tmp.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "7aea5dd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_sample_values(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    rewritten_message = state.get(\"rewritten_message\")\n",
        "    if not rewritten_message:\n",
        "        state[\"sample_values\"] = {}\n",
        "        return state\n",
        "    state[\"sample_values\"] = await database.search_similar_values_from_message(\n",
        "        rewritten_message,\n",
        "        k=5\n",
        "    )\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "57db5f66",
      "metadata": {},
      "outputs": [],
      "source": [
        "SCHEMA_LINKING_TEMPLATE = \"\"\"\n",
        "You are an expert in SQL schema linking. \n",
        "Given a {dialect} table schema (DDL) and a conversation history, determine if the table is relevant to the latest customer query.\n",
        "\n",
        "Your task:\n",
        "1. Analyze the table schema and the conversation history. Focus on the latest customer message, using previous messages for context (e.g., to resolve references). Evaluate the Table Name and Table Comment to see if the general topic matches the query. Answer \"Y\" (Yes) or \"N\" (No) regarding the table's relevance to the latest query.\n",
        "2. If the answer is \"Y\", list ALL columns that are semantically related. \n",
        "   - You do NOT need to identify the exact columns for the final SQL query. \n",
        "   - You MUST include all columns that provide context, identifiers, or potential join keys related to the entities in the query.\n",
        "\n",
        "Output must be a valid JSON object inside a ```json code block using this format:\n",
        "```json\n",
        "{{\n",
        "    \"explanation\": \"Explanation of the decision\",\n",
        "    \"is_related\": \"Y or N\",\n",
        "    \"columns\": [\"column name 1\", \"column name 2\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "Table Schema (DDL):\n",
        "{table_info}\n",
        "\n",
        "Conversation History:\n",
        "{formatted_conversation}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# Cache for schema linking chains keyed by model instance ID\n",
        "_schema_linking_chain_cache: Dict[int, Runnable] = {}\n",
        "def get_schema_linking_chain(chat_model: BaseChatModel) -> Runnable:\n",
        "    # Use model instance ID as cache key (since ChatOpenAI objects aren't hashable)\n",
        "    chat_model_id = id(chat_model)\n",
        "    \n",
        "    if chat_model_id not in _schema_linking_chain_cache:\n",
        "        _schema_linking_chain_cache[chat_model_id] = (\n",
        "            ChatPromptTemplate([(\"human\", SCHEMA_LINKING_TEMPLATE)])\n",
        "            | chat_model\n",
        "            | JsonOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _schema_linking_chain_cache[chat_model_id]\n",
        "\n",
        "\n",
        "async def _link_schema_one(\n",
        "    conversation: List[AnyMessage],\n",
        "    table_name: str,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        "    column_sample_values: Optional[Dict[str, List[str]]] = None,\n",
        "    allowed_col_names: Optional[List[str]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    try:\n",
        "        column_names = database.get_column_names(table_name)\n",
        "        if isinstance(column_names, list) and len(column_names) <= 5:\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"conversation\": conversation,\n",
        "                    \"allowed_col_names\": allowed_col_names,\n",
        "                    \"column_sample_values\": column_sample_values\n",
        "                },\n",
        "                \"filtered_schema\": (table_name, column_names),\n",
        "                \"error\": None\n",
        "            }\n",
        "        table_info = database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=allowed_col_names,\n",
        "            column_sample_values=column_sample_values,\n",
        "            sample_count=3\n",
        "        )\n",
        "        result = await get_schema_linking_chain(chat_model).ainvoke({\n",
        "            \"table_info\": table_info, \n",
        "            \"formatted_conversation\": format_conversation(conversation), \n",
        "            \"dialect\": database.dialect\n",
        "        })\n",
        "        print(result[\"explanation\"])\n",
        "        if \"is_related\" not in result or result[\"is_related\"] not in [\"Y\", \"N\"]:\n",
        "            raise ValueError(\"Invalid response from schema linking chain\")\n",
        "        if result[\"is_related\"] == \"Y\" and not result.get(\"columns\"):\n",
        "            result[\"columns\"] = [\"ROWID\"]\n",
        "\n",
        "        if result[\"is_related\"] == \"N\":\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"conversation\": conversation,\n",
        "                    \"allowed_col_names\": allowed_col_names,\n",
        "                    \"column_sample_values\": column_sample_values\n",
        "                },\n",
        "                \"filtered_schema\": None,\n",
        "                \"error\": None\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"input_item\": {\n",
        "                    \"table_name\": table_name,\n",
        "                    \"conversation\": conversation,\n",
        "                    \"allowed_col_names\": allowed_col_names,\n",
        "                    \"column_sample_values\": column_sample_values\n",
        "                },\n",
        "                \"filtered_schema\": (table_name, result[\"columns\"]),\n",
        "                \"error\": None\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"input_item\": {\n",
        "                \"table_name\": table_name, \n",
        "                \"conversation\": conversation,\n",
        "                \"allowed_col_names\": allowed_col_names,\n",
        "                \"column_sample_values\": column_sample_values\n",
        "            },\n",
        "            \"filtered_schema\": None,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "async def link_schema(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> Dict[str, Dict[str, str]]:\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation is required\")\n",
        "    sample_values = state.get(\"sample_values\", {})\n",
        "    max_retries = 1\n",
        "    # queue = []\n",
        "    # for table in  database.get_usable_table_names():\n",
        "    #     for col_group in database.get_column_groups(table):\n",
        "    #         queue.append({\n",
        "    #             \"table_name\": table,\n",
        "    #             \"allowed_col_names\": col_group,\n",
        "    #             \"conversation\": conversation\n",
        "    #         })\n",
        "    queue = [\n",
        "        {\n",
        "            \"table_name\": table_name, \n",
        "            \"conversation\": conversation,\n",
        "            \"column_sample_values\": sample_values.get(table_name),\n",
        "        } \n",
        "        for table_name in database.get_usable_table_names()\n",
        "    ]\n",
        "    successful_results = []\n",
        "    for _ in range(max_retries):\n",
        "        tasks = [_link_schema_one(chat_model=chat_model, database=database, **input_item) for input_item in queue]\n",
        "        results = await tqdm_asyncio.gather(*tasks)\n",
        "        successful_results.extend([\n",
        "            res for res in results if res[\"error\"] is None\n",
        "        ])\n",
        "        failed_items = [\n",
        "            res[\"input_item\"] for res in results if res[\"error\"] is not None\n",
        "        ]\n",
        "        queue = failed_items\n",
        "        if not queue:\n",
        "            break\n",
        "    \n",
        "    linked_schema = [\n",
        "        result[\"filtered_schema\"] \n",
        "        for result in successful_results \n",
        "        if result[\"filtered_schema\"]\n",
        "    ]\n",
        "    # Return per-table mapping: column_name -> datatype\n",
        "    final_schema: Dict[str, Dict[str, str]] = {}\n",
        "    for table_name, col_names in linked_schema:\n",
        "        table_schema = final_schema.setdefault(table_name, {})\n",
        "        for col_name in col_names:\n",
        "            col_type = database.get_column_datatype(\n",
        "                table_name,\n",
        "                col_name,\n",
        "                default=\"NULL\",\n",
        "            )\n",
        "            if col_type != \"NULL\":\n",
        "                table_schema[col_name] = col_type\n",
        "\n",
        "    state[\"linked_schema\"] = final_schema\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "2e95a2ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "SQL_GEN_TEMPLATE = \"\"\"\n",
        "### DATE INFORMATION:\n",
        "Today is {date}\n",
        "\n",
        "### INSTRUCTIONS:\n",
        "You write SQL queries for a {dialect} database. The Support Team is querying the database to answer Customer questions, and your task is to assist by generating valid SQL queries strictly adhering to the database schema provided.\n",
        "\n",
        "**Table Schema**:\n",
        "{table_infos}\n",
        "\n",
        "\n",
        "Translate the latest customer message into one valid {dialect} query, using the conversation history for context (e.g., resolving pronouns or follow-up filters). SQL should be written as a markdown code block:\n",
        "For example:\n",
        "```sql\n",
        "SELECT column1, column2 FROM table WHERE condition;\n",
        "```\n",
        "\n",
        "### GUIDELINES:\n",
        "\n",
        "1.  **Schema Adherence**:\n",
        "    *   Use only tables, columns, and relationships explicitly listed in the provided schema.\n",
        "    *   Do not make assumptions about missing or inferred columns/tables.\n",
        "\n",
        "2.  **{dialect}-Specific Syntax**:\n",
        "    *   Use only {dialect} syntax. Be aware that {dialect} has limited built-in date/time functions compared to other sql dialects.\n",
        "\n",
        "3.  **Conditions**:\n",
        "    *   Always include default conditions for filtering invalid data, e.g., `deleted_at IS NULL` and `status != 'cancelled'` if relevant.\n",
        "    *   Ensure these conditions match the query's intent unless explicitly omitted in the customer's request.\n",
        "\n",
        "4.  **Output Consistency**:\n",
        "    *   The output fields must match the query's intent exactly. Do not add extra columns or omit requested fields.\n",
        "\n",
        "5.  **Reserved Keywords and Case Sensitivity**:\n",
        "    *   Escape reserved keywords or case-sensitive identifiers using double quotes (\" \"), e.g., \"order\".\n",
        "\n",
        "If the customer's question is ambiguous or unclear, you must make your best reasonable guess based on the schema.\n",
        "Translate the customer's intent into a **single valid {dialect} query** based on the schema provided.\n",
        "Ensure the query is optimized, precise, and error-free.\n",
        "\n",
        "**You must ONLY output ONE SINGLE valid SQL query as markdown codeblock.**\n",
        "\n",
        "### CONVERSATION HISTORY:\n",
        "{formatted_conversation}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "_sql_markdown_re = re.compile(r\"```sql\\s*([\\s\\S]*?)\\s*```\", re.DOTALL)\n",
        "def parse_sql_output(msg_content: str) -> str:\n",
        "    try:\n",
        "        match = _sql_markdown_re.findall(msg_content)\n",
        "        if match:\n",
        "            return match[-1].strip()\n",
        "        else:\n",
        "            raise ValueError(\"No SQL query found in the content\")\n",
        "    except Exception:\n",
        "        return msg_content\n",
        "\n",
        "\n",
        "def preprocess_for_sql_query_generation(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> List[AnyMessage]:\n",
        "    linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation not found in the input\")\n",
        "    formatted_conversation = format_conversation(conversation)\n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=state.get(\"sample_values\", {}).get(table_name),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    return [HumanMessage(SQL_GEN_TEMPLATE.format(\n",
        "        date=get_today_date_en(),\n",
        "        dialect=database.dialect,\n",
        "        table_infos=table_infos,\n",
        "        formatted_conversation=formatted_conversation,\n",
        "    ))]\n",
        "\n",
        "\n",
        "_sql_query_generation_chain_cache: Dict[tuple[int, int], Runnable] = {}\n",
        "def get_sql_query_generation_chain(\n",
        "    chat_model: BaseChatModel, database: SQLiteDatabase\n",
        ") -> Runnable:\n",
        "    chat_model_id, database_id = id(chat_model), id(database)\n",
        "    if (chat_model_id, database_id) not in _sql_query_generation_chain_cache:\n",
        "        _sql_query_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "            RunnableLambda(partial(preprocess_for_sql_query_generation, database=database))\n",
        "            | chat_model\n",
        "            | StrOutputParser()\n",
        "            | parse_sql_output\n",
        "        )\n",
        "    \n",
        "    return _sql_query_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "async def generate_sql_query(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    if not state.get(\"sql_queries\"):\n",
        "        state[\"sql_queries\"] = []\n",
        "    sql_gen_chain = get_sql_query_generation_chain(chat_model, database)\n",
        "    sql_query = await sql_gen_chain.ainvoke(state)\n",
        "    state[\"sql_queries\"].append(sql_query)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "f518f985",
      "metadata": {},
      "outputs": [],
      "source": [
        "def restrict_select_columns(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    \"\"\"\n",
        "    Replaces SELECT * with SELECT t.col1, t.col2 based on filtered_schema.\n",
        "    \"\"\"\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\")\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if not sql_query:\n",
        "        raise ValueError(\"SQL query is required\")\n",
        "    schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not schema:\n",
        "        raise ValueError(\"Schema is required\")\n",
        "    parsed = parse_one(sql_query, read=database.dialect.lower())\n",
        "    \n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Build Alias Map (Map Alias -> Real Table Name)\n",
        "    # ---------------------------------------------------------\n",
        "    # We need to know the order of tables to expand * correctly\n",
        "    active_tables_ordered = [] \n",
        "    alias_map = {}\n",
        "\n",
        "    def register_table(table_node):\n",
        "        real_name = table_node.name\n",
        "        alias = table_node.alias if table_node.alias else real_name\n",
        "        \n",
        "        # Only register if we haven't seen this alias yet\n",
        "        if alias not in alias_map:\n",
        "            alias_map[alias] = real_name\n",
        "            active_tables_ordered.append(alias)\n",
        "\n",
        "    # Scan FROM\n",
        "    for from_node in parsed.find_all(exp.From):\n",
        "        for table in from_node.find_all(exp.Table):\n",
        "            register_table(table)\n",
        "\n",
        "    # Scan JOINs\n",
        "    for join_node in parsed.find_all(exp.Join):\n",
        "        register_table(join_node.this)\n",
        "\n",
        "    print(f\"DEBUG: Active Tables: {alias_map}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Helper to Generate Column Expressions\n",
        "    # ---------------------------------------------------------\n",
        "    def get_columns_for_table(table_alias):\n",
        "        real_name = alias_map.get(table_alias)\n",
        "        if not real_name or real_name not in schema:\n",
        "            return [] # Table not in our allowed schema, return nothing (or handle error)\n",
        "        \n",
        "        # Create sqlglot Column objects: alias.column_name\n",
        "        cols = schema[real_name].keys()\n",
        "        return [\n",
        "            exp.Column(\n",
        "                this=exp.Identifier(this=col, quoted=True),\n",
        "                table=exp.Identifier(this=table_alias, quoted=True)\n",
        "            ) for col in cols\n",
        "        ]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Rewrite SELECT Expressions\n",
        "    # ---------------------------------------------------------\n",
        "    # We only want to transform the main SELECT statement(s)\n",
        "    for select_node in parsed.find_all(exp.Select):\n",
        "        new_expressions = []\n",
        "        \n",
        "        for expression in select_node.expressions:\n",
        "            # Case A: Naked * (SELECT *)\n",
        "            if isinstance(expression, exp.Star) and not isinstance(expression, exp.Count):\n",
        "                # Expand columns for ALL active tables in the query\n",
        "                for alias in active_tables_ordered:\n",
        "                    expanded_cols = get_columns_for_table(alias)\n",
        "                    new_expressions.extend(expanded_cols)\n",
        "            \n",
        "            # Case B: Qualified * (SELECT t.*)\n",
        "            elif isinstance(expression, exp.Column) and isinstance(expression.this, exp.Star):\n",
        "                # Extract the table alias (e.g., 't' from 't.*')\n",
        "                table_alias = expression.table\n",
        "                expanded_cols = get_columns_for_table(table_alias)\n",
        "                new_expressions.extend(expanded_cols)\n",
        "                \n",
        "            # Case C: Regular column or other expression (Keep it)\n",
        "            else:\n",
        "                new_expressions.append(expression)\n",
        "\n",
        "        # Replace the old expressions with the new expanded list\n",
        "        if new_expressions:\n",
        "            select_node.set(\"expressions\", new_expressions)\n",
        "\n",
        "    restricted_sql_query = parsed.sql(dialect=database.dialect.lower())\n",
        "    def normalize_sql_query(sql_query: str) -> str:\n",
        "        return re.sub(r\"\\s+\", \" \", sql_query).strip().strip(\";\").lower()\n",
        "    if normalize_sql_query(restricted_sql_query) != normalize_sql_query(sql_query):\n",
        "        state[\"sql_queries\"].append(restricted_sql_query)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "88402335",
      "metadata": {},
      "outputs": [],
      "source": [
        "QUERY_DATABASE_TOOL = json.dumps({\n",
        "    'type': 'function',\n",
        "    'function': {\n",
        "        'name': 'query_database',\n",
        "        'description': 'Thực hiện câu truy vấn {{dialect}} và trả về kết quả',\n",
        "        'parameters': {\n",
        "            'properties': {\n",
        "                'sql_query': {\n",
        "                    'description': 'Câu truy vấn {{dialect}}',\n",
        "                    'type': 'string'\n",
        "                }\n",
        "            },\n",
        "            'required': ['sql_query'],\n",
        "            'type': 'object'\n",
        "        }\n",
        "    }\n",
        "})\n",
        "\n",
        "\n",
        "ANSWER_GEN_TEMPLATE = \"\"\"\n",
        "### THÔNG TIN NGÀY THÁNG:\n",
        "Hôm nay là {date}\n",
        "\n",
        "### NHIỆM VỤ:\n",
        "Bạn là một trợ lý phân tích dữ liệu chuyên nghiệp. Nhiệm vụ của bạn là đưa ra câu trả lời bằng **Tiếng Việt** rõ ràng, chính xác và súc tích cho câu hỏi của người dùng, dựa hoàn toàn vào kết quả cơ sở dữ liệu (database results) được cung cấp.\n",
        "\n",
        "**Lược đồ bảng (Table Schema)**:\n",
        "{table_infos}\n",
        "\n",
        "### CÁC NGUYÊN TẮC HƯỚNG DẪN:\n",
        "\n",
        "1.  **Chính xác và Tuân thủ dữ liệu**:\n",
        "    *   Câu trả lời phải dựa **TUYỆT ĐỐI** vào phần \"Kết quả từ Database\".\n",
        "    *   Không được tự suy diễn hoặc đưa vào các kiến thức bên ngoài không có trong dữ liệu.\n",
        "    *   Nếu kết quả trả về là rỗng (empty), hãy thông báo lịch sự rằng không tìm thấy dữ liệu phù hợp với yêu cầu.\n",
        "\n",
        "2.  **Định dạng câu trả lời**:\n",
        "    *   **Trả lời trực tiếp**: Đi thẳng vào vấn đề.\n",
        "    *   **Danh sách/Bảng**: Nếu kết quả có nhiều dòng, hãy trình bày dưới dạng danh sách gạch đầu dòng hoặc bảng Markdown cho dễ đọc.\n",
        "    *   **Số liệu tổng hợp**: Nếu kết quả là một con số duy nhất (tổng, đếm, trung bình), hãy viết thành một câu hoàn chỉnh (Ví dụ: \"Tổng doanh thu là 50.000.000 VNĐ\").\n",
        "\n",
        "3.  **Trình bày dữ liệu (Formatting)**:\n",
        "    *   **Con số**: Sử dụng dấu phân cách hàng nghìn (ví dụ: 1.000 hoặc 1,000 tùy theo ngữ cảnh, nhưng phải nhất quán).\n",
        "    *   **Tiền tệ**: Thêm đơn vị tiền tệ phù hợp nếu có (ví dụ: VNĐ, $, USD).\n",
        "    *   **Ngày tháng**: Chuyển đổi sang định dạng ngày tháng Tiếng Việt tự nhiên (ví dụ: \"Ngày 01 tháng 01 năm 2024\").\n",
        "\n",
        "4.  **Ngữ cảnh và Thuật ngữ**:\n",
        "    *   Sử dụng \"Truy vấn SQL\" để hiểu ngữ cảnh lọc dữ liệu (ví dụ: nếu SQL có `WHERE status = 'active'`, hãy nói rõ đây là các đơn hàng có trạng thái là \"đang hoạt động\").\n",
        "    *   Sử dụng ngôn ngữ kinh doanh/đời thường. **Không** nhắc đến tên bảng kỹ thuật (như `tbl_users`, `col_price`) hoặc cú pháp code trong câu trả lời cuối cùng.\n",
        "\n",
        "5.  **Văn phong**:\n",
        "    *   Chuyên nghiệp, khách quan và hữu ích.\n",
        "    *   Tránh các câu máy móc như mà hãy trả lời tự nhiên như một con người.\n",
        "\n",
        "**Đầu ra**:\n",
        "Chỉ xuất ra câu trả lời cuối cùng bằng Tiếng Việt (sử dụng Markdown).\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "class QueryDatabaseInput(BaseModel):\n",
        "    sql_query: str = Field(description=\"Câu truy vấn SQLite\")\n",
        "\n",
        "\n",
        "@tool(\"query_database\", args_schema=QueryDatabaseInput)\n",
        "def query_database(sql_query: str) -> str:\n",
        "    \"\"\"Thực hiện câu truy vấn SQLite và trả về kết quả\"\"\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def preprocess_for_answer_generation(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> List[AnyMessage]:\n",
        "    conversation = state.get(\"conversation\")\n",
        "    if not conversation:\n",
        "        raise ValueError(\"conversation not found in the input\")\n",
        "    linked_schema: Dict[str, Dict[str, str]] = state.get(\"linked_schema\")\n",
        "    if not linked_schema:\n",
        "        raise ValueError(\"linked_schema not found in the input\")\n",
        "    db_output: Dict[str, Any] = state.get(\"db_output\", {})\n",
        "    sql_queries: List[str] = state.get(\"sql_queries\", [])\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"sql_queries not found in the input\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    if db_output.get(\"error\", \"Error\") is not None:\n",
        "        raise ValueError(\"No valid database result found\")\n",
        "    db_result = db_output.get(\"result\", [])\n",
        "    \n",
        "    table_infos = \"\\n\\n\".join([\n",
        "        database.get_table_info_no_throw(\n",
        "            table_name,\n",
        "            get_col_comments=True,\n",
        "            allowed_col_names=list(col_types.keys()),\n",
        "            sample_count=5,\n",
        "            column_sample_values=state.get(\"sample_values\", {}).get(table_name, None),\n",
        "        )\n",
        "        for table_name, col_types in linked_schema.items()\n",
        "    ])\n",
        "    tool_call = {\n",
        "        \"name\": \"query_database\",\n",
        "        \"arguments\": {\"sql_query\": sql_query}\n",
        "    }\n",
        "    system_message = SystemMessage(content=ANSWER_GEN_TEMPLATE.format(\n",
        "        date=get_today_date_vi(),\n",
        "        table_infos=table_infos,\n",
        "    ))\n",
        "    \n",
        "    sql_conversation = [system_message] + conversation\n",
        "    sql_conversation.append(AIMessage('<tool_call>\\n' + json.dumps(tool_call, ensure_ascii=False) + '\\n</tool_call>'))\n",
        "    sql_conversation.append(HumanMessage(content=str(db_result)))\n",
        "    return sql_conversation\n",
        "\n",
        "\n",
        "_answer_generation_chain_cache: Dict[tuple[int, int], Runnable] = {}\n",
        "def get_answer_generation_chain(chat_model: BaseChatModel, database: SQLiteDatabase) -> Runnable:\n",
        "    chat_model_id, database_id = id(chat_model), id(database)\n",
        "    openai_tool_schema = QUERY_DATABASE_TOOL.replace(\"{{dialect}}\", database.dialect)\n",
        "    if (chat_model_id, database_id) not in _answer_generation_chain_cache:\n",
        "        _answer_generation_chain_cache[(chat_model_id, database_id)] = (\n",
        "            RunnableLambda(partial(preprocess_for_answer_generation, database=database))\n",
        "            | chat_model.bind(tools=[openai_tool_schema])\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    \n",
        "    return _answer_generation_chain_cache[(chat_model_id, database_id)]\n",
        "\n",
        "\n",
        "async def generate_answer(\n",
        "    state: SQLAssistantState,\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    answer_chain = get_answer_generation_chain(chat_model, database)\n",
        "    answer = await answer_chain.ainvoke(state)\n",
        "    state[\"final_answer\"] = answer\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "56258285",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def sql_execution(\n",
        "    state: SQLAssistantState,\n",
        "    database: SQLiteDatabase,\n",
        ") -> SQLAssistantState:\n",
        "    sql_queries = state.get(\"sql_queries\", [])\n",
        "    if not sql_queries:\n",
        "        raise ValueError(\"SQL queries are required\")\n",
        "    sql_query = sql_queries[-1]\n",
        "    state[\"db_output\"] = await database.run_no_throw(sql_query, include_columns=True)\n",
        "    return state\n",
        "\n",
        "\n",
        "def build_sql_assistant(\n",
        "    chat_model: BaseChatModel,\n",
        "    database: SQLiteDatabase,\n",
        ") -> CompiledStateGraph:\n",
        "    builder = StateGraph(SQLAssistantState)\n",
        "    # Add nodes\n",
        "    builder.add_node(\n",
        "        \"rewrite_message\",\n",
        "        partial(\n",
        "            rewrite_message,\n",
        "            chat_model=chat_model,\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"get_sample_values\",\n",
        "        partial(\n",
        "            get_sample_values,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"link_schema\",\n",
        "        partial(\n",
        "            link_schema,\n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"gen_sql_query\",\n",
        "        partial(\n",
        "            generate_sql_query, \n",
        "            chat_model=chat_model,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"restrict_select_columns\",\n",
        "        partial(\n",
        "            restrict_select_columns,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    builder.add_node(\n",
        "        \"sql_execution\", \n",
        "        partial(\n",
        "            sql_execution,\n",
        "            database=database\n",
        "        )\n",
        "    )\n",
        "    # builder.add_node(\n",
        "    #     \"answer_generation\", \n",
        "    #     partial(\n",
        "    #         generate_answer,\n",
        "    #         chat_model=chat_model,\n",
        "    #         database=database\n",
        "    #     )\n",
        "    # )\n",
        "\n",
        "    # Add edges\n",
        "    builder.add_edge(START, \"rewrite_message\")\n",
        "    builder.add_edge(\"rewrite_message\", \"get_sample_values\")\n",
        "    builder.add_edge(\"get_sample_values\", \"link_schema\")\n",
        "    builder.add_edge(\"link_schema\", \"gen_sql_query\")\n",
        "    builder.add_edge(\"gen_sql_query\", \"restrict_select_columns\")\n",
        "    builder.add_edge(\"restrict_select_columns\", \"sql_execution\")\n",
        "    # builder.add_edge(\"sql_execution\", \"answer_generation\")\n",
        "    # builder.add_edge(\"answer_generation\", END)\n",
        "    builder.add_edge(\"sql_execution\", END)\n",
        "\n",
        "    return builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "3f8e7672",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_assistant = build_sql_assistant(llm, db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "213d29b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:04<00:04,  4.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The latest customer query is asking for information about office spaces in HCM with a price below 6 million VND per month. The table 'BĐS Cho thuê 500' contains data about rental properties, including the 'Loại BĐS' column which can identify office spaces and the 'Giá thuê (triệu/tháng)' column which indicates the monthly rent. Additionally, the 'Tỉnh/TP' column can be used to filter for HCM. These columns are semantically related to the query.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:04<00:00,  2.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The latest customer query is asking for information about office spaces in HCM with a price below 6 million VND per month. The table 'BĐS Bán 500' represents real estate properties for sale, not rental or office spaces. The table contains columns related to property type (e.g., 'Loại BĐS'), location (e.g., 'Quận/Huyện', 'Tỉnh/TP'), and price (e.g., 'Giá (tỷ VNĐ)', 'Giá/m²'). However, none of the columns directly relate to rental properties, office spaces, or monthly pricing. Therefore, the table is not relevant to the customer's query.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Active Tables: {'BĐS Cho thuê 500': 'BĐS Cho thuê 500'}\n"
          ]
        }
      ],
      "source": [
        "state = await sql_assistant.ainvoke({\"conversation\": [\n",
        "    # HumanMessage(\"hello có văn phòng cho thuê dưới 10tr ko, văn phòng nhé\"),\n",
        "    # AIMessage(\"Không tìm thấy văn phòng cho thuê có giá dưới 10 triệu đồng/tháng trong cơ sở dữ liệu.\"),\n",
        "    # HumanMessage(\"thế dưới 30tr thì sao\"),\n",
        "    HumanMessage(\"xin thông tin các văn phòng giá dưới 6tr 1 tháng ở HCM\"),\n",
        "]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511e0ade",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370444aa",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d199c0c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8e3d20",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acbb6310",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "50620079",
      "metadata": {},
      "source": [
        "## Generate table description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "7767eecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "TABLE_DESCRIPTION_PROMPT = \"\"\"\n",
        "Bạn là một chuyên gia quản trị dữ liệu (Data Steward). Nhiệm vụ của bạn là phân tích cấu trúc và dữ liệu mẫu của một bảng (table) để tạo ra mô tả tóm tắt (metadata).\n",
        "\n",
        "Mô tả này sẽ được sử dụng bởi một AI Router để quyết định xem câu hỏi của người dùng có liên quan đến bảng này hay không.\n",
        "\n",
        "HÃY PHÂN TÍCH DỰA TRÊN SCHEMA CỦA BẢNG:\n",
        "{table_info}\n",
        "\n",
        "YÊU CẦU ĐẦU RA (Định dạng JSON):\n",
        "Hãy trả về một JSON object duy nhất đặt trong ```json ...``` với các trường sau:\n",
        "- \"human_name\": Tên ngắn gọn, dễ hiểu cho người dùng.\n",
        "- \"summary\": Phần mô tả bảng bằng tiếng Việt. Nêu rõ bảng này chứa thông tin về **đối tượng gì** (Entity) và **thuộc tính quan trọng nào**.\n",
        "\"\"\".strip()\n",
        "\n",
        "table_names = db.get_usable_table_names()\n",
        "table_info = db.get_table_info_no_throw(\n",
        "    table_name=table_names[1],\n",
        "    get_col_comments=True,\n",
        "    sample_count=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "079d37ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, let's tackle this. The user wants me to act as a Data Steward and create a metadata summary for the \"BĐS Cho thuê 500\" table. The goal is to help an AI Router determine if user questions relate to this table.\n",
            "\n",
            "First, I need to understand the schema. Let me go through each column. The table name translates to \"Real Estate for Rent 500,\" so it's about rental properties. The columns include details like security, balcony, parking, kitchen, pet policy, area, project name, contact info, notes, price, location, etc.\n",
            "\n",
            "I should identify the main entity here. It's clearly real estate properties available for rent. Now, the important attributes. Let me list them out: security features, outdoor spaces, parking options, kitchen facilities, pet policies, area in square meters, project names, contact details, notes, rental prices, price per square meter, location details (district, ward, province), dates for move-in, number of bedrooms and bathrooms, floor numbers, elevator availability, lease terms, title descriptions, deposit amounts, utility costs, nearby amenities, property status, and more.\n",
            "\n",
            "The summary needs to be in Vietnamese, concise, and highlight the entity and key attributes. I should mention that the table contains information about rental properties, including their features, pricing, location details, and contact information. Also, note the various attributes like area, price, location, and amenities. Make sure to cover both the main entity and the critical attributes that would be relevant for queries related to renting properties.\n",
            "</think>\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"human_name\": \"Thông tin Bất động sản cho thuê\",\n",
            "  \"summary\": \"Bảng chứa thông tin chi tiết về các bất động sản cho thuê, bao gồm đặc điểm cơ bản (diện tích, loại hình, số phòng), giá cả (giá thuê, giá/m²), vị trí (quận/huyện, phường/xã, tỉnh/TP), tiện ích (an ninh, thang máy, điều hòa), điều kiện thuê (thời hạn tối thiểu, tiền cọc), và thông tin liên hệ. Các thuộc tính quan trọng gồm: Diện tích (m²), Giá thuê (triệu/tháng), Địa chỉ, Loại BĐS, Quận/Huyện, Số phòng ngủ, Số phòng tắm, Tình trạng, và Tiện ích lân cận.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "tmp = chat_reasoning_model.invoke(TABLE_DESCRIPTION_PROMPT.format(table_info=table_info))\n",
        "print(tmp.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e81bd36",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "faiss-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
